---
title: "Using Rasch and WLS to account for measurement uncertainties in regression models"
author: 
  name: Magnus Johansson
  affiliation: RISE Research Institutes of Sweden
  affiliation-url: https://www.ri.se/en/what-we-do/projects/center-for-category-based-measurements
  orcid: 0000-0003-1669-592X
date: last-modified
citation:
  type: 'webpage'
csl: apa.csl
execute: 
  cache: true
  warning: false
  message: false
editor: 
  markdown: 
    wrap: 72
editor_options: 
  chunk_output_type: console
---

## Introduction

There are several reasons for putting this blog post together. The larger issue is to investigate ways to analyze relationships between measures that have varying levels of measurement uncertainty across their respective range or continuum. This is  relevant to any kind of latent variable measurement that uses multiple indicators/items/questions as a way to assess a latent variable. 

In classical test theory (CTT; i.e. factor analysis) the assumption is generally that the measurement error is a single value, constant across the scale and across participants. Modern test theory (Rasch Measurement Theory (RMT) or Item Response Theory (IRT)) has tools to describe the varying uncertainties of the measure itself, and also allows for estimation of measurement uncertainty for each individual, based on the item properties.

In the "business-as-usual" approach, no matter which  types of measurement uncertainty are ignored. The simple case of have two variables and an ordinary least squares (OLS) linear regression model will take the input from each variable as a perfect measurement. Bayesians may have another take on this, and I will look into that at a later point, so for now this is relevant for frequentist statistics. 

"Business-as-usual" also makes use of ordinal sum scores disguised as interval scores. There is a seemingly wide-spread idea that estimated person interval scores is not significantly different from ordinal sum scores, and we'll look further into that as well by including ordinal sum scores.

One way to take measurement uncertainty into account in a linear regression model is to use Weighted Least Squares (WLS) instead of OLS. However, this approach only allows weights for one variable. The weights will be based on the measurement uncertainty for the interval scores.

Additionally, we will use Quantile Regression, with and without weights.

## Data

The data used were collected for a project evaluating multiple work environment questionnaires. Two of the analyzed scales will be used in our example. More information is available at the GitHub [repository](https://github.com/pgmj/PreventOSA) and [website](https://pgmj.github.io/PreventOSA/).

The two questionnaires cover the domains of "recovery" and "agency", where the latter refers to workers' perceived control over their work situation. Our analyses will look at how "agency" affects "recovery". We will retain an interval scale score for the "recovery" scale throughout, while varying the "agency" between ordinal sum score, interval score, and interval score with weights.

Variables will be named `SEM` for standard error of measurement, `Score` for interval scores, and `SumScore` for ordinal sum scores.

::: {.callout-note}
This analysis is intended to be fully reproducible, which means some data wrangling is done initially. Please jump ahead to @sec-viz for data visualizations or @sec-lmm for the regression model output.
:::

```{r}
library(RISEkbmRasch)
library(foreign)
library(readxl)
library(easystats)
library(lmtest)
library(quantreg)
```

```{r}
### some commands exist in multiple packages, here we define preferred ones that are frequently used
select <- dplyr::select
count <- dplyr::count
recode <- car::recode
rename <- dplyr::rename

# read full dataset, remove ppl w missing data
df <- read.spss("data/PreventOSAspss.sav",
                to.data.frame = TRUE) %>% 
  na.omit()
# and item information
itemlabels <- read_excel("data/PreventOSAitemlabels.xlsx")
```

```{r}
# vector of items
agencyItems <- itemlabels %>%
  filter(Dimension == "Möjlighet att påverka") %>%
  select(!Dimension)

# data import, with recode of response categories to numerics
agencyData <- df %>%
  select(starts_with("q0007")) %>%
  mutate(across(everything(), ~ car::recode(.x, "'Aldrig'=0;
                                            'Sällan' =1;
                                            'Ibland'=2;
                                            'Ganska ofta'=3;
                                            'Mycket ofta'=4;
                                            'Alltid'=5",
    as.factor = FALSE
  )))
names(agencyData) <- agencyItems$itemnr

# adjustments based on psychometric analysis
agencyData$mp4 <- NULL
agencyData$mp1<-recode(agencyData$mp1,"1=0;2=1;3=2;4=3;5=4",as.factor=FALSE)
agencyData$mp3<-recode(agencyData$mp3,"1=0;2=1;3=2;4=3;5=4",as.factor=FALSE)
agencyData$mp2<-recode(agencyData$mp2,"1=0;2=1;3=2;4=3;5=4",as.factor=FALSE)
agencyData$mp5<-recode(agencyData$mp5,"2=1;3=2;4=3;5=4",as.factor=FALSE)
```

```{r}
# vector of items
recoveryItems <- itemlabels %>%
  filter(Dimension == "Återhämtning") %>%
  select(!Dimension)

# data import, with recode of response categories to numerics
recoveryData <- df %>%
  select(starts_with("q0009")) %>%
  mutate(across(everything(), ~ car::recode(.x, "'Aldrig'=0;
                                            'Sällan' =1;
                                            'Ibland'=2;
                                            'Ganska ofta'=3;
                                            'Mycket ofta'=4;
                                            'Alltid'=5",
    as.factor = FALSE
  )))
names(recoveryData) <- recoveryItems$itemnr

# adjustments based on psychometric analysis
recoveryData$å2 <- NULL
```

## Measurement properties

::: {.panel-tabset} 
### Targeting recovery
```{r}
RItargeting(recoveryData)
recoveryRange <- c(-4,6)
```

### Targeting agency
```{r}
RItargeting(agencyData)
agencyRange <- c(-4,6)
```

### Reliability recovery
```{r}
RItif(recoveryData)
```
### Reliability agency
```{r}
RItif(agencyData, cutoff = 2.5) +
  geom_hline(yintercept = 2.5, 
    color = "#e83c63", linetype = 5, size = 0.5)
```
:::

## Estimating person score/locations.

```{r}
df$agencyScore <- RIestThetas(agencyData, theta_range = agencyRange)
df$agencyScore <- round(df$agencyScore,3)

df$recoveryScore <- RIestThetas(recoveryData, theta_range = recoveryRange)
df$recoveryScore <- round(df$recoveryScore,3)
```

### Plot relationship ordinal/interval

```{r}
RIscoreSE(recoveryData, sdx = 15, score_range = recoveryRange, output = "figure")
#RIscoreSE(recoveryData, sdx = 15, score_range = recoveryRange)
RIscoreSE(agencyData, sdx = 10, score_range = agencyRange, output = "figure")
#RIscoreSE(agencyData, sdx = 10, score_range = agencyRange, output = "table")

```

### Measurement uncertainties
```{r}
recoveryTable <- RIscoreSE(recoveryData, sdx = 15, width = 50, score_range = recoveryRange, output = "dataframe") 
recoveryTable <- recoveryTable %>% 
  rename(recoverySEM = `Logit std.error`,
         recoveryScore = `Logit score`,
         recoverySumScore = `Ordinal sum score`) %>% 
  mutate(recoveryScore = round(recoveryScore,3))

agencyTable <- RIscoreSE(agencyData, width = 50, sdx = 10, score_range = agencyRange, output = "dataframe")
agencyTable <- agencyTable %>% 
  rename(agencySEM = `Logit std.error`,
         agencyScore = `Logit score`,
         agencySumScore = `Ordinal sum score`) %>% 
  mutate(agencyScore = round(agencyScore,3))
```

Join SEM values to df based on scores

```{r}

data <- data.frame(
  agencyScore = df$agencyScore,
  recoveryScore = df$recoveryScore
)

data <- left_join(data,recoveryTable, by = "recoveryScore")
data <- left_join(data,agencyTable, by = "agencyScore")

glimpse(data)
```

We have some respondents with unexpected responses and thetas outside what we have generic SEM info about. It is possible to estimate individual SEM but this has not yet been implemented in the RISEkbmRasch package and for this exercise it'll be sufficient to use those with 

```{r}
data <- na.omit(data)
```

## Visualizing data {#sec-viz}

### Histograms

::: panel-tabset
#### Interval score
```{r}
#| fig-height: 3
data %>% 
  pivot_longer(cols = c("agencyScore","recoveryScore")) %>%
  ggplot(aes(x = value)) +
  geom_histogram(bins = 20) +
  facet_wrap(~name) +
  theme_rise()

```
#### Ordinal sum score
```{r}
#| fig-height: 3
data %>% 
  pivot_longer(cols = c("agencySumScore","recoverySumScore")) %>%
  ggplot(aes(x = value)) +
  geom_histogram(bins = 20) +
  facet_wrap(~name) +
  theme_rise()
```
:::

### SEM

::: panel-tabset
#### Agency
```{r}
#| fig-height: 4
data %>% 

  ggplot(aes(y = agencySEM, 
             x = agencyScore)) +
  geom_point() +
  geom_line(aes(group = 1)) +
  geom_hline(yintercept = 0.54, linetype = 2) +
  theme_rise() +
  coord_cartesian(ylim = c(0,2)) +
  labs(caption = str_wrap("Note. Lower SEM = improved reliability. A SEM at 0.54 (dashed line) corresponds to a reliability coefficient of 0.7 on a 0 to 1 scale. SEM = 0.45 corresponds to reliability 0.8"))
```

#### Recovery
```{r}
#| fig-height: 4
data %>% 

  ggplot(aes(y = recoverySEM, 
             x = recoveryScore)) +
  geom_point() +
  geom_line(aes(group = 1)) +
  geom_hline(yintercept = 0.54, linetype = 2) +
  theme_rise() +
  coord_cartesian(ylim = c(0,2)) +
  labs(caption = str_wrap("Note. Lower SEM = improved reliability. A SEM at 0.54 (dashed line) corresponds to a reliability coefficient of 0.7 on a 0 to 1 scale. SEM = 0.45 corresponds to reliability 0.8"))
```
:::

### Scatter plots

::: panel-tabset
#### Interval score 

```{r}
ggplot(data,
       aes(x = agencyScore,
           y = recoveryScore)) +
  geom_point() +
  geom_smooth(method = "lm") +
  theme_rise()
```
#### Ordinal sum score 
```{r}
ggplot(data,
       aes(x = agencySumScore,
           y = recoveryScore)) +
  geom_point() +
  geom_smooth(method = "lm") +
  theme_rise()
```
:::

## Weights for WLS regression

```{r}
data %>% 
  mutate(weights = 1/agencySEM) %>% 
  ggplot(aes(x = agencySEM, 
             y = weights)) +
  geom_point(size = 2) +
  theme_rise() +
  coord_cartesian(ylim = c(0,2))

data %>% 
  mutate(weights = 1/agencySEM) %>% 
  select(weights,agencySEM) %>% 
  unique() %>% 
  arrange(agencySEM) %>% 
  head(20)
```
  
## Linear regression models {#sec-lmm}

::: panel-tabset
### OLS interval score
```{r}
#| fig-height: 7
olsInt <- lm(recoveryScore ~ agencyScore, data = data)
summary(olsInt)
bptest(olsInt)
dwtest(olsInt)
check_model(olsInt)
```

### WLS interval score 
```{r}
#| fig-height: 7
wlsInt <- lm(recoveryScore ~ agencyScore, 
         data = data,
         weights = 1/data$agencySEM)
summary(wlsInt)
check_model(wlsInt)

```

### OLS ordinal score
```{r}
#| fig-height: 7
olsOrd <- lm(recoveryScore ~ agencySumScore, data = data)
summary(olsOrd)
check_model(olsOrd)
```
:::

### Linear model comparison

```{r}
library(broom)
models <- c("OLSinterval","WLSinterval","OLSordinal")

rbind(tidy(olsInt, conf.int = TRUE),
      tidy(wlsInt, conf.int = TRUE),
      tidy(olsOrd, conf.int = TRUE)) %>% 
  add_column(model = rep(models, each = 2), .before = "term") %>% 
  mutate(across(where(is.numeric), ~ round(.x, 3))) %>% 
  arrange(term) %>% 
  kbl_rise()

rbind(glance(olsInt),
      glance(wlsInt),
      glance(olsOrd)) %>% 
  add_column(model = rep(models, each = 1), .before = "r.squared") %>% 
  mutate(across(where(is.numeric), ~ round(.x, 3))) %>% 
  kbl_rise()

```


## Quantile regression

We can use any set of quantiles using the `tau` option. Here, we choose 0.25, 0.5, and 0.75. Tau = 0.5 is the same as "median regression".

### Visualization

```{r}
dviz <- data %>%
  add_column(id = seq(1, nrow(data), by = 1)) %>% 
  pivot_longer(c("agencyScore","recoveryScore")) %>% 
  group_by(name) %>%
  mutate(qgroup = case_when(value < quantile(value, probs = .25) ~ "lower",
                            value >= quantile(value, probs = .25) & value < quantile(value, probs = .75) ~ "middle",
                            value >= quantile(value, probs = .75) ~ "upper")) %>% 
  ungroup() %>% 
  pivot_wider(id_cols = "id", 
              names_from = "name", 
              values_from = c("value","qgroup"))

dviz %>%
  ggplot(aes(
    x = value_agencyScore,
    y = value_recoveryScore,
    color = qgroup_recoveryScore
  )) +
  geom_point() +
  theme_rise() +
  scale_color_brewer(type = "qual", palette = "Dark2") +
  theme(legend.position = "none") +
  labs(x = "Agency Score",
       y = "Recovery Score")
```


::: panel-tabset
### Interval
```{r}
#| fig-height: 7
olsIntQ <- rq(recoveryScore ~ agencyScore, 
              tau = c(0.25,0.5,0.75),
              data = data)
summary(olsIntQ)
```

### Interval weighted
```{r}
wlsIntQ <- rq(recoveryScore ~ agencyScore, 
              tau = c(0.25,0.5,0.75),
              data = data,
              weights = 1/data$agencySEM)
summary(wlsIntQ)
```

### Ordinal
```{r}
olsOrdQ <- rq(recoveryScore ~ agencySumScore, 
              tau = c(0.25,0.5,0.75),
              data = data)
summary(olsOrdQ)
```
:::

### QR parameter plots

::: panel-tabset
#### Interval OLS
```{r}
plot(parameters(olsIntQ)) +
  geom_vline(xintercept = 0.5, linetype = 2) +
  geom_vline(xintercept = 0.6, linetype = 2) +
  labs(title = "OLS quantile regression with interval predictor",
       caption = "Note. Dashed lines indicate 0.5 and 0.6.") +
  coord_cartesian(xlim = c(0,0.7))
```
#### Interval WLS
```{r}
plot(parameters(wlsIntQ)) +
  geom_vline(xintercept = 0.5, linetype = 2) +
  geom_vline(xintercept = 0.6, linetype = 2) +
  labs(title = "WLS quantile regression with interval predictor",
       caption = "Note. Dashed lines indicate 0.5 and 0.6.") +  coord_cartesian(xlim = c(0,0.7))
```
#### Ordinal OLS
```{r}
plot(parameters(olsOrdQ)) +
  geom_vline(xintercept = 0.25, linetype = 2) +
  labs(title = "OLS quantile regression with ordinal sum score predictor",
        caption = "Note. Dashed line indicates 0.25.") +
  coord_cartesian(xlim = c(0,0.4))
```
:::

### Quantile regression scatter plots

I haven't figured out how to make ggplot use weighted model, so these are only unweighted.

::: panel-tabset
#### Interval OLS quartiles
```{r}
ggplot(data,
       aes(x = agencyScore, y = recoveryScore)) +
  geom_point() +
  geom_quantile(quantiles = c(0.25,0.5,0.75),
                method = "rq")
```
#### Ordinal OLS quartiles
```{r}
ggplot(data,
       aes(x = agencySumScore, y = recoveryScore)) +
  geom_point() +
  geom_quantile(quantiles = c(0.25,0.5,0.75),
                method = "rq")
```
#### Interval OLS quintiles
```{r}
ggplot(data,
       aes(x = agencyScore, y = recoveryScore)) +
  geom_point() +
  geom_quantile(quantiles = c(0.2,0.4,0.6,0.8),
                method = "rq")
```
#### Ordinal OLS quintiles
```{r}
ggplot(data,
       aes(x = agencySumScore, y = recoveryScore)) +
  geom_point() +
  geom_quantile(quantiles = c(0.2,0.4,0.6,0.8),
                method = "rq")
```
:::
