
@article{burkner_bayesian_2021,
	title = {Bayesian {Item} {Response} {Modeling} in {R} with brms and {Stan}},
	volume = {100},
	copyright = {Copyright (c) 2021 Paul-Christian Bürkner},
	issn = {1548-7660},
	url = {https://doi.org/10.18637/jss.v100.i05},
	doi = {10.18637/jss.v100.i05},
	abstract = {Item response theory (IRT) is widely applied in the human sciences to model persons' responses on a set of items measuring one or more latent constructs. While several R packages have been developed that implement IRT models, they tend to be restricted to respective pre-specified classes of models. Further, most implementations are frequentist while the availability of Bayesian methods remains comparably limited. I demonstrate how to use the R package brms together with the probabilistic programming language Stan to specify and fit a wide range of Bayesian IRT models using flexible and intuitive multilevel formula syntax. Further, item and person parameters can be related in both a linear or non-linear manner. Various distributions for categorical, ordinal, and continuous responses are supported. Users may even define their own custom response distribution for use in the presented framework. Common IRT model classes that can be specified natively in the presented framework include 1PL and 2PL logistic models optionally also containing guessing parameters, graded response and partial credit ordinal models, as well as drift diffusion models of response times coupled with binary decisions. Posterior distributions of item and person parameters can be conveniently extracted and postprocessed. Model fit can be evaluated and compared using Bayes factors and efficient cross-validation procedures.},
	language = {en},
	urldate = {2023-08-21},
	journal = {Journal of Statistical Software},
	author = {Bürkner, Paul-Christian},
	month = nov,
	year = {2021},
	keywords = {R, Item Response Theory, brms, Bayesian Statistics, Stan},
	pages = {1--54},
	file = {Full Text PDF:/Users/magnuspjo/Zotero/storage/R2RFBE9I/Bürkner - 2021 - Bayesian Item Response Modeling in R with brms and.pdf:application/pdf},
}


@misc{bignardi_general_2025,
	title = {A general method for estimating reliability using {Bayesian} {Measurement} {Uncertainty}},
	url = {https://osf.io/h54k8_v1},
	doi = {10.31234/osf.io/h54k8_v1},
	abstract = {Unreliable measurement can lead to lower statistical power, attenuated effect sizes and residual confounding. However, estimating reliability can be challenging for complex cognitive and behavioural assessments without test-retest data. Most statistical methods for assessing reliability with a single test administration are designed for fixed-item questionnaires. We introduce a novel Bayesian procedure called relative measurement uncertainty (RMU) for estimating reliability that can be broadly applied, including to many widely used computational models. Our approach draws pairs of samples from each subject's posterior and calculates the correlation between draws. We demonstrate analytically and via simulation that this method provides accurate reliability estimates and well-calibrated credible intervals across linear factor, signal detection and reinforcement learning models. Simulations found that RMU was more accurate (lower root mean square error) than existing measures (coefficient alpha, coefficient H, split-half), but more importantly, it can be applied across a wide range of computational models. RMU offers a general method for estimating reliability, leveraging the modelling flexibility of Bayesian statistical methods.},
	language = {en-us},
	urldate = {2025-10-01},
	publisher = {OSF},
	author = {Bignardi, Giacomo and Kievit, Rogier and Bürkner, Paul-Christian},
	month = sep,
	year = {2025},
	file = {PDF:/Users/magnuspjo/Zotero/storage/BB5CRHVM/Bignardi et al. - 2025 - A general method for estimating reliability using Bayesian Measurement Uncertainty.pdf:application/pdf},
}


@article{kreiner_specific_2025,
	title = {On {Specific} {Objectivity} and {Measurement} by {Rasch} {Models}: {A} {Statistical} {Viewpoint}},
	volume = {3},
	url = {https://dx.doi.org/10.61186/emp.2025.7},
	doi = {10.61186/emp.2025.7},
	abstract = {Rasch’s original definition of specific objectivity insisted that measurement by items of educational tests had to be provided by estimates of person parameters of a measurement model that did not involve estimates of item parameters. It was never stated clearly, but there is no doubt that Rasch was thinking about conditional maximum likelihood (CML) estimates similar to the well-known CML estimates of item parameters. However, for technical reasons, there are only few examples of attempts to implement the CML estimates of person parameters in software applications for item analysis by Rasch models. Nevertheless, today CML estimation of person parameters in Rasch models is possible. And since the exact distribution of estimates of person parameters in Rasch models is known, it is possible to compare the performance of the specific objective CML estimates of person parameters to the performances of the other estimates of the person parameters. This paper provides one example of such an exercise. It compares five different estimators of the person parameters of a Rasch model with forty dichotomous items for a Danish cognitive test. In this paper, the point of view on the measurement quality is purely statistical. The superior measure of the latent trait has to have ignorable bias and as little standard error of measurement as possible. In this example, it turns out that the specific objective CML estimate is not the superior estimate. This motivates a slightly weaker notion of essential specific objectivity than many other estimates of person parameters in Rasch models live up to.},
	language = {en},
	journal = {Educational Methods \& Psychometrics},
	author = {Kreiner, Svend},
	year = {2025},
	file = {PDF:/Users/magnuspjo/Zotero/storage/H2U4GVHI/Kreiner - ON SPECIFIC OBJECTIVITY AND MEASUREMENT BY RASCH MODELS A STATISTICAL VIEWPOINT.pdf:application/pdf},
}


@article{mcneish_reliability_2025,
	title = {Reliability representativeness: {How} well does coefficient alpha summarize reliability across the score distribution?},
	volume = {57},
	issn = {1554-3528},
	shorttitle = {Reliability representativeness},
	url = {https://doi.org/10.3758/s13428-025-02611-8},
	doi = {10.3758/s13428-025-02611-8},
	abstract = {Scale scores in psychology studies are commonly accompanied by a reliability coefficient like alpha. Coefficient alpha is an index that summarizes reliability across the entire score distribution, implying equal precision for all scores. However, an underappreciated fact is that reliability can be conditional such that scores in certain parts of the score distribution may be more reliable than others. This conditional perspective of reliability is common in item response theory (IRT), but psychologists are generally not well versed in IRT. Correspondingly, the representativeness of a single summary index like alpha across the entire score distribution can be unclear but is rarely considered. If conditional reliability is fairly homogeneous across the score distribution, coefficient alpha may be sufficiently representative and a useful summary. But, if conditional reliability is heterogeneous across the score distribution, alpha may be unrepresentative and may not align with the reliability of a typical score in the data or with a particularly important score like a cut point where decisions are made. This paper proposes a method, R package, and Shiny application to quantify the potential differences between coefficient alpha and conditional reliability across the score distribution. The goal is to facilitate comparisons between conditional reliability and reliability summary indices so that psychologists can contextualize the reliability of their scores more clearly and comprehensively.},
	language = {en},
	number = {3},
	urldate = {2025-03-12},
	journal = {Behavior Research Methods},
	author = {McNeish, Daniel and Dumas, Denis},
	month = feb,
	year = {2025},
	keywords = {Reliability, Coefficient alpha, Conditional reliability, Cronbach's alpha, Omega},
	pages = {93},
	file = {Full Text PDF:/Users/magnuspjo/Zotero/storage/KLZX8ZIQ/McNeish and Dumas - 2025 - Reliability representativeness How well does coefficient alpha summarize reliability across the sco.pdf:application/pdf},
}

@article{wu_role_2005,
	title = {The role of plausible values in large-scale surveys},
	volume = {31},
	copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
	issn = {0191491X},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0191491X05000209},
	doi = {10.1016/j.stueduc.2005.05.005},
	abstract = {In large-scale assessment programs such as NAEP, TIMSS and PISA, students' achievement data sets provided for secondary analysts contain so-called plausible values. Plausible values are multiple imputations of the unobservable latent achievement for each student. In this article it has been shown how plausible values are used to: (1) address concerns with bias in the estimation of certain population parameters when point estimates of latent achievement are used to estimate those population parameters; (2) allow secondary data analysts to employ standard techniques and tools (e.g., SPSS, SAS procedures) to analyse achievement data that contains substantial measurement error components; and (3) facilitate the conlputation or" standard errors of estimates when the sample design is complex. The advantages of plausible values have been illustrated by comparing the use of maximum likelihood estimates and plausible values (PV) for estimating a range of population statistics.},
	language = {en},
	number = {2-3},
	urldate = {2025-03-05},
	journal = {Studies in Educational Evaluation},
	author = {Wu, Margaret},
	month = jan,
	year = {2005},
	pages = {114--128},
}

@article{mislevy_randomization_based_1991,
	title = {Randomization-{Based} {Inference} about {Latent} {Variables} from {Complex} {Samples}},
	volume = {56},
	issn = {0033-3123, 1860-0980},
	url = {https://www.cambridge.org/core/journals/psychometrika/article/abs/randomizationbased-inference-about-latent-variables-from-complex-samples/E981F9B78896C2D3567863944CCA8D83},
	doi = {10.1007/BF02294457},
	abstract = {Standard procedures for drawing inferences from complex samples do not apply when the variable of interest θ cannot be observed directly, but must be inferred from the values of secondary random variables that depend on θ stochastically. Examples are proficiency variables in item response models and class memberships in latent class models. Rubin's “multiple imputation” techniques yield approximations of sample statistics that would have been obtained, had θ been observable, and associated variance estimates that account for uncertainty due to both the sampling of respondents and the latent nature of θ. The approach is illustrated with data from the National Assessment for Educational Progress.},
	language = {en},
	number = {2},
	urldate = {2025-10-10},
	journal = {Psychometrika},
	author = {Mislevy, Robert J.},
	month = jun,
	year = {1991},
	keywords = {complex samples, item response theory, latent structure, missing data, multiple imputation, National Assessment of Educational Progress, sample surveys},
	pages = {177--196},
}