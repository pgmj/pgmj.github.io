

@article{chalmers_it_2016,
	title = {It {Might} {Not} {Make} a {Big} {DIF}: {Improved} {Differential} {Test} {Functioning} {Statistics} {That} {Account} for {Sampling} {Variability}},
	volume = {76},
	issn = {0013-1644},
	shorttitle = {It {Might} {Not} {Make} a {Big} {DIF}},
	url = {https://doi.org/10.1177/0013164415584576},
	doi = {10.1177/0013164415584576},
	abstract = {Differential test functioning, or DTF, occurs when one or more items in a test demonstrate differential item functioning (DIF) and the aggregate of these effects are witnessed at the test level. In many applications, DTF can be more important than DIF when the overall effects of DIF at the test level can be quantified. However, optimal statistical methodology for detecting and understanding DTF has not been developed. This article proposes improved DTF statistics that properly account for sampling variability in item parameter estimates while avoiding the necessity of predicting provisional latent trait estimates to create two-step approximations. The properties of the DTF statistics were examined with two Monte Carlo simulation studies using dichotomous and polytomous IRT models. The simulation results revealed that the improved DTF statistics obtained optimal and consistent statistical properties, such as obtaining consistent Type I error rates. Next, an empirical analysis demonstrated the application of the proposed methodology. Applied settings where the DTF statistics can be beneficial are suggested and future DTF research areas are proposed.},
	language = {EN},
	number = {1},
	urldate = {2025-10-19},
	journal = {Educational and Psychological Measurement},
	author = {Chalmers, R. Philip and Counsell, Alyssa and Flora, David B.},
	month = feb,
	year = {2016},
	note = {Publisher: SAGE Publications Inc},
	pages = {114--140},
}


@article{thissen_review_2025,
	title = {A {Review} of {Some} of the {History} of {Factorial} {Invariance} and {Differential} {Item} {Functioning}},
	volume = {60},
	issn = {0027-3171},
	url = {https://doi.org/10.1080/00273171.2024.2396148},
	doi = {10.1080/00273171.2024.2396148},
	abstract = {The concept of factorial invariance has evolved since it originated in the 1930s as a criterion for the usefulness of the multiple factor model; it has become a form of analysis supporting the validity of inferences about group differences on underlying latent variables. The analysis of differential item functioning (DIF) arose in the literature of item response theory (IRT), where its original purpose was the detection and removal of test items that are differentially difficult for, or biased against, one subpopulation or another. The two traditions merge at the level of the underlying latent variable model, but their separate origins and different purposes have led them to differ in details of terminology and procedure. This review traces some aspects of the histories of the two traditions, ultimately drawing some conclusions about how analysts may draw on elements of both, and how the nature of the research question determines the procedures used. Whether statistical tests are grouped by parameter (as in studies of factorial invariance) or across parameters by variable (as in DIF analysis) depends on the context and is independent of the model, as are subtle aspects of the order of the tests. In any case in which DIF or partial invariance is a possibility, the invariant parameters, or anchor items in DIF analysis, are best selected in an interplay between the statistics and judgment about what is being measured.},
	number = {2},
	urldate = {2025-10-20},
	journal = {Multivariate Behavioral Research},
	author = {Thissen, David},
	month = mar,
	year = {2025},
	pmid = {39264323},
	note = {Publisher: Routledge
\_eprint: https://doi.org/10.1080/00273171.2024.2396148},
	keywords = {DIF, differential item functioning, factor analysis, Factorial invariance, item response theory},
	pages = {211--235},
}


@article{kreiner_specific_2025,
	title = {On {Specific} {Objectivity} and {Measurement} by {Rasch} {Models}: {A} {Statistical} {Viewpoint}},
	volume = {3},
	url = {https://dx.doi.org/10.61186/emp.2025.7},
	doi = {10.61186/emp.2025.7},
	abstract = {Rasch’s original definition of specific objectivity insisted that measurement by items of educational tests had to be provided by estimates of person parameters of a measurement model that did not involve estimates of item parameters. It was never stated clearly, but there is no doubt that Rasch was thinking about conditional maximum likelihood (CML) estimates similar to the well-known CML estimates of item parameters. However, for technical reasons, there are only few examples of attempts to implement the CML estimates of person parameters in software applications for item analysis by Rasch models. Nevertheless, today CML estimation of person parameters in Rasch models is possible. And since the exact distribution of estimates of person parameters in Rasch models is known, it is possible to compare the performance of the specific objective CML estimates of person parameters to the performances of the other estimates of the person parameters. This paper provides one example of such an exercise. It compares five different estimators of the person parameters of a Rasch model with forty dichotomous items for a Danish cognitive test. In this paper, the point of view on the measurement quality is purely statistical. The superior measure of the latent trait has to have ignorable bias and as little standard error of measurement as possible. In this example, it turns out that the specific objective CML estimate is not the superior estimate. This motivates a slightly weaker notion of essential specific objectivity than many other estimates of person parameters in Rasch models live up to.},
	language = {en},
	journal = {Educational Methods \& Psychometrics},
	author = {Kreiner, Svend},
	year = {2025},
}


@article{mcneish_thanks_2018,
	title = {Thanks coefficient alpha, we’ll take it from here.},
	volume = {23},
	issn = {1939-1463, 1082-989X},
	url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/met0000144},
	doi = {10.1037/met0000144},
	abstract = {Empirical studies in psychology commonly report Cronbach’s alpha as a measure of internal consistency reliability despite the fact that many methodological studies have shown that Cronbach’s alpha is riddled with problems stemming from unrealistic assumptions. In many circumstances, violating these assumptions yields estimates of reliability that are too small, making measures look less reliable than they actually are. Although methodological critiques of Cronbach’s alpha are being cited with increasing frequency in empirical studies, in this tutorial we discuss how the trend is not necessarily improving methodology used in the literature. That is, many studies continue to use Cronbach’s alpha without regard for its assumptions or merely cite methodological articles advising against its use to rationalize unfavorable Cronbach’s alpha estimates. This tutorial first provides evidence that recommendations against Cronbach’s alpha have not appreciably changed how empirical studies report reliability. Then, we summarize the drawbacks of Cronbach’s alpha conceptually without relying on mathematical or simulation-based arguments so that these arguments are accessible to a broad audience. We continue by discussing several alternative measures that make less rigid assumptions which provide justifiably higher estimates of reliability compared to Cronbach’s alpha. We conclude with empirical examples to illustrate advantages of alternative measures of reliability including omega total, Revelle’s omega total, the greatest lower bound, and Coefficient H. A detailed software appendix is also provided to help researchers implement alternative methods.},
	language = {en},
	number = {3},
	urldate = {2022-06-02},
	journal = {Psychological Methods},
	author = {McNeish, Daniel},
	month = sep,
	year = {2018},
	pages = {412--433},
}

@article{sijtsma_use_2008,
	title = {On the {Use}, the {Misuse}, and the {Very} {Limited} {Usefulness} of {Cronbach}’s {Alpha}},
	volume = {74},
	issn = {1860-0980},
	url = {https://doi.org/10.1007/s11336-008-9101-0},
	doi = {10.1007/s11336-008-9101-0},
	abstract = {This discussion paper argues that both the use of Cronbach’s alpha as a reliability estimate and as a measure of internal consistency suffer from major problems. First, alpha always has a value, which cannot be equal to the test score’s reliability given the interitem covariance matrix and the usual assumptions about measurement error. Second, in practice, alpha is used more often as a measure of the test’s internal consistency than as an estimate of reliability. However, it can be shown easily that alpha is unrelated to the internal structure of the test. It is further discussed that statistics based on a single test administration do not convey much information about the accuracy of individuals’ test performance. The paper ends with a list of conclusions about the usefulness of alpha.},
	language = {en},
	number = {1},
	urldate = {2022-11-29},
	journal = {Psychometrika},
	author = {Sijtsma, Klaas},
	month = dec,
	year = {2008},
	keywords = {reliability, unidimensionality, internal consistency, Cronbach’s alpha},
	pages = {107},
}


@article{wu_role_2005,
	title = {The role of plausible values in large-scale surveys},
	volume = {31},
	copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
	issn = {0191491X},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0191491X05000209},
	doi = {10.1016/j.stueduc.2005.05.005},
	abstract = {In large-scale assessment programs such as NAEP, TIMSS and PISA, students' achievement data sets provided for secondary analysts contain so-called plausible values. Plausible values are multiple imputations of the unobservable latent achievement for each student. In this article it has been shown how plausible values are used to: (1) address concerns with bias in the estimation of certain population parameters when point estimates of latent achievement are used to estimate those population parameters; (2) allow secondary data analysts to employ standard techniques and tools (e.g., SPSS, SAS procedures) to analyse achievement data that contains substantial measurement error components; and (3) facilitate the conlputation or" standard errors of estimates when the sample design is complex. The advantages of plausible values have been illustrated by comparing the use of maximum likelihood estimates and plausible values (PV) for estimating a range of population statistics.},
	language = {en},
	number = {2-3},
	urldate = {2025-03-05},
	journal = {Studies in Educational Evaluation},
	author = {Wu, Margaret},
	month = jan,
	year = {2005},
	pages = {114--128},
}

@article{mislevy_randomization-based_1991,
	title = {Randomization-{Based} {Inference} about {Latent} {Variables} from {Complex} {Samples}},
	volume = {56},
	issn = {0033-3123, 1860-0980},
	url = {https://www.cambridge.org/core/journals/psychometrika/article/abs/randomizationbased-inference-about-latent-variables-from-complex-samples/E981F9B78896C2D3567863944CCA8D83},
	doi = {10.1007/BF02294457},
	abstract = {Standard procedures for drawing inferences from complex samples do not apply when the variable of interest θ cannot be observed directly, but must be inferred from the values of secondary random variables that depend on θ stochastically. Examples are proficiency variables in item response models and class memberships in latent class models. Rubin's “multiple imputation” techniques yield approximations of sample statistics that would have been obtained, had θ been observable, and associated variance estimates that account for uncertainty due to both the sampling of respondents and the latent nature of θ. The approach is illustrated with data from the National Assessment for Educational Progress.},
	language = {en},
	number = {2},
	urldate = {2025-10-10},
	journal = {Psychometrika},
	author = {Mislevy, Robert J.},
	month = jun,
	year = {1991},
	keywords = {complex samples, item response theory, latent structure, missing data, multiple imputation, National Assessment of Educational Progress, sample surveys},
	pages = {177--196},
}

@article{mcneish_reliability_2025,
	title = {Reliability representativeness: {How} well does coefficient alpha summarize reliability across the score distribution?},
	volume = {57},
	issn = {1554-3528},
	shorttitle = {Reliability representativeness},
	url = {https://doi.org/10.3758/s13428-025-02611-8},
	doi = {10.3758/s13428-025-02611-8},
	abstract = {Scale scores in psychology studies are commonly accompanied by a reliability coefficient like alpha. Coefficient alpha is an index that summarizes reliability across the entire score distribution, implying equal precision for all scores. However, an underappreciated fact is that reliability can be conditional such that scores in certain parts of the score distribution may be more reliable than others. This conditional perspective of reliability is common in item response theory (IRT), but psychologists are generally not well versed in IRT. Correspondingly, the representativeness of a single summary index like alpha across the entire score distribution can be unclear but is rarely considered. If conditional reliability is fairly homogeneous across the score distribution, coefficient alpha may be sufficiently representative and a useful summary. But, if conditional reliability is heterogeneous across the score distribution, alpha may be unrepresentative and may not align with the reliability of a typical score in the data or with a particularly important score like a cut point where decisions are made. This paper proposes a method, R package, and Shiny application to quantify the potential differences between coefficient alpha and conditional reliability across the score distribution. The goal is to facilitate comparisons between conditional reliability and reliability summary indices so that psychologists can contextualize the reliability of their scores more clearly and comprehensively.},
	language = {en},
	number = {3},
	urldate = {2025-03-12},
	journal = {Behavior Research Methods},
	author = {McNeish, Daniel and Dumas, Denis},
	month = feb,
	year = {2025},
	keywords = {Reliability, Coefficient alpha, Conditional reliability, Cronbach's alpha, Omega},
	pages = {93},
	file = {Full Text PDF:/Users/magnuspjo/Zotero/storage/KLZX8ZIQ/McNeish and Dumas - 2025 - Reliability representativeness How well does coefficient alpha summarize reliability across the sco.pdf:application/pdf},
}

@misc{bignardi_general_2025,
	title = {A general method for estimating reliability using {Bayesian} {Measurement} {Uncertainty}},
	url = {https://osf.io/h54k8_v1},
	doi = {10.31234/osf.io/h54k8_v1},
	abstract = {Unreliable measurement can lead to lower statistical power, attenuated effect sizes and residual confounding. However, estimating reliability can be challenging for complex cognitive and behavioural assessments without test-retest data. Most statistical methods for assessing reliability with a single test administration are designed for fixed-item questionnaires. We introduce a novel Bayesian procedure called relative measurement uncertainty (RMU) for estimating reliability that can be broadly applied, including to many widely used computational models. Our approach draws pairs of samples from each subject's posterior and calculates the correlation between draws. We demonstrate analytically and via simulation that this method provides accurate reliability estimates and well-calibrated credible intervals across linear factor, signal detection and reinforcement learning models. Simulations found that RMU was more accurate (lower root mean square error) than existing measures (coefficient alpha, coefficient H, split-half), but more importantly, it can be applied across a wide range of computational models. RMU offers a general method for estimating reliability, leveraging the modelling flexibility of Bayesian statistical methods.},
	language = {en-us},
	urldate = {2025-10-01},
	publisher = {OSF},
	author = {Bignardi, Giacomo and Kievit, Rogier and Bürkner, Paul-Christian},
	month = sep,
	year = {2025},
	file = {PDF:/Users/magnuspjo/Zotero/storage/BB5CRHVM/Bignardi et al. - 2025 - A general method for estimating reliability using Bayesian Measurement Uncertainty.pdf:application/pdf},
}

@article{milanzi_reliability_2015,
	title = {Reliability measures in item response theory: {Manifest} versus latent correlation functions},
	volume = {68},
	copyright = {© 2014 The British Psychological Society},
	issn = {2044-8317},
	shorttitle = {Reliability measures in item response theory},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/bmsp.12033},
	doi = {10.1111/bmsp.12033},
	abstract = {For item response theory (IRT) models, which belong to the class of generalized linear or non-linear mixed models, reliability at the scale of observed scores (i.e., manifest correlation) is more difficult to calculate than latent correlation based reliability, but usually of greater scientific interest. This is not least because it cannot be calculated explicitly when the logit link is used in conjunction with normal random effects. As such, approximations such as Fisher's information coefficient, Cronbach's α, or the latent correlation are calculated, allegedly because it is easy to do so. Cronbach's α has well-known and serious drawbacks, Fisher's information is not meaningful under certain circumstances, and there is an important but often overlooked difference between latent and manifest correlations. Here, manifest correlation refers to correlation between observed scores, while latent correlation refers to correlation between scores at the latent (e.g., logit or probit) scale. Thus, using one in place of the other can lead to erroneous conclusions. Taylor series based reliability measures, which are based on manifest correlation functions, are derived and a careful comparison of reliability measures based on latent correlations, Fisher's information, and exact reliability is carried out. The latent correlations are virtually always considerably higher than their manifest counterparts, Fisher's information measure shows no coherent behaviour (it is even negative in some cases), while the newly introduced Taylor series based approximations reflect the exact reliability very closely. Comparisons among the various types of correlations, for various IRT models, are made using algebraic expressions, Monte Carlo simulations, and data analysis. Given the light computational burden and the performance of Taylor series based reliability measures, their use is recommended.},
	language = {en},
	number = {1},
	urldate = {2025-10-10},
	journal = {British Journal of Mathematical and Statistical Psychology},
	author = {Milanzi, Elasma and Molenberghs, Geert and Alonso, Ariel and Verbeke, Geert and De Boeck, Paul},
	year = {2015},
	note = {\_eprint: https://bpspsychub.onlinelibrary.wiley.com/doi/pdf/10.1111/bmsp.12033},
	keywords = {logit link, one-parameter logistic model, probit link, Rasch model, two-parameter logistic model},
	pages = {43--64},
	file = {Full Text PDF:/Users/magnuspjo/Zotero/storage/5ZERDGUZ/Milanzi et al. - 2015 - Reliability measures in item response theory Manifest versus latent correlation functions.pdf:application/pdf;Snapshot:/Users/magnuspjo/Zotero/storage/SFWKTECM/bmsp.html:text/html},
}

@article{chen_local_1997,
	title = {Local {Dependence} {Indexes} for {Item} {Pairs} {Using} {Item} {Response} {Theory}},
	volume = {22},
	issn = {1076-9986},
	url = {https://www.jstor.org/stable/1165285},
	doi = {10.2307/1165285},
	abstract = {Four statistics are proposed for the detection of local dependence (LD) among items analyzed using item response theory. Among them, the X$^{\textrm{2}}$ and G$^{\textrm{2}}$ LD indexes are of special interest. Simulated data are used to study the distribution and sensitivity of these statistics under the null condition, as well as under conditions in which LD is introduced. The results show that under the null condition of local independence, both the X$^{\textrm{2}}$ and G$^{\textrm{2}}$ LD indexes have distributions very similar to the ? $^{\textrm{2}}$ distribution with 1 degree of freedom. Under the locally dependent conditions, both indexes appear to be sensitive in detecting LD or multidimensionality among items. When compared to Q$_{\textrm{3}}$, another statistic often used to detect LD, these new statistics are somewhat less powerful for underlying LD, equally powerful for surface LD, and better behaved in the null case.},
	number = {3},
	urldate = {2025-09-19},
	journal = {Journal of Educational and Behavioral Statistics},
	author = {Chen, Wen-Hung and Thissen, David},
	year = {1997},
	note = {Publisher: [American Educational Research Association, Sage Publications, Inc., American Statistical Association]},
	pages = {265--289},
	file = {PDF:/Users/magnuspjo/Zotero/storage/JATE6R7G/Chen and Thissen - 1997 - Local Dependence Indexes for Item Pairs Using Item Response Theory.pdf:application/pdf},
}

@article{edwards_diagnostic_2018,
	title = {A diagnostic procedure to detect departures from local independence in item response theory models},
	volume = {23},
	issn = {1939-1463},
	doi = {10.1037/met0000121},
	abstract = {Item response theory (IRT) is a widely used measurement model. When considering its use in education, health outcomes, and psychology, it is likely to be one of the most impactful psychometric models in existence. IRT has many advantages over classical test theory-based measurement models. For these advantages to hold in practice, strong assumptions must be satisfied. One of these assumptions, local independence, is the focus of the work described here. Local independence is the assumption that, conditional on the latent variable(s), item responses are unrelated to one another (i.e., independent). Stated another way, local independence implies that the only thing causing items to covary is the modeled latent variable(s). Violations of this assumption, quite aptly titled local dependence, can have serious consequences for the estimated parameters. A new diagnostic is proposed, based on parameter stability in an item-level jackknife resampling procedure. We review the ideas underlying the new diagnostic and how it is computed before covering some simulated and real examples demonstrating its effectiveness. (PsycInfo Database Record (c) 2020 APA, all rights reserved)},
	number = {1},
	journal = {Psychological Methods},
	author = {Edwards, Michael C. and Houts, Carrie R. and Cai, Li},
	year = {2018},
	note = {Place: US
Publisher: American Psychological Association},
	keywords = {Measurement, Psychometrics, Models, Item Response Theory, Statistical Estimation},
	pages = {138--149},
}

@article{johansson_detecting_2025,
	title = {Detecting {Item} {Misfit} in {Rasch} {Models}},
	volume = {3},
	url = {https://pgmj.github.io/rasch_itemfit/},
	doi = {10.61186/emp.2025.5},
	number = {18},
	urldate = {2025-03-11},
	journal = {Educational Methods \& Psychometrics},
	author = {Johansson, Magnus},
	year = {2025},
	file = {Full Text PDF:/Users/magnuspjo/Zotero/storage/NVHLS9SX/Johansson - 2025 - Detecting Item Misfit in Rasch Models.pdf:application/pdf},
}


@article{andersen_goodness_1973,
	title = {A goodness of fit test for the rasch model},
	volume = {38},
	issn = {1860-0980},
	url = {https://doi.org/10.1007/BF02291180},
	doi = {10.1007/BF02291180},
	abstract = {The Rasch model is an item analysis model with logistic item characteristic curves of equal slope,i.e. with constant item discriminating powers. The proposed goodness of fit test is based on a comparison between difficulties estimated from different scoregroups and over-all estimates.},
	language = {en},
	number = {1},
	urldate = {2025-01-11},
	journal = {Psychometrika},
	author = {Andersen, Erling B.},
	month = mar,
	year = {1973},
	keywords = {Asymptotic Covariance Matrix, Conditional Likelihood, Item Difficulty, Item Parameter, Likelihood Equation},
	pages = {123--140},
}

@article{smith_detecting_2002,
	title = {Detecting and evaluating the impact of multidimensionality using item fit statistics and principal component analysis of residuals},
	volume = {3},
	issn = {1529-7713},
	abstract = {The purpose of this research is twofold. First is to extend the work of Smith (1992, 1996) and Smith and Miao (1991, 1994) in comparing item fit statistics and principal component analysis as tools for assessing the unidimensionality requirement of Rasch models. Second is to demonstrate methods to explore how violations of the unidimensionality requirement influence person measurement. For the first study, rating scale data were simulated to represent varying degrees of multidimensionality and the proportion of items contributing to each component. The second study used responses to a 24 item Attention Deficit Hyperactivity Disorder scale obtained from 317 college undergraduates. The simulation study reveals both an iterative item fit approach and principal component analysis of standardized residuals are effective in detecting items simulated to contribute to multidimensionality. The methods presented in Study 2 demonstrate the potential impact of multidimensionality on norm and criterion-reference person measure interpretations. The results provide researchers with quantitative information to help assist with the qualitative judgment as to whether the impact of multidimensionality is severe enough to warrant removing items from the analysis.},
	language = {eng},
	number = {2},
	journal = {Journal of applied measurement},
	author = {Smith, Everett V},
	month = jan,
	year = {2002},
	pmid = {12011501},
	pages = {205--231},
	file = {Smith 2002.pdf:/Users/magnuspjo/Zotero/storage/8GJGJ58P/Smith - 2002 - Detecting and evaluating the impact of multidimensionality using item fit statistics and principal c.pdf:application/pdf},
}



@article{chou_checking_2010,
	title = {Checking {Dimensionality} in {Item} {Response} {Models} {With} {Principal} {Component} {Analysis} on {Standardized} {Residuals}},
	volume = {70},
	issn = {0013-1644},
	url = {https://doi.org/10.1177/0013164410379322},
	doi = {10.1177/0013164410379322},
	abstract = {Dimensionality is an important assumption in item response theory (IRT). Principal component analysis on standardized residuals has been used to check dimensionality, especially under the family of Rasch models. It has been suggested that an eigenvalue greater than 1.5 for the first eigenvalue signifies a violation of unidimensionality when there are 500 persons and 30 items. The cut-point of 1.5 is often used beyond this specific condition of sample size and test length. This study argues that a fixed cut-point is not applicable because the distribution of eigenvalues or their ratios depends on sample size and test length, just like other statistics. The authors conducted a series of simulations to verify this argument. They then proposed three chi-square statistics for multivariate independence to test the correlation matrix obtained from the standardized residuals. Through simulations, it was found that Steiger’s statistic behaved fairly like a chi-square distribution, when its degrees of freedom were adjusted.},
	language = {en},
	number = {5},
	urldate = {2025-01-10},
	journal = {Educational and Psychological Measurement},
	author = {Chou, Yeh-Tai and Wang, Wen-Chung},
	month = oct,
	year = {2010},
	note = {Publisher: SAGE Publications Inc},
	pages = {717--731},
	file = {SAGE PDF Full Text:/Users/magnuspjo/Zotero/storage/WTAEUBDM/Chou and Wang - 2010 - Checking Dimensionality in Item Response Models With Principal Component Analysis on Standardized Re.pdf:application/pdf},
}


@article{kreiner_analysis_2004,
	title = {Analysis of {Local} {Dependence} and {Multidimensionality} in {Graphical} {Loglinear} {Rasch} {Models}},
	volume = {33},
	issn = {0361-0926},
	url = {https://doi.org/10.1081/STA-120030148},
	doi = {10.1081/STA-120030148},
	abstract = {This paper proposes a procedure for analysis of multidimensionality in graphical loglinear Rasch models. The procedure combines exploratory techniques based on analysis of local dependence by coefficients measuring conditional item association and confirmatory techniques fitting and testing the adequacy of graphical loglinear Rasch models. The course of action is motivated by the observation that evidence of conditional item association suggesting multidimensionality may be generated by very different phenomena having nothing to do with multidimensionality. This means that an analysis of conditional association is never enough in itself: additional procedures are required to distinguish between multidimensionality and other causes of local dependence. The procedure for analysis of conditional item association may be regarded as a variation of well-known procedures for analysis of local dependence. Compared to these methods the techniques for the family of Rasch models described in this paper eliminate the bias inherent in conventional methods for analysis of conditional item association.},
	number = {6},
	journal = {Communications in Statistics - Theory and Methods},
	author = {Kreiner, Svend and Christensen, Karl Bang},
	month = dec,
	year = {2004},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1081/STA-120030148},
	keywords = {Differential item functioning, Graphical Rasch models, Local independence, Loglinear Rasch models, Multidimensionality, Uniform local dependence and DIF},
	pages = {1239--1276},
	file = {Full Text PDF:/Users/magnuspjo/Zotero/storage/C2UL65QT/Kreiner and Christensen - 2004 - Analysis of Local Dependence and Multidimensionali.pdf:application/pdf;Snapshot:/Users/magnuspjo/Zotero/storage/6PL6W227/STA-120030148.html:text/html},
}


@article{kreiner_note_2011,
	title = {A {Note} on {Item}–{Restscore} {Association} in {Rasch} {Models}},
	volume = {35},
	issn = {0146-6216},
	url = {https://doi.org/10.1177/0146621611410227},
	doi = {10.1177/0146621611410227},
	language = {en},
	number = {7},
	journal = {Applied Psychological Measurement},
	author = {Kreiner, Svend},
	month = oct,
	year = {2011},
	note = {Publisher: SAGE Publications Inc},
	pages = {557--561},
	file = {SAGE PDF Full Text:/Users/magnuspjo/Zotero/storage/WMCBQQ2S/Kreiner - 2011 - A Note on Item–Restscore Association in Rasch Mode.pdf:application/pdf},
}


@article{smith_using_1998,
	title = {Using item mean squares to evaluate fit to the {Rasch} model},
	volume = {2},
	issn = {1090-655X},
	abstract = {Throughout the mid to late 1970's considerable research was conducted on the properties of Rasch fit mean squares. This work culminated in a variety of transformations to convert the mean squares into approximate t-statistics. This work was primarily motivated by the influence sample size has on the magnitude of the mean squares and the desire to have a single critical value that can generally be applied to most cases. In the late 1980's and the early 1990's the trend seems to have reversed, with numerous researchers using the untransformed fit mean squares as a means of testing fit to the Rasch measurement models. The principal motivation is cited as the influence sample size has on the sensitivity of the t-converted mean squares. The purpose of this paper is to present the historical development of these fit indices and the various transformations and to examine the impact of sample size on both the fit mean squares and the t-transformations of those mean squares. Because the sample size problem has little influence on the person mean square problem, due to the relatively short length (100 items or less), this paper focuses on the item fit mean squares, where it is common to find the statistics used with sample sizes ranging from 30 to 10,000.},
	language = {eng},
	number = {1},
	journal = {Journal of Outcome Measurement},
	author = {Smith, R. M. and Schumacker, R. E. and Bush, M. J.},
	year = {1998},
	pmid = {9661732},
	keywords = {Computer Simulation, Humans, Models, Statistical, Psychometrics, Reference Standards, Statistics as Topic},
	pages = {66--78},
}

@article{bjorner_differential_1998,
	title = {Differential {Item} {Functioning} in the {Danish} {Translation} of the {SF}-36},
	volume = {51},
	issn = {0895-4356},
	url = {https://www.sciencedirect.com/science/article/pii/S0895435698001115},
	doi = {10.1016/S0895-4356(98)00111-5},
	abstract = {Statistical analyses of Differential Item Functioning (DIF) can be used for rigorous translation evaluations. DIF techniques test whether each item functions in the same way, irrespective of the country, language, or culture of the respondents. For a given level of health, the score on any item should be independent of nationality. This requirement can be tested through contingency-table methods, which are efficient for analyzing all types of items. We investigated DIF in the Danish translation of the SF-36 Health Survey, using two general population samples (USA, n = 1,506; Denmark, n = 3,950). DIF was identified for 12 out of 35 items. These results agreed with independent ratings of translation quality, but the statistical techniques were more sensitive. When included in scales, the items exhibiting DIF had only a little impact on conclusions about cross-national differences in health in the general population. However, if used as single items, the DIF items could seriously bias results from cross-national comparisons. Also, the DIF items might have larger impact on cross-national comparison of groups with poorer health status. We conclude that analysis of DIF is useful for evaluating questionnaire translations.},
	number = {11},
	urldate = {2024-08-22},
	journal = {Journal of Clinical Epidemiology},
	author = {Bjorner, Jakob B. and Kreiner, Svend and Ware, John E. and Damsgaard, Mogens T. and Bech, Per},
	month = nov,
	year = {1998},
	keywords = {cross-cultural comparison, Differential Item Functioning, Health status indicators, psychometrics, questionnaires, SF-36 Health Survey, statistical methods, translations},
	pages = {1189--1202}
}

@article{smith_rasch_2008,
	title = {Rasch fit statistics and sample size considerations for polytomous data},
	volume = {8},
	issn = {1471-2288},
	url = {https://doi.org/10.1186/1471-2288-8-33},
	doi = {10.1186/1471-2288-8-33},
	abstract = {Previous research on educational data has demonstrated that Rasch fit statistics (mean squares and t-statistics) are highly susceptible to sample size variation for dichotomously scored rating data, although little is known about this relationship for polytomous data. These statistics help inform researchers about how well items fit to a unidimensional latent trait, and are an important adjunct to modern psychometrics. Given the increasing use of Rasch models in health research the purpose of this study was therefore to explore the relationship between fit statistics and sample size for polytomous data.},
	number = {1},
	urldate = {2020-10-25},
	journal = {BMC Medical Research Methodology},
	author = {Smith, Adam B. and Rush, Robert and Fallowfield, Lesley J. and Velikova, Galina and Sharpe, Michael},
	month = may,
	year = {2008},
	pages = {33},
	file = {Full Text:/Users/magnuspjo/Zotero/storage/UELYDK5X/Smith et al. - 2008 - Rasch fit statistics and sample size consideration.pdf:application/pdf;Snapshot:/Users/magnuspjo/Zotero/storage/CPXU8JKE/1471-2288-8-33.html:text/html},
}

@misc{mueller_iarm_2022,
	title = {iarm: {Item} {Analysis} in {Rasch} {Models}},
	copyright = {GPL-2},
	shorttitle = {iarm},
	url = {https://cran.r-project.org/web/packages/iarm/index.html},
	abstract = {Tools to assess model fit and identify misfitting items for Rasch models (RM) and partial credit models (PCM). Included are item fit statistics, item characteristic curves, item-restscore association, conditional likelihood ratio tests, assessment of measurement error, estimates of the reliability and test targeting as described in Christensen et al. (Eds.) (2013, ISBN:978-1-84821-222-0).},
	author = {Mueller, Marianne and Santiago, Pedro Henrique Ribeiro},
	month = aug,
	year = {2022},
	keywords = {Psychometrics},
}

@article{buchardt_visualizing_2023,
	title = {Visualizing {Rasch} item fit using conditional item characteristic curves in {R}},
	volume = {65},
	abstract = {New R computer routines that support rigorous statistical validation of psychological tests have recently appeared. We illustrate how Rasch item fit can be evaluated visually and propose an extension of existing implementations in R. We illustrate the utility using two short psychological tests.},
	language = {en},
	number = {2},
	journal = {Psychological Test and Assessment Modeling},
	author = {Buchardt, Ann-Sophie and Christensen, Karl Bang and Jensen, Normann},
	year = {2023},
	pages = {206--219}
}

@article{muller_item_2020,
	title = {Item fit statistics for {Rasch} analysis: can we trust them?},
	volume = {7},
	issn = {2195-5832},
	shorttitle = {Item fit statistics for {Rasch} analysis},
	url = {https://doi.org/10.1186/s40488-020-00108-7},
	doi = {10.1186/s40488-020-00108-7},
	number = {1},
	urldate = {2020-10-19},
	journal = {Journal of Statistical Distributions and Applications},
	author = {Müller, Marianne},
	month = aug,
	year = {2020},
	pages = {5}
}


@article{artner_simulation_2016,
	title = {A simulation study of person-fit in the {Rasch} model},
	volume = {58},
	language = {en},
	number = {3},
	journal = {Psychological Test and Assessment Modeling},
	author = {Artner, Richard},
	year = {2016},
	pages = {531--563}
}


@article{choi_lordif_2011,
	title = {lordif: {An} {R} {Package} for {Detecting} {Differential} {Item} {Functioning} {Using} {Iterative} {Hybrid} {Ordinal} {Logistic} {Regression}/{Item} {Response} {Theory} and {Monte} {Carlo} {Simulations}},
	volume = {39},
	copyright = {Copyright (c) 2010 Seung W. Choi, Laura E. Gibbons, Paul K. Crane},
	issn = {1548-7660},
	shorttitle = {lordif},
	url = {https://doi.org/10.18637/jss.v039.i08},
	doi = {10.18637/jss.v039.i08},
	language = {en},
	number = {1},
	urldate = {2019-04-19},
	journal = {Journal of Statistical Software},
	author = {Choi, Seung W. and Gibbons, Laura E. and Crane, Paul K.},
	month = mar,
	year = {2011},
	pages = {1--30}
}


@article{emons_nonparametric_2008,
	title = {Nonparametric {Person}-{Fit} {Analysis} of {Polytomous} {Item} {Scores}},
	volume = {32},
	issn = {0146-6216},
	url = {https://doi.org/10.1177/0146621607302479},
	doi = {10.1177/0146621607302479},
	language = {en},
	number = {3},
	urldate = {2024-02-28},
	journal = {Applied Psychological Measurement},
	author = {Emons, Wilco H. M.},
	month = may,
	year = {2008},
	note = {Publisher: SAGE Publications Inc},
	pages = {224--247}
}


@misc{henninger_partial_2024,
	title = {Partial credit trees meet the partial gamma coefficient for quantifying {DIF} and {DSF} in polytomous items},
	url = {https://osf.io/47sah},
	doi = {10.31234/osf.io/47sah},
	language = {en-us},
	urldate = {2024-05-29},
	publisher = {OSF},
	author = {Henninger, Mirka and Radek, Jan and Sengewald, Marie-Ann and Strobl, Carolin},
	month = may,
	year = {2024},
	keywords = {differential item functioning, differential step functioning, machine learning, model-based recursive partitioning, partial credit model, psychometrics}
}

@article{anvari2022,
	title = {Bias in Self-Reports: An Initial Elevation Phenomenon},
	author = {Anvari, Farid and {Efendi{\'{c}}}, Emir and Olsen, Jerome and Arslan, Ruben C. and Elson, Malte and Schneider, Iris K.},
	year = {2022},
	month = {10},
	date = {2022-10-07},
	journal = {Social Psychological and Personality Science},
	pages = {19485506221129160},
	doi = {10.1177/19485506221129160},
	url = {https://doi.org/10.1177/19485506221129160},
	note = {Publisher: SAGE Publications Inc},
	langid = {en}
}

@article{hagell2016,
	title = {Sample Size and Statistical Conclusions from Tests of Fit to the Rasch Model According to the Rasch Unidimensional Measurement Model (RUMM) Program in Health Outcome Measurement},
	author = {Hagell, Peter and Westergren, Albert},
	year = {2016},
	date = {2016},
	journal = {Journal of Applied Measurement},
	pages = {416--431},
	volume = {17},
	number = {4}
}

@article{christensen2017,
	title = {Critical Values for Yen{\textquoteright}s Q3: Identification of Local Dependence in the Rasch Model Using Residual Correlations},
	author = {Christensen, Karl Bang and Makransky, Guido and Horton, Mike},
	year = {2017},
	month = {05},
	date = {2017-05-01},
	journal = {Applied Psychological Measurement},
	pages = {178--194},
	volume = {41},
	number = {3},
	doi = {10.1177/0146621616677520},
	url = {https://doi.org/10.1177/0146621616677520},
	note = {Publisher: SAGE Publications Inc},
	langid = {en}
}

@article{johansson,
	title = {Valid and Reliable? Basic and Expanded Recommendations for Psychometric Reporting and Quality Assessment.},
	author = {Johansson, Magnus and Preuter, Marit and Karlsson, Simon and {Möllerberg}, Marie-Louise and Svensson, Hanna and Melin, Jeanette},
	year = {2023},
	doi = {10.31219/osf.io/3htzc},
	langid = {en-us}
}

@article{warm1989,
	title = {Weighted likelihood estimation of ability in item response theory},
	author = {Warm, Thomas A.},
	year = {1989},
	month = {09},
	date = {1989-09-01},
	journal = {Psychometrika},
	pages = {427--450},
	volume = {54},
	number = {3},
	doi = {10.1007/BF02294627},
	url = {https://doi.org/10.1007/BF02294627},
	langid = {en}
}

@article{strobl2015,
	title = {Rasch Trees: A New Method for Detecting Differential Item Functioning in the Rasch Model},
	author = {Strobl, Carolin and Kopf, Julia and Zeileis, Achim},
	year = {2015},
	month = {06},
	date = {2015-06-01},
	journal = {Psychometrika},
	pages = {289--316},
	volume = {80},
	number = {2},
	doi = {10.1007/s11336-013-9388-3},
	url = {https://doi.org/10.1007/s11336-013-9388-3},
	langid = {en}
}

@article{strobl2021,
	title = {Using the raschtree function for detecting differential item functioning in the Rasch model},
	author = {Strobl, Carolin and Schneider, Lennart and Kopf, Julia and Zeileis, Achim},
	year = {2021},
	date = {2021},
	pages = {12},
	langid = {en}
}


@book{ostini_polytomous_2006,
	address = {2455 Teller Road, Thousand Oaks California 91320 United States of America},
	title = {Polytomous {Item} {Response} {Theory} {Models}},
	isbn = {978-0-7619-3068-6 978-1-4129-8541-3},
	url = {https://methods.sagepub.com/book/polytomous-item-response-theory-models},
	language = {en},
	urldate = {2024-08-19},
	publisher = {SAGE Publications, Inc.},
	author = {Ostini, Remo and Nering, Michael},
	year = {2006},
	doi = {10.4135/9781412985413}
}

