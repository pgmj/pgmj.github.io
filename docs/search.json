[
  {
    "objectID": "raschrvignette/RaschRvign.html",
    "href": "raschrvignette/RaschRvign.html",
    "title": "easyRasch vignette",
    "section": "",
    "text": "This is an introduction to using the easyRasch R package. A changelog for package updates is available here.\nDetails on package installation are available at the package GitHub page.\nIf you are new to Rasch Measurement Theory, you may find this intro presentation useful: https://pgmj.github.io/RaschIRTlecture/slides.html\nThis vignette will walk through a sample analysis using an open dataset with polytomous questionnaire data. This will include some data wrangling to structure the item data and itemlabels, then provide examples of the different functions. The full source code of this document can be found either in this repository or by clicking on &lt;/&gt; CODE at the top of this page. You should be able to use the source code “as is” and reproduce this document locally, as long as you have the required packages installed. This page and this website are built using the open source publishing tool Quarto.\nOne of the aims with this package is to simplify reproducible psychometric analysis to shed light on the measurement properties of a scale, questionnaire or test. In a paper recently made available as a preprint (Johansson et al., 2023), our research group propose that the basic aspects of a psychometric analysis should include information about:\nWe’ll include several ways to investigate these measurement properties, using Rasch Measurement Theory. There are also functions in the package less directly related to the criteria above, that will be demonstrated in this vignette.\nPlease note that this is just a sample analysis to showcase the R package. It is not intended as a “best practice” psychometric analysis example.\nYou can skip ahead to the Rasch analysis part in Section 3 if you are eager to look at the package output :)\nThere is a separate GitHub repository containing a template R-project to simplify using the easyRasch package when conducting a reproducible Rasch analysis in R: https://github.com/pgmj/RISEraschTemplate"
  },
  {
    "objectID": "raschrvignette/RaschRvign.html#getting-started",
    "href": "raschrvignette/RaschRvign.html#getting-started",
    "title": "easyRasch vignette",
    "section": "\n1 Getting started",
    "text": "1 Getting started\nSince the package is intended for use with Quarto, this vignette has also been created with Quarto. A “template” .qmd file is available that can be useful to have handy for copy&paste when running a new analysis. You can also download a complete copy of the Quarto/R code to produce this document here.\nLoading the easyRasch package will automatically load all the packages it depends on. However, it could be desirable to explicitly load all packages used, to simplify the automatic creation of citations for them, using the grateful package (see Section 12).\n\nCodelibrary(easyRasch) # devtools::install_github(\"pgmj/easyRasch\", dependencies = TRUE)\nlibrary(grateful)\nlibrary(ggrepel)\nlibrary(car)\nlibrary(kableExtra)\nlibrary(readxl)\nlibrary(tidyverse)\nlibrary(eRm)\nlibrary(iarm)\nlibrary(mirt)\nlibrary(psych)\nlibrary(ggplot2)\nlibrary(psychotree)\nlibrary(matrixStats)\nlibrary(reshape)\nlibrary(knitr)\nlibrary(patchwork)\nlibrary(formattable) \nlibrary(glue)\nlibrary(foreach)\n\n### some commands exist in multiple packages, here we define preferred ones that are frequently used\nselect &lt;- dplyr::select\ncount &lt;- dplyr::count\nrecode &lt;- car::recode\nrename &lt;- dplyr::rename\n\n\n\n\n\n\n\n\nNoteNote\n\n\n\nQuarto automatically adds links to R packages and functions throughout this document. However, this feature only works properly for packages available on CRAN. Since the easyRasch package is not on CRAN the links related to functions starting with RI will not work.\n\n\n\n1.1 Loading data\nWe will use data from a recent paper investigating the “initial elevation effect” (Anvari et al., 2022), and focus on the 10 negative items from the PANAS. The data is available at the OSF website.\n\nCodedf.all &lt;- read_csv(\"https://osf.io/download/6fbr5/\")\n# if you have issues with the link, please try downloading manually using the same URL as above\n# and read the file from your local drive.\n\n# subset items and demographic variables\ndf &lt;- df.all %&gt;% \n  select(starts_with(\"PANASD2_1\"),\n         starts_with(\"PANASD2_20\"),\n         age,Sex,Group) %&gt;% \n  select(!PANASD2_10_Active) %&gt;% \n  select(!PANASD2_1_Attentive)\n\n\nThe glimpse() function provides a quick overview of our dataframe.\n\nCodeglimpse(df)\n\nRows: 1,856\nColumns: 13\n$ PANASD2_11_Distressed &lt;dbl&gt; 2, 2, 2, 1, 2, 2, 4, 1, 1, 3, 1, 4, 2, 4, 4, 1, …\n$ PANASD2_12_Upset      &lt;dbl&gt; 1, 1, 4, 1, 1, 5, 2, 1, 2, 2, 2, 3, 1, 3, 5, 1, …\n$ PANASD2_13_Hostile    &lt;dbl&gt; 1, 1, 2, 1, 1, 3, 1, 1, 1, 4, 1, 1, 1, 1, 1, 1, …\n$ PANASD2_14_Irritable  &lt;dbl&gt; 1, 1, 3, 1, 2, 5, 3, 1, 2, 4, 2, 3, 1, 2, 3, 1, …\n$ PANASD2_15_Scared     &lt;dbl&gt; 1, 1, 3, 1, 1, 4, 1, 1, 1, 2, 2, 2, 1, 4, 4, 1, …\n$ PANASD2_16_Afraid     &lt;dbl&gt; 1, 1, 4, 1, 1, 3, 1, 1, 1, 3, 1, 2, 1, 4, 4, 1, …\n$ PANASD2_17_Ashamed    &lt;dbl&gt; 1, 1, 2, 1, 1, 3, 1, 1, 1, 2, 1, 4, 1, 1, 3, 1, …\n$ PANASD2_18_Guilty     &lt;dbl&gt; 2, 1, 2, 1, 1, 3, 3, 1, 1, 3, 1, 4, 1, 1, 3, 1, …\n$ PANASD2_19_Nervous    &lt;dbl&gt; 1, 1, 2, 1, 2, 4, 4, 1, 1, 4, 2, 4, 2, 1, 5, 1, …\n$ PANASD2_20_Jittery    &lt;dbl&gt; 1, 2, 3, 1, 1, 2, 3, 3, 2, 1, 2, 2, 1, 1, 4, 1, …\n$ age                   &lt;dbl&gt; 27, 32, 21, 27, 20, 22, 23, 25, 21, 26, 38, 36, …\n$ Sex                   &lt;chr&gt; \"Male\", \"Male\", \"Female\", \"Male\", \"Male\", \"Male\"…\n$ Group                 &lt;chr&gt; \"Later Start\", \"Later Start\", \"Later Start\", \"La…\n\n\nWe have 1856 rows, ie. respondents. All variables except Sex and Group are of class dbl, which means they are numeric and can have decimals. Integer (numeric with no decimals) would also be fine for our purposes. The two demographic variables currently of class chr (character) will need to be converted to factors (fct), and we will do that later on.\n(If you import a dataset where item variables are of class character, you will need to recode to numeric.)\n\n1.2 Itemlabels\nThen we set up the itemlabels dataframe. This could also be done using the free LibreOffice Calc or MS Excel. Just make sure the file has the same structure, with two variables named itemnr and item that contain the item variable names and item description. The item variable names have to match the variable names in the item dataframe.\n\nCodeitemlabels &lt;- df %&gt;% \n  select(starts_with(\"PAN\")) %&gt;% \n  names() %&gt;% \n  as_tibble() %&gt;% \n  separate(value, c(NA, \"item\"), sep =\"_[0-9][0-9]_\") %&gt;% \n  mutate(itemnr = paste0(\"PANAS_\",c(11:20)), .before = \"item\")\n\n\nThe itemlabels dataframe looks like this.\n\nCodeitemlabels\n\n# A tibble: 10 × 2\n   itemnr   item      \n   &lt;chr&gt;    &lt;chr&gt;     \n 1 PANAS_11 Distressed\n 2 PANAS_12 Upset     \n 3 PANAS_13 Hostile   \n 4 PANAS_14 Irritable \n 5 PANAS_15 Scared    \n 6 PANAS_16 Afraid    \n 7 PANAS_17 Ashamed   \n 8 PANAS_18 Guilty    \n 9 PANAS_19 Nervous   \n10 PANAS_20 Jittery   \n\n\n\n1.3 Demographics\nVariables for invariance tests such as Differential Item Functioning (DIF) need to be separated into vectors (ideally as factors with specified levels and labels) with the same length as the number of rows in the dataset. This means that any kind of removal of respondents/rows with missing data needs to be done before separating the DIF variables.\nFirst, we’ll have a look at the class of our variables.\n\nCodedf %&gt;% \n  select(Sex,age,Group) %&gt;% \n  glimpse()\n\nRows: 1,856\nColumns: 3\n$ Sex   &lt;chr&gt; \"Male\", \"Male\", \"Female\", \"Male\", \"Male\", \"Male\", \"Female\", \"Fem…\n$ age   &lt;dbl&gt; 27, 32, 21, 27, 20, 22, 23, 25, 21, 26, 38, 36, 24, 21, 19, 26, …\n$ Group &lt;chr&gt; \"Later Start\", \"Later Start\", \"Later Start\", \"Later Start\", \"Ear…\n\n\nSex and Group are class character, while age is numeric. Below are the response categories represented in data.\n\nCodetable(df$Sex)\n\n\n  CONSENT REVOKED      DATA EXPIRED            Female              Male \n                2                 1               896               955 \nPrefer not to say \n                2 \n\n\nSince there are only 5 respondents using labels that are not Female or Male (too few for meaningful statistical analysis), we will remove them to have a complete dataset for all variables in this example.\n\nCodedf &lt;- df %&gt;% \n  filter(Sex %in% c(\"Female\",\"Male\")) # filtering to only include those with Sex being either Female or Male.\n\n\nLet’s make the variable a factor (instead of class “character”), since that will be helpful in our later analyses.\n\nCodedf$Sex &lt;- factor(df$Sex)\n\n\nSometimes age is provided in categories, but here we have a numeric variable with age in years. Let’s have a quick look at the age distribution using a histogram, and get some summary statistics.\n\nCodesummary(df$age)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  18.00   21.00   24.00   26.74   30.00   71.00 \n\n\n\nCode### simpler version of the ggplot below using base R function hist()\n# hist(df$age, col = \"#009ca6\")\n# abline(v = median(age, na.rm = TRUE))\n# \n# df %&gt;% \n#   summarise(Mean = round(mean(age, na.rm = T),1),\n#             StDev = round(sd(age, na.rm = T),1)\n#             )\n\nggplot(df) +\n  geom_histogram(aes(x = age), \n                 fill = \"#009ca6\",\n                 col = \"black\") +\n  # add the average as a vertical line\n  geom_vline(xintercept = median(df$age), \n             linewidth = 1.5,\n             linetype = 2,\n             col = \"orange\") +\n  # add a light grey field indicating the standard deviation\n  annotate(\"rect\", ymin = 0, ymax = Inf, \n           xmin = summary(df$age)[2], \n           xmax = summary(df$age)[5], \n           alpha = .2) +\n  labs(title = \"\",\n       x = \"Age in years\",\n       y = \"Number of respondents\",\n       caption = str_wrap(glue(\"Note. Median age is {round(median(df$age, na.rm = T),1)}, shown with the dashed vertical line. Age range is {min(df$age)} to {max(df$age)}. Interquartile range ({summary(df$age)[2]} to {summary(df$age)[5]}) is indicated with the grey area.\")\n       )) +\n  scale_x_continuous(limits = c(18,75)) +\n  theme_bw() +\n  theme(plot.caption = element_text(hjust = 0, face = \"italic\")) \n\n\n\n\n\n\n\nSince the distribution of the age variable is clearly not following a gaussian (normal) distribution, the median seems like a better option than the mean.\nThere is also a grouping variable which needs to be converted to a factor.\n\nCodedf$Group &lt;- factor(df$Group)\n\n\nNow our demographic data has been checked, and we need to move it to a separate dataframe from the item data.\n\nCodedif &lt;- df %&gt;% \n  select(Sex,age,Group) %&gt;% \n  rename(Age = age) # just for consistency\n\ndf &lt;- df %&gt;% \n  select(!c(Sex,age,Group))\n\n\nTo create a “Table 1” containing participant demographic information, the tbl_summary() function from the gtsummary package is really handy to get a nice table.\n\nCodegtsummary::tbl_summary(dif)\n\n\n\n\n\n\n\n\n\nCharacteristic\n\nN = 1,8511\n\n\n\n\nSex\n\n\n\n    Female\n896 (48%)\n\n\n    Male\n955 (52%)\n\n\nAge\n24 (21, 30)\n\n\nGroup\n\n\n\n    Earlier Start\n901 (49%)\n\n\n    Later Start\n950 (51%)\n\n\n\n\n1 n (%); Median (Q1, Q3)\n\n\n\n\n\nWith only item data remaining in the df dataframe, we can easily rename the items in the item dataframe and make sure that they match the itemlabels variable itemnr. It will be useful to have short item labels to get less clutter in the tables and figures during analysis.\n\nCodenames(df) &lt;- itemlabels$itemnr\n\n\nNow we are all set for the psychometric analysis!"
  },
  {
    "objectID": "raschrvignette/RaschRvign.html#descriptives",
    "href": "raschrvignette/RaschRvign.html#descriptives",
    "title": "easyRasch vignette",
    "section": "\n2 Descriptives",
    "text": "2 Descriptives\nLet’s familiarize ourselves with the data before diving into the analysis.\n\n2.1 Missing data\nFirst, we visualize the proportion of missing data on item level.\n\nCodeRImissing(df)\n\n[1] \"No missing data.\"\n\n\nNo missing data in this dataset. If we had missing data, we could also use RImissingP() to look at which respondents have missing data and how much. Just to show what the output of these functions looks like, we’ll use mice::ampute() to temporarily remove 15% of data.\n\nCodeRImissing(mice::ampute(df, prop = 0.1)$amp)\n\n\n\n\n\n\nCodeRImissingP(mice::ampute(df, prop = 0.1)$amp, N = 20)\n\n\n\n\n\n\n\n\n2.2 Overall responses\nThis provides us with an overall picture of the data distribution. As a bonus, any oddities/mistakes in recoding the item data from categories to numbers will be clearly visible.\n\nCodeRIallresp(df)\n\n\n\n\nResponse category\n\n\nNumber of responses\n\n\nPercent\n\n\n\n\n\n1\n\n\n9430\n\n\n50.9\n\n\n\n\n2\n\n\n4136\n\n\n22.3\n\n\n\n\n3\n\n\n2676\n\n\n14.5\n\n\n\n\n4\n\n\n1722\n\n\n9.3\n\n\n\n\n5\n\n\n546\n\n\n2.9\n\n\n\n\n\n\nMost R packages for Rasch analysis require the lowest response category to be zero, which makes it necessary for us to recode our data, from using the range of 1-5 to 0-4.\n\nCodedf &lt;- df %&gt;% \n  mutate(across(everything(), ~ .x - 1))\n\n# always check that your recoding worked as intended.\nRIallresp(df)\n\n\n\n\nResponse category\n\n\nNumber of responses\n\n\nPercent\n\n\n\n\n\n0\n\n\n9430\n\n\n50.9\n\n\n\n\n1\n\n\n4136\n\n\n22.3\n\n\n\n\n2\n\n\n2676\n\n\n14.5\n\n\n\n\n3\n\n\n1722\n\n\n9.3\n\n\n\n\n4\n\n\n546\n\n\n2.9\n\n\n\n\n\n\n\n2.2.1 Floor/ceiling effects\nWe can also look at the raw distribution of ordinal sum scores. The RIrawdist() function is a bit crude, since it requires responses in all response categories to accurately calculate max and min scores.\n\nCodeRIrawdist(df)\n\n\n\n\n\n\n\nThere is a floor effect with 11.8% of participants responding in the lowest category for all items.\n\n2.2.2 Guttman structure\nWhile not really necessary, it can be interesting to see whether the response patterns follow a Guttman-like structure. Items and persons are sorted based on lower-&gt;higher responses, and we should see the color move from yellow in the lower left corner to blue in the upper right corner.\n\nCodeRIheatmap(df) +\n  theme(axis.text.x = element_blank())\n\n\n\n\n\n\n\nIn this data, we see the floor effect on the left, with 11.8% of respondents all yellow, and a rather weak Guttman structure. This could also be due to a low variation in item locations/difficulties. Since we have a very large sample I added a theme() option to remove the x-axis text, which would anyway just be a blur of the 1851 respondent row numbers. Each thin vertical slice in the figure is one respondent.\n\n2.3 Item level descriptives\nThere are many ways to look at the item level data, and we’ll get them all together in the tab-panel below. The RItileplot() is probably most informative, since it provides the number of responses in each response category for each item. It is usually recommended to have at least ~10 responses in each category for psychometric analysis, no matter which methodology is used.\nKudos to Solomon Kurz for providing the idea and code on which the tile plot function is built!\nMost people will be familiar with the barplot, and this is probably most intuitive to understand the response distribution within each item. However, if there are many items it will take a while to review, and does not provide the same overview as a tileplot or stacked bars.\n\nCode```{r}\n#| column: margin\n#| code-fold: true\n\n# This code chunk creates a small table in the margin beside the panel-tabset output below, showing all items currently in the df dataframe.\n# The Quarto code chunk option \"#| column: margin\" is necessary for the layout to work as intended.\nRIlistItemsMargin(df, fontsize = 13)\n```\n\n\n\n\n\nitemnr\n\n\nitem\n\n\n\n\n\nPANAS_11\n\n\nDistressed\n\n\n\n\nPANAS_12\n\n\nUpset\n\n\n\n\nPANAS_13\n\n\nHostile\n\n\n\n\nPANAS_14\n\n\nIrritable\n\n\n\n\nPANAS_15\n\n\nScared\n\n\n\n\nPANAS_16\n\n\nAfraid\n\n\n\n\nPANAS_17\n\n\nAshamed\n\n\n\n\nPANAS_18\n\n\nGuilty\n\n\n\n\nPANAS_19\n\n\nNervous\n\n\n\n\nPANAS_20\n\n\nJittery\n\n\n\n\n\n\n\nTile plot\nStacked bars\nBarplots\n\n\n\n\nCodeRItileplot(df)\n\n\n\n\n\n\n\nWhile response patterns are skewed for all items, there are more than 10 responses in each category for all items which is helpful for the analysis.\n\n\n\nCodeRIbarstack(df) +\n  theme_minimal() + # theming is optional, see section 11 for more on this\n  theme_rise() \n\n\n\n\n\n\n\n\n\n\nCodeRIitemcols(df)"
  },
  {
    "objectID": "raschrvignette/RaschRvign.html#sec-rasch",
    "href": "raschrvignette/RaschRvign.html#sec-rasch",
    "title": "easyRasch vignette",
    "section": "\n3 Rasch analysis 1",
    "text": "3 Rasch analysis 1\nThe eRm package and Conditional Maximum Likelihood (CML) estimation will be used primarily, with the Partial Credit Model since this is polytomous data.\nThis is also where the five basic psychometric aspects are good to recall.\n\nUnidimensionality & local independence\nOrdering of response categories\nInvariance\nTargeting\nMeasurement uncertainties (reliability)\n\nWe will begin by looking at unidimensionality, response categories, and targeting in parallel below. For unidimensionality, we are mostly interested in item fit and residual correlations, as well as PCA of residuals and loadings on the first residual contrast. At the same time, disordered response categories can influence item fit to some extent (and vice versa), and knowledge about targeting can be useful if it is necessary to remove items due to residual correlations.\nWhen unidimensionality and response categories are found to work adequately, we will move on to invariance testing (Differential Item Functioning, DIF). It should be noted that DIF should be evaluated in parallel with all other psychometric aspects, but since it is a more complex issue it is kept in a separate section in this vignette (as is person fit). Finally, when/if invariance/DIF also looks acceptable, we can investigate reliability/measurement uncertainties.\nSince our sample is quite large (n &gt; 800), the sample size sensitive tests (item fit and item-restscore) that are likely to produce problematic false positive rates (Johansson, 2025a) will use a randomly selected smaller sample of n = 500 as examples. When analyzing large datasets, item-restscore bootstrap is recommended as the primary method of assessing item fit to the Rasch model. The use of one smaller subsample in this vignette is only for demonstration purposes, it is not a recommended method of analysis for large datasets.\n\nCodedf500 &lt;- df %&gt;% \n  slice_sample(n = 500)\n\n\n\n\n\n\n\n\nNote\n\n\n\nIn the tabset-panel below, each tab contains explanatory text, which is sometimes a bit lengthy. Remember to scroll back up and click on all tabs.\n\n\n\n\n\n\n\n\nitemnr\n\n\nitem\n\n\n\n\n\nPANAS_11\n\n\nDistressed\n\n\n\n\nPANAS_12\n\n\nUpset\n\n\n\n\nPANAS_13\n\n\nHostile\n\n\n\n\nPANAS_14\n\n\nIrritable\n\n\n\n\nPANAS_15\n\n\nScared\n\n\n\n\nPANAS_16\n\n\nAfraid\n\n\n\n\nPANAS_17\n\n\nAshamed\n\n\n\n\nPANAS_18\n\n\nGuilty\n\n\n\n\nPANAS_19\n\n\nNervous\n\n\n\n\nPANAS_20\n\n\nJittery\n\n\n\n\n\n\n\nConditional item fit\nItem-restscore\nItem-restscore bootstrap\nConditional item characteristic curves\nPCA of residuals\nConditional LRT\nLocal dependence (Q3)\nLocal dependence (G2)\nLD (partial gamma)\n1st contrast loadings\nAnalysis of response categories\nResponse categories MIRT\nTargeting\nItem hierarchy\n\n\n\n\nCodesimfit1 &lt;- RIgetfit(df500, iterations = 200, cpu = 8) # save simulation output to object `simfit1`\nRIitemfit(df500, simfit1)\n\n\n\n Item \n    InfitMSQ \n    Infit thresholds \n    Infit diff \n    Relative location \n  \n\n\n PANAS_11 \n    1.164 \n    [0.874, 1.138] \n    0.026 \n    1.18 \n  \n\n PANAS_12 \n    0.816 \n    [0.859, 1.153] \n    0.043 \n    1.46 \n  \n\n PANAS_13 \n    1.285 \n    [0.865, 1.143] \n    0.142 \n    1.96 \n  \n\n PANAS_14 \n    1.016 \n    [0.843, 1.167] \n    no misfit \n    1.28 \n  \n\n PANAS_15 \n    0.825 \n    [0.833, 1.168] \n    0.008 \n    1.75 \n  \n\n PANAS_16 \n    0.809 \n    [0.816, 1.154] \n    0.007 \n    1.60 \n  \n\n PANAS_17 \n    1.002 \n    [0.836, 1.209] \n    no misfit \n    1.77 \n  \n\n PANAS_18 \n    1.04 \n    [0.86, 1.238] \n    no misfit \n    1.69 \n  \n\n PANAS_19 \n    0.891 \n    [0.857, 1.17] \n    no misfit \n    0.99 \n  \n\n PANAS_20 \n    1.205 \n    [0.843, 1.126] \n    0.079 \n    1.34 \n  \n\n\nNote: \n\n MSQ values based on conditional calculations (n = 500 complete cases).                                Simulation based thresholds from 200 simulated datasets.\n\n\n\n\nRIitemfit() and RIgetfit() both work with both dichotomous and polytomous data (using the partial credit model) and automatically selects the model based on the data structure.\nIt is important to note that the RIitemfit() function uses conditional outfit/infit, which is both robust to different sample sizes and makes ZSTD unnecessary (Müller, 2020).\nSince the distribution of item fit statistics are not known, we need to use simulation to determine appropriate cutoff threshold values for the current sample and items. RIitemfit() can also use the simulation based cutoff values and use them for conditional highlighting of misfitting items. See the blog post on simulation based cutoffs for some more details on this.\nBriefly stated, the simulation uses the estimated properties of the current sample and items (your dataset), and simulates n iterations of data that fit the Rasch model to get an empirical distribution of item fit that we can use for comparison with the observed data. This is also known as “parametric bootstrapping”.\nThe simulations can take a bit of time to run if you have complex data/many items/many participants, and/or choose to use many iterations. Simulation experiments (Johansson, 2025a) indicate that 100-400 iterations should be a useful range, where smaller samples (n &lt; 300) should use 100 iterations, and 200-400 is more appropriate when one has larger samples. To improve speed, make good use of your CPU cores.\n\n\n\n\n\n\nNote\n\n\n\nAnother important finding from simulation studies is that there is a risk of false positive indication of misfit in samples larger than n = 1000 when using item infit or item-restscore. The recommended primary method for determining item fit in large samples is the bootstrapped item-restscore (Johansson, 2025a), as illustrated in an adjacent tab labeled “Item-restscore bootstrap” .\n\n\nFor reference, the simulation above, using 10 items with 5 response categories each and 1851 respondents, takes about 24 seconds to run on 8 cpu cores (Macbook Pro M1 Max) for 400 iterations.\nI’ll cite Ostini & Nering (2006) on the description of outfit and infit (pages 86-87):\n\nResponse residuals can be summed over respondents to obtain an item fit measure. Generally, the accumulation is done with squared standardized residuals, which are then divided by the total number of respondents to obtain a mean square statistic. In this form, the statistic is referred to as an unweighted mean square (Masters & Wright, 1997; Wright & Masters, 1982) and has also come to be known as “outfit” (Smith, Schumacker, & Bush, 1998; Wu, 1997) […]\n\n\nA weighted version of this statistic was developed to counteract its sensitivity to outliers (Smith, 2000). In its weighted form, the squared standardized residual is multiplied by the observed response variance and then divided by the sum of the item response variances. This is sometimes referred to as an information weighted mean square and has become known as “infit” (Smith et al., 1998; Wu, 1997).\n\nA low item fit value (sometimes referred to as an item “overfitting” the Rasch model) indicates that responses may be too predictable. This is often the case for items that are very general/broad in scope in relation to the latent variable, for instance asking about feeling depressed in a depression questionnaire. You will often find overfitting items to also have residual correlations with other items. Overfit may be likened to having a much stronger factor loading than other items in a confirmatory factor analysis.\nA high item fit value (sometimes referred to as “underfitting” the Rasch model) can indicate several things, often multidimensionality or a question that is difficult to interpret and thus has noisy response data. The latter could for instance be caused by a question that asks about two things at the same time or is ambiguous for other reasons.\nOutfit has been found to be a not very useful method to detect item misfit, infit should be relied upon when sample sizes are appropriate (Johansson, 2025a). Outfit was removed from the output in version 0.3.9.\n\n\n\n\n\n\nNote\n\n\n\nRemember to scroll back up and click on all tabs.\n\n\n\n\nThis is another useful function from the iarm package. It shows the expected and observed correlation between an item and a score based on the rest of the items (Kreiner, 2011). Similarly, but inverted, to item fit, a lower observed correlation value than expected indicates underfit, that the item may not belong to the dimension. A higher than expected observed value indicates an overfitting and possibly redundant item. Overfitting items will often also show issues with residual correlations. Both of these problems can often be (at least partially) resolved by removing underfitting items.\n\nCodeRIrestscore(df500)\n\n\n\n Item \n    Observed value \n    Model expected value \n    Absolute difference \n    Adjusted p-value (BH) \n    Statistical significance level \n    Location \n    Relative location \n  \n\n\n PANAS_11 \n    0.59 \n    0.64 \n    0.05 \n    0.180 \n     \n    -0.32 \n    1.18 \n  \n\n PANAS_12 \n    0.72 \n    0.63 \n    0.09 \n    0.002 \n    ** \n    -0.04 \n    1.46 \n  \n\n PANAS_13 \n    0.52 \n    0.62 \n    0.10 \n    0.020 \n    * \n    0.46 \n    1.96 \n  \n\n PANAS_14 \n    0.62 \n    0.64 \n    0.02 \n    0.417 \n     \n    -0.22 \n    1.28 \n  \n\n PANAS_15 \n    0.72 \n    0.62 \n    0.10 \n    0.002 \n    ** \n    0.25 \n    1.75 \n  \n\n PANAS_16 \n    0.73 \n    0.64 \n    0.09 \n    0.002 \n    ** \n    0.10 \n    1.60 \n  \n\n PANAS_17 \n    0.66 \n    0.62 \n    0.04 \n    0.268 \n     \n    0.27 \n    1.77 \n  \n\n PANAS_18 \n    0.64 \n    0.63 \n    0.01 \n    0.795 \n     \n    0.19 \n    1.69 \n  \n\n PANAS_19 \n    0.69 \n    0.65 \n    0.04 \n    0.182 \n     \n    -0.52 \n    0.99 \n  \n\n PANAS_20 \n    0.58 \n    0.64 \n    0.06 \n    0.144 \n     \n    -0.16 \n    1.34 \n  \n\n\n\n\n\n\nBoth item-restscore and conditional item infit/outfit will indicate “false misfit” when sample sizes are large (even when using simulation/bootstrap based cutoff values). This behavior can occur from about n = 500, and certainly will occur at samples of 800 and above (Johansson, 2025a). This “false misfit” is caused by truly misfitting items, which underlines the importance of removing one item at a time when one finds issues with misfit/multidimensionality. However, a useful way to get additional information about the probability of actual misfit is to use non-parametric bootstrapping. This function resamples with replacement from your response data and reports the percentage and type of misfit indicated by the item-restscore function. You will also get information about conditional MSQ infit (based on the full sample, using complete responders). Simulation studies indicate that a sample size of 800 results in 95+% detection rate of 1-3 misfitting items amongst 20 dichotomous items (Johansson, 2025a).\n\nCodeRIbootRestscore(df, iterations = 250, samplesize = 800)\n\n\n\n Item \n    Item-restscore result \n    % of iterations \n    Conditional MSQ infit \n    Relative average item location \n  \n\n\n PANAS_20 \n    underfit \n    80.4 \n    1.20 \n    1.42 \n  \n\n PANAS_11 \n    underfit \n    49.2 \n    1.20 \n    1.14 \n  \n\n PANAS_13 \n    underfit \n    28.8 \n    1.17 \n    1.91 \n  \n\n PANAS_14 \n    underfit \n    13.6 \n    1.06 \n    1.15 \n  \n\n PANAS_15 \n    overfit \n    100.0 \n    0.78 \n    1.63 \n  \n\n PANAS_16 \n    overfit \n    99.2 \n    0.80 \n    1.48 \n  \n\n PANAS_12 \n    overfit \n    93.6 \n    0.84 \n    1.42 \n  \n\n PANAS_17 \n    overfit \n    42.0 \n    0.95 \n    1.83 \n  \n\n PANAS_19 \n    overfit \n    14.0 \n    0.92 \n    0.98 \n  \n\n\nNote: \n\n Results based on 250 bootstrap iterations with n = 800 and 10 items. Conditional mean-square infit based on complete responders only (n = 1851).\n\n\n\n\n\n\nThe iarm package (Mueller & Santiago, 2022) provides several interesting functions for assessing item fit, DIF and other things. Some of these functions have been implemented in simplified versions in easyRasch, and others may be included in a future version. Below are conditional item characteristic curves (ICC’s) using the estimated theta (factor score).\nThese curves indicate item fit on a group level, where respondents are split into “class intervals” based on their sum score/factor score.\n\nCodeRIciccPlot(df) +\n  guide_area() # this relocates the legend/guide, which may produce a nicer output\n\n\n\n\n\n\n\nA similar visualization, even more informative and flexible, has been made available in the RASCHplot package (Buchardt et al., 2023). The package needs to be installed from GitHub (see commented code below). The linked paper is recommended reading, not least for descriptions of the useful options available. Below are some sample plots showing conditional ICC’s using the raw sum score.\n\nCodelibrary(RASCHplot) # devtools::install_github(\"ERRTG/RASCHplot\")\n\nCICCplot(PCM(df),\n         which.item = c(1:4),\n         lower.groups = c(0,7,14,21,28),\n         grid.items = TRUE)\n\n\n\n\n\n\n\n\n\nPrincipal Component Analysis of Rasch model residuals (PCAR).\n\nCodeRIpcmPCA(df)\n\n\n\n Eigenvalues \n    Proportion of variance \n  \n\n\n 1.79 \n    16.9% \n  \n\n 1.47 \n    15.1% \n  \n\n 1.28 \n    13.6% \n  \n\n 1.14 \n    13.3% \n  \n\n 1.06 \n    11.9% \n  \n\n\n\n\nBased on an old rule-of-thumb, the first eigenvalue should be below 1.5 to support unidimensionality (Smith, 2002). However, as with many other metrics the expected PCAR eigenvalue is also dependent on sample size and test length (Chou & Wang, 2010). To be clear, there is no single or easily calculated rule-of-thumb critical value for the largest PCA eigenvalue that will be generally applicable. A parametric bootstrap function has been implemented in easyRasch to determine a potentially appropriate cutoff value for the largest PCAR eigenvalue, but it has not been systematically evaluated yet. Below is an example, illustrated with a histogram of the simulated distribution of largest eigenvalues, the 99th percentile and the max value.\n\nCodepcasim &lt;- RIbootPCA(df, iterations = 500, cpu = 8)\nhist(pcasim$results, breaks = 50)\n\n\n\n\n\n\nCodepcasim$p99\n\n    99% \n1.27605 \n\nCodepcasim$max\n\n[1] 1.3\n\n\nIf the bootstrap turns out to provide an appropriate cutoff value, it still needs to be used together with checking item fit (or item-restscore) and residual correlations (local dependence) to evaluate unidimensionality.\nThe PCA eigenvalues are mostly included here for those coming from Winsteps who may be looking for this metric. Speaking of Winsteps, the “explained variance” generated by RIpcmPCA() will not be comparable to Winsteps corresponding metric, since this function only shows the results from the analysis of residuals and not the variance explained by the Rasch model itself. Additionally, it should be noted that the PCA rotation is set to “oblimin”.\n\n\nThe Conditional Likelihood Ratio Test (CLRT, Andersen, 1973) is a global test of fit and can be a useful addition to more informative item-level metrics (Johansson, 2025a). CLRT compares the responses of those scoring below the median to those scoring above the median to assess the homogeneity of the test. Below, we run the test using the smaller sample with a function from the iarm package.\n\nCodeclr_tests(df500, model = \"PCM\")\n\n\nConditional Likelihood Ratio Tests:\n\n\n        clr df pvalue sig \noverall 108 39 2e-08   ***\n\n\nThe test is clearly statistically significant, even with the smaller sample, which indicates that unidimensionality does not hold. However, CLRT is sensitive to large sample sizes and number of items, and produces false positives above the nominal 5% level. See https://pgmj.github.io/clrt.html for a very small simulation study. To minimize the sample size issue, a non-parametric bootstrap function has been implemented in easyRasch, where one can set the sample size manually. The function will then sample with replacement from the response data and conduct a separate CLRT for each sample drawn.\n\nCodeRIbootLRT(df, iterations = 1000, samplesize = 400, cpu = 8)\n\n\n\nResult\nn\nPercent\n\n\n\nNot statistically significant\n12\n1.2\n\n\nStatistically significant\n988\n98.8\n\n\n\n\n\nThe bootstrap method also shows a strong majority of statistically significant results.\nGlobal tests do not provide any information about the reason for misfit. The iarm package has also implemented a function to get item specific information related to the CLRT, showing observed and expected values for each item.\n\nCodeitem_obsexp(PCM(df500))\n\nScore group 1: \n         mean obs mean exp std.res sig\nPANAS_11  0.7783   0.7024   1.5569    \nPANAS_12  0.3348   0.4207  -2.0798 -  \nPANAS_13  0.3891   0.2664   3.4933 ++ \nPANAS_14  0.6244   0.5966   0.5993    \nPANAS_15  0.2715   0.3188  -1.2667    \nPANAS_16  0.2489   0.3428  -2.3426 -  \nPANAS_17  0.2262   0.2242   0.0631    \nPANAS_18  0.1991   0.2236  -0.7230    \nPANAS_19  0.6923   0.7378  -0.8948    \nPANAS_20  0.6335   0.5391   2.0591 +  \n\nScore group 2: \n         mean obs mean exp std.res sig\nPANAS_11  1.9905   2.0704  -1.1993    \nPANAS_12  1.7714   1.6778   1.3185    \nPANAS_13  1.1524   1.2811  -1.9035    \nPANAS_14  1.9190   1.9430  -0.3525    \nPANAS_15  1.4905   1.4395   0.7289    \nPANAS_16  1.7286   1.6277   1.3989    \nPANAS_17  1.2048   1.2069  -0.0309    \nPANAS_18  1.3667   1.3408   0.3520    \nPANAS_19  2.3667   2.3038   0.9228    \nPANAS_20  1.7714   1.8708  -1.4461    \n\n\n\n\nIn order to support unidimensionality, items should only be related to each other through the latent variable, and otherwise be independent of each other. This is called “local independence” when fulfilled and “local dependence” (LD) when there are issues. The extreme case of dependence is when all respondents give the same response for two items. Two aspects of LD are often described (Chen & Thissen, 1997):\n\nUnderlying local dependence (ULD), which is when the locally dependent items are measuring a different latent variable than the other items.\nSurface local dependence (SLD), when item content is very similar.\n\nBy investigating patterns in model residuals, we can determine whether items are independent or not. There are several tests that can be used to assess LD, of which Yen’s Q3 generally has good properties. Please see (Edwards et al., 2018) for an overview of different methods. It is recommended to use multiple methods, since they may perform differently with different types of LD. For instance, Chen & Thissen (1997) report that G2 is slightly more powerful than Yen’s Q3 in detecting SLD, while the reverse is true for ULD.\nSimilarly to item fit, we need to run simulations based on the observed data to get a useful cutoff threshold value for when residual correlations amongst item pairs are larger than would be expected from items that fit a unidimensional Rasch model (Christensen et al., 2017).\nThe simulation/bootstrap procedure can take some time to run, depending on the complexity of your data, but it is necessary to set the appropriate cutoff value. The number of iterations to use has yet to be systematically evaluated, but I recommend no less than 500, and ideally at least 1000 as a starting point. T\n\nCodesimcor1 &lt;- RIgetResidCor(df, iterations = 500, cpu = 8)\nRIresidcorr(df, cutoff = simcor1$p995)\n\n\n\n   \n    PANAS_11 \n    PANAS_12 \n    PANAS_13 \n    PANAS_14 \n    PANAS_15 \n    PANAS_16 \n    PANAS_17 \n    PANAS_18 \n    PANAS_19 \n    PANAS_20 \n  \n\n\n PANAS_11 \n     \n     \n     \n     \n     \n     \n     \n     \n     \n     \n  \n\n PANAS_12 \n    -0.1 \n     \n     \n     \n     \n     \n     \n     \n     \n     \n  \n\n PANAS_13 \n    -0.05 \n    -0.01 \n     \n     \n     \n     \n     \n     \n     \n     \n  \n\n PANAS_14 \n    -0.11 \n    0.09 \n    0.07 \n     \n     \n     \n     \n     \n     \n     \n  \n\n PANAS_15 \n    -0.14 \n    -0.13 \n    -0.22 \n    -0.29 \n     \n     \n     \n     \n     \n     \n  \n\n PANAS_16 \n    -0.17 \n    -0.1 \n    -0.25 \n    -0.27 \n    0.38 \n     \n     \n     \n     \n     \n  \n\n PANAS_17 \n    -0.18 \n    -0.09 \n    -0.09 \n    -0.19 \n    -0.13 \n    -0.08 \n     \n     \n     \n     \n  \n\n PANAS_18 \n    -0.19 \n    -0.15 \n    -0.16 \n    -0.18 \n    -0.15 \n    -0.13 \n    0.32 \n     \n     \n     \n  \n\n PANAS_19 \n    -0.13 \n    -0.13 \n    -0.25 \n    -0.14 \n    0.1 \n    0.08 \n    -0.21 \n    -0.12 \n     \n     \n  \n\n PANAS_20 \n    -0.06 \n    -0.22 \n    -0.07 \n    -0.05 \n    -0.13 \n    -0.19 \n    -0.16 \n    -0.15 \n    -0.07 \n     \n  \n\n\nNote: \n\n Relative cut-off value is 0.006, which is 0.108 above the average correlation (-0.102).                                Correlations above the cut-off are highlighted in red text.\n\n\n\n\nThe matrix above shows item-pair correlations of item residuals, with highlights in red showing correlations crossing the threshold compared to the average item-pair correlation (for all item-pairs) (Christensen et al., 2017). The threshold used in the example above has selected the 99.5th percentile from the simulations. The simcor1 object also pre-calculates values for the 95th, 99th and 99.9th percentiles as well as the maximum. If you desire another percentile, you can manually calculate it from the results in the simcor1 object using quantile().\nRasch model residual correlations using Yen’s Q3 are calculated using the mirt package.\n\n\nRasch model residual correlations using Chen & Thissen’s G2 are calculated using the mirt package.\n\nCodesimcor_g2 &lt;- RIgetResidCorG2(df, iterations = 500, cpu = 8)\nRIresidcorrG2(df, cutoff = simcor_g2$p995)\n\n\n\n   \n    PANAS_11 \n    PANAS_12 \n    PANAS_13 \n    PANAS_14 \n    PANAS_15 \n    PANAS_16 \n    PANAS_17 \n    PANAS_18 \n    PANAS_19 \n    PANAS_20 \n  \n\n\n PANAS_11 \n     \n    -0.14 \n    -0.13 \n    -0.11 \n    -0.1 \n    -0.11 \n    -0.12 \n    -0.14 \n    -0.13 \n    -0.12 \n  \n\n PANAS_12 \n     \n     \n    0.1 \n    0.13 \n    0.09 \n    0.1 \n    0.09 \n    -0.12 \n    0.1 \n    -0.12 \n  \n\n PANAS_13 \n     \n     \n     \n    0.1 \n    -0.1 \n    -0.12 \n    -0.1 \n    -0.12 \n    -0.12 \n    -0.13 \n  \n\n PANAS_14 \n     \n     \n     \n     \n    -0.09 \n    -0.1 \n    -0.09 \n    -0.11 \n    -0.09 \n    -0.09 \n  \n\n PANAS_15 \n     \n     \n     \n     \n     \n    0.24 \n    0.1 \n    -0.08 \n    0.14 \n    -0.1 \n  \n\n PANAS_16 \n     \n     \n     \n     \n     \n     \n    0.1 \n    0.1 \n    0.14 \n    -0.1 \n  \n\n PANAS_17 \n     \n     \n     \n     \n     \n     \n     \n    0.17 \n    -0.11 \n    -0.11 \n  \n\n PANAS_18 \n     \n     \n     \n     \n     \n     \n     \n     \n    -0.11 \n    -0.13 \n  \n\n PANAS_19 \n     \n     \n     \n     \n     \n     \n     \n     \n     \n    -0.12 \n  \n\n PANAS_20 \n     \n     \n     \n     \n     \n     \n     \n     \n     \n     \n  \n\n\nNote: \n\n Relative cut-off value is 0.098, which is 0.136 above the average correlation (-0.039).                                Correlations above the cut-off are highlighted in red text.\n\n\n\n\n\n\nAnother way to assess local (in)dependence is by partial gamma coefficients (Kreiner & Christensen, 2004). This is also a function from the iarm package. See ?iarm::partgam_LD for details.\n\nCodeRIpartgamLD(df)\n\n\n\n Item 1 \n    Item 2 \n    Partial gamma \n    SE \n    Lower CI \n    Upper CI \n    Adjusted p-value (BH) \n  \n\n\n PANAS_15 \n    PANAS_16 \n    0.634 \n    0.030 \n    0.576 \n    0.693 \n    0.000 \n  \n\n PANAS_16 \n    PANAS_15 \n    0.634 \n    0.030 \n    0.576 \n    0.692 \n    0.000 \n  \n\n PANAS_17 \n    PANAS_18 \n    0.577 \n    0.033 \n    0.512 \n    0.642 \n    0.000 \n  \n\n PANAS_18 \n    PANAS_17 \n    0.551 \n    0.034 \n    0.484 \n    0.619 \n    0.000 \n  \n\n PANAS_12 \n    PANAS_14 \n    0.341 \n    0.037 \n    0.269 \n    0.413 \n    0.000 \n  \n\n PANAS_16 \n    PANAS_19 \n    0.297 \n    0.040 \n    0.219 \n    0.375 \n    0.000 \n  \n\n PANAS_14 \n    PANAS_12 \n    0.293 \n    0.038 \n    0.217 \n    0.368 \n    0.000 \n  \n\n PANAS_15 \n    PANAS_19 \n    0.287 \n    0.040 \n    0.208 \n    0.366 \n    0.000 \n  \n\n PANAS_19 \n    PANAS_16 \n    0.274 \n    0.040 \n    0.197 \n    0.352 \n    0.000 \n  \n\n PANAS_19 \n    PANAS_15 \n    0.261 \n    0.041 \n    0.181 \n    0.340 \n    0.000 \n  \n\n PANAS_14 \n    PANAS_13 \n    0.209 \n    0.042 \n    0.127 \n    0.291 \n    0.000 \n  \n\n PANAS_13 \n    PANAS_14 \n    0.202 \n    0.041 \n    0.121 \n    0.283 \n    0.000 \n  \n\n PANAS_12 \n    PANAS_13 \n    0.191 \n    0.044 \n    0.105 \n    0.278 \n    0.001 \n  \n\n\n\n\n\n\n\nCodeRIloadLoc(df)\n\n\n\n\n\n\n\nHere we see item locations and their loadings on the first residual contrast. This figure can be helpful to identify clusters in data or multidimensionality. You can get a table with standardized factor loadings using RIloadLoc(df, output = \"table\").\n\n\nThe xlims setting changes the x-axis limits for the plots. The default values usually make sense, and we mostly add this option to point out the possibility of doing so. You can also choose to only show plots for only specific items.\nCodeRIitemCats(df, xlims = c(-5,5))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEach response category for each item should have a curve that indicates it to be the most probably response at some point on the latent variable (x axis in the figure).\n\n\nThis produces a single figure with all items. Note that mirt() uses marginal maximum likelihood (MML) estimation which may produce slightly different results compared to eRm/CML when the distribution of person locations/scores are non-normal.\n\nCodemirt(df, model=1, itemtype='Rasch', verbose = FALSE) %&gt;% \n  plot(type=\"trace\", as.table = TRUE, \n       theta_lim = c(-5,5)) # changes x axis limits\n\n\n\n\n\n\n\n\n\n\nCode# increase fig-height in the chunk option above if you have many items\nRItargeting(df, xlim = c(-5,4)) # xlim defaults to c(-4,4) if you omit this option\n\n\n\n\n\n\n\nThis figure shows how well the items fit the respondents/persons. It is a sort of Wright Map that shows person locations and item threshold locations on the same logit scale.\nThe top part shows person location histogram, the middle part an inverted histogram of item threshold locations, and the bottom part shows individual item threshold locations. The histograms also show means and standard deviations.\n\n\nHere the items are sorted on their average threshold location (black diamonds). 84% confidence intervals are shown around each item threshold location. For further details, see the caption text below the figure.\nThe numbers displayed in the plot can be disabled using the option numbers = FALSE.\n\nCodeRIitemHierarchy(df)\n\n\n\n\n\n\n\n\n\n\n\n3.1 Analysis 1 comments\nItem fit shows a lot of issues.\nItem 18 has issues with the second lowest category being disordered. Several other items have very short distances between thresholds 1 and 2, which is also clearly seen in the Item Hierarchy figure above.\nTwo item-pairs show residual correlations far above the cutoff value:\n\n15 and 16 (scared and afraid)\n17 and 18 (ashamed and guilty)\n\nSince item 15 also has a residual correlation with item 19, we will remove it. In the second pair, item 18 will be removed since it also has problems with disordered response categories.\n\n\n\n\n\n\nNote\n\n\n\nWe have multiple “diagnostics” to review when deciding which item to remove if there are strong residual correlations between two items. Here is a list of commonly used criteria:\n\nitem fit\nitem threshold locations compared to sample locations (targeting)\nordering of response categories\nDIF\nand whether there are residual correlations between one item and multiple other items\n\n\n\n\nCoderemoved.items &lt;- c(\"PANAS_15\",\"PANAS_18\")\n\ndf_backup &lt;- df\ndf500_backup &lt;- df500\n\ndf &lt;- df_backup %&gt;% \n  select(!any_of(removed.items))\n\ndf500 &lt;- df500_backup %&gt;% \n  select(!any_of(removed.items))\n\n\nAs seen in the code above, I chose to create a backup copy of the original dataframes, then removing the items. This can be useful if, at a later stage in the analysis, I want to be able to quickly “go back” and reinstate an item or undo any other change I have made."
  },
  {
    "objectID": "raschrvignette/RaschRvign.html#rasch-analysis-2",
    "href": "raschrvignette/RaschRvign.html#rasch-analysis-2",
    "title": "easyRasch vignette",
    "section": "\n4 Rasch analysis 2",
    "text": "4 Rasch analysis 2\nWith items 15 and 18 removed.\n\n\n\n\n\n\nitemnr\n\n\nitem\n\n\n\n\n\nPANAS_11\n\n\nDistressed\n\n\n\n\nPANAS_12\n\n\nUpset\n\n\n\n\nPANAS_13\n\n\nHostile\n\n\n\n\nPANAS_14\n\n\nIrritable\n\n\n\n\nPANAS_16\n\n\nAfraid\n\n\n\n\nPANAS_17\n\n\nAshamed\n\n\n\n\nPANAS_19\n\n\nNervous\n\n\n\n\nPANAS_20\n\n\nJittery\n\n\n\n\n\n\n\nItem-restscore bootstrap\nConditional item fit\nPCA of residuals\nResidual correlations\n1st contrast loadings\nTargeting\nItem hierarchy\n\n\n\n\nCodeRIbootRestscore(df, iterations = 250, samplesize = 800)\n\n\n\n Item \n    Item-restscore result \n    % of iterations \n    Conditional MSQ infit \n    Relative average item location \n  \n\n\n PANAS_20 \n    underfit \n    40.8 \n    1.16 \n    1.39 \n  \n\n PANAS_11 \n    underfit \n    13.2 \n    1.14 \n    1.11 \n  \n\n PANAS_12 \n    overfit \n    98.0 \n    0.80 \n    1.38 \n  \n\n PANAS_16 \n    overfit \n    56.0 \n    0.88 \n    1.44 \n  \n\n PANAS_17 \n    overfit \n    8.8 \n    1.04 \n    1.79 \n  \n\n\nNote: \n\n Results based on 250 bootstrap iterations with n = 800 and 8 items. Conditional mean-square infit based on complete responders only (n = 1851).\n\n\n\n\n\n\nStill using the smaller sample of n = 500.\n\nCodesimfit2 &lt;- RIgetfit(df500, iterations = 400, cpu = 8)\nRIitemfit(df500, simcut = simfit2)\n\n\n\n Item \n    InfitMSQ \n    Infit thresholds \n    Infit diff \n    Relative location \n  \n\n\n PANAS_11 \n    1.121 \n    [0.822, 1.17] \n    no misfit \n    1.14 \n  \n\n PANAS_12 \n    0.754 \n    [0.846, 1.201] \n    0.092 \n    1.42 \n  \n\n PANAS_13 \n    1.198 \n    [0.809, 1.155] \n    0.043 \n    1.91 \n  \n\n PANAS_14 \n    0.912 \n    [0.858, 1.129] \n    no misfit \n    1.24 \n  \n\n PANAS_16 \n    0.91 \n    [0.852, 1.226] \n    no misfit \n    1.56 \n  \n\n PANAS_17 \n    1.079 \n    [0.817, 1.172] \n    no misfit \n    1.72 \n  \n\n PANAS_19 \n    0.93 \n    [0.861, 1.169] \n    no misfit \n    0.95 \n  \n\n PANAS_20 \n    1.161 \n    [0.847, 1.168] \n    no misfit \n    1.30 \n  \n\n\nNote: \n\n MSQ values based on conditional calculations (n = 500 complete cases).                                Simulation based thresholds from 400 simulated datasets.\n\n\n\n\n\n\n\nCodeRIpcmPCA(df)\n\n\n\n Eigenvalues \n    Proportion of variance \n  \n\n\n 1.52 \n    18.9% \n  \n\n 1.33 \n    17.1% \n  \n\n 1.19 \n    16.6% \n  \n\n 1.15 \n    14.6% \n  \n\n 1.00 \n    13.1% \n  \n\n\n\n\n\n\n\nCodesimcor2 &lt;- RIgetResidCor(df, iterations = 400, cpu = 8)\nRIresidcorr(df, cutoff = simcor2$p99)\n\n\n\n   \n    PANAS_11 \n    PANAS_12 \n    PANAS_13 \n    PANAS_14 \n    PANAS_16 \n    PANAS_17 \n    PANAS_19 \n    PANAS_20 \n  \n\n\n PANAS_11 \n     \n     \n     \n     \n     \n     \n     \n     \n  \n\n PANAS_12 \n    -0.16 \n     \n     \n     \n     \n     \n     \n     \n  \n\n PANAS_13 \n    -0.11 \n    -0.06 \n     \n     \n     \n     \n     \n     \n  \n\n PANAS_14 \n    -0.19 \n    0.03 \n    0.01 \n     \n     \n     \n     \n     \n  \n\n PANAS_16 \n    -0.16 \n    -0.08 \n    -0.25 \n    -0.28 \n     \n     \n     \n     \n  \n\n PANAS_17 \n    -0.18 \n    -0.06 \n    -0.09 \n    -0.19 \n    0 \n     \n     \n     \n  \n\n PANAS_19 \n    -0.15 \n    -0.15 \n    -0.28 \n    -0.18 \n    0.12 \n    -0.16 \n     \n     \n  \n\n PANAS_20 \n    -0.1 \n    -0.27 \n    -0.13 \n    -0.11 \n    -0.18 \n    -0.15 \n    -0.09 \n     \n  \n\n\nNote: \n\n Relative cut-off value is -0.036, which is 0.093 above the average correlation (-0.129).                                Correlations above the cut-off are highlighted in red text.\n\n\n\n\n\n\n\nCodeRIloadLoc(df)\n\n\n\n\n\n\n\n\n\n\nCodeRItargeting(df, xlim = c(-4,4), bins = 45)\n\n\n\n\n\n\n\n\n\n\nCodeRIitemHierarchy(df)\n\n\n\n\n\n\n\n\n\n\n\n4.1 Analysis 2 comments\nItems 16 & 19, and 12 & 14 show problematic residual correlations.\nLet’s look at DIF before taking action upon this information. While we are keeping DIF as a separate section in this vignette, it is recommended to include DIF-analysis in the panel-tabset above (on item fit, PCA, residual correlations, etc)."
  },
  {
    "objectID": "raschrvignette/RaschRvign.html#dif---differential-item-functioning",
    "href": "raschrvignette/RaschRvign.html#dif---differential-item-functioning",
    "title": "easyRasch vignette",
    "section": "\n5 DIF - differential item functioning",
    "text": "5 DIF - differential item functioning\n\n\n\n\n\n\nNote\n\n\n\nWhile we are keeping DIF as a separate section in this vignette, it is recommended to include DIF-analysis in the parallel analysis above (on item fit, targeting, residual correlations, etc) since DIF tests will help you understand or evaluate item functioning together with the other tests.\n\n\nWe’ll be looking at whether item (threshold) locations are stable between demographic subgroups.\nThere are several DIF analysis tools available. The first one uses the package psychotree, which relies on statistical significance at p &lt; .05 as an indicator for DIF. This is a criterion that is highly sample size sensitive, and we are always interested in the size/magnitude of DIF as well, since that will inform us about the impact of DIF on the estimated latent variable.\nThe structure of DIF is also an important and complex aspect, particularly for polytomous data. Uniform DIF means that the DIF is similar across the latent continuum. We can test this in R using the lordif package, as demonstrated in Section 5.7. However, it should be noted that the lordif package does not provide an option to use Rasch models, and there may be results that are caused by also allowing the discrimination parameter to vary across items.\nA recent preprint (Henninger et al., 2024) does a great job illustrating “differential step functioning” (DSF), which is when item threshold locations in polytomous data show varying levels of DIF. It also describes a forthcoming development of the psychotree where one can use DIF effect size and purification functions to evaluate DIF/DSF. When the updated package is available, I will work to implement these new functions into the easyRasch package as well.\n\n\n\n\n\n\nNote\n\n\n\nIt is important to ensure that no cells in the data are empty for subgroups when conducting a DIF analysis. Split the data using the DIF-variable and create separate tileplots to review the response distribution in the DIF-groups.\n\n\n\nCodeRIdifTileplot(df, dif$Sex)\n\n\n\n\n\n\n\n\n5.1 Sex\n\n\n\n\n\n\nitemnr\n\n\nitem\n\n\n\n\n\nPANAS_11\n\n\nDistressed\n\n\n\n\nPANAS_12\n\n\nUpset\n\n\n\n\nPANAS_13\n\n\nHostile\n\n\n\n\nPANAS_14\n\n\nIrritable\n\n\n\n\nPANAS_16\n\n\nAfraid\n\n\n\n\nPANAS_17\n\n\nAshamed\n\n\n\n\nPANAS_19\n\n\nNervous\n\n\n\n\nPANAS_20\n\n\nJittery\n\n\n\n\n\n\n\nTable\nFigure items\nFigure thresholds\n\n\n\n\nCodeRIdifTable(df, dif$Sex)\n\n\n\n\n\n\n\n\n\n\nItem\n\n\n2\n\n\n3\n\n\nMean location\n\n\nStDev\n\n\nMaxDiff\n\n\n\n\n\nPANAS_11\n\n\n-0.314\n\n\n-0.196\n\n\n-0.255\n\n\n0.083\n\n\n0.117\n\n\n\n\nPANAS_12\n\n\n0.028\n\n\n-0.044\n\n\n-0.008\n\n\n0.051\n\n\n0.073\n\n\n\n\nPANAS_13\n\n\n0.553\n\n\n0.402\n\n\n0.478\n\n\n0.107\n\n\n0.151\n\n\n\n\nPANAS_14\n\n\n-0.328\n\n\n-0.183\n\n\n-0.255\n\n\n0.103\n\n\n0.146\n\n\n\n\nPANAS_16\n\n\n0.004\n\n\n0.114\n\n\n0.059\n\n\n0.078\n\n\n0.111\n\n\n\n\nPANAS_17\n\n\n0.520\n\n\n0.290\n\n\n0.405\n\n\n0.163\n\n\n0.230\n\n\n\n\nPANAS_19\n\n\n-0.495\n\n\n-0.355\n\n\n-0.425\n\n\n0.099\n\n\n0.140\n\n\n\n\nPANAS_20\n\n\n0.032\n\n\n-0.028\n\n\n0.002\n\n\n0.042\n\n\n0.059\n\n\n\n\n\n\n\n\n\nCodeRIdifFigure(df, dif$Sex)\n\n\n\n\n\n\n\n\n\n\nCodeRIdifFigThresh(df, dif$Sex)\n\n\n\n\n\n\n\n\n\n\nWhile no item shows problematic levels of DIF regarding item location, as shown by the table, there is an interesting pattern in the thresholds figure. The lowest threshold seems to be slightly lower for node 3 (Male) for all items. Also, item 11 shows a much wider spread of item locations for node 3 compared to node 2.\nThe results do not require any action since the difference is small.\n\n5.2 Age\nThe psychotree package uses a model-based recursive partitioning that is particularly useful when you have a continuous variable such as age in years and a large enough sample. It will test different ways to partition the age variable to determine potential group differences (Strobl et al., 2015b, 2021).\n\nCodeRIdifTable(df, dif$Age)\n\n[1] \"No statistically significant DIF found.\"\n\n\nNo DIF found for age.\n\n5.3 Group\n\nCodeRIdifTable(df, dif$Group)\n\n[1] \"No statistically significant DIF found.\"\n\n\nAnd no DIF for group.\n\n5.4 Sex and age\nThe psychotree package also allows for DIF interaction analysis with multiple DIF variables. We can use RIdifTable2() to input two DIF variables.\n\nCodeRIdifTable2(df, dif$Sex, dif$Age)\n\n\n\n\n\n\n\n\n\n\nItem\n\n\n2\n\n\n3\n\n\nMean location\n\n\nStDev\n\n\nMaxDiff\n\n\n\n\n\nPANAS_11\n\n\n-0.314\n\n\n-0.196\n\n\n-0.255\n\n\n0.083\n\n\n0.117\n\n\n\n\nPANAS_12\n\n\n0.028\n\n\n-0.044\n\n\n-0.008\n\n\n0.051\n\n\n0.073\n\n\n\n\nPANAS_13\n\n\n0.553\n\n\n0.402\n\n\n0.478\n\n\n0.107\n\n\n0.151\n\n\n\n\nPANAS_14\n\n\n-0.328\n\n\n-0.183\n\n\n-0.255\n\n\n0.103\n\n\n0.146\n\n\n\n\nPANAS_16\n\n\n0.004\n\n\n0.114\n\n\n0.059\n\n\n0.078\n\n\n0.111\n\n\n\n\nPANAS_17\n\n\n0.520\n\n\n0.290\n\n\n0.405\n\n\n0.163\n\n\n0.230\n\n\n\n\nPANAS_19\n\n\n-0.495\n\n\n-0.355\n\n\n-0.425\n\n\n0.099\n\n\n0.140\n\n\n\n\nPANAS_20\n\n\n0.032\n\n\n-0.028\n\n\n0.002\n\n\n0.042\n\n\n0.059\n\n\n\n\n\n\nNo interaction effect found for sex and age. The analysis only shows the previously identified DIF for sex.\n\n5.5 LRT-based DIF\nWe’ll use the group variable as an example. It’s always important to review the response distribution across DIF groups prior to running the DIF analysis. We need to make sure that there are responses in all cells in all subgroups.\n\nCodeRIdifTileplot(df, dif$Group)\n\n\n\n\n\n\n\nFirst, we can simply run the test to get the overall result.\n\nCodeerm.out &lt;- PCM(df)\nLRtest(erm.out, splitcr = dif$Group)\n\n\nAndersen LR-test: \nLR-value: 46.864 \nChi-square df: 31 \np-value:  0.034 \n\n\nReview the documentation for further details, using ?LRtest in your R console panel in Rstudio. There is also a plotting function, plotGOF() that may be of interest.\n\n\n\n\n\n\nitemnr\n\n\nitem\n\n\n\n\n\nPANAS_11\n\n\nDistressed\n\n\n\n\nPANAS_12\n\n\nUpset\n\n\n\n\nPANAS_13\n\n\nHostile\n\n\n\n\nPANAS_14\n\n\nIrritable\n\n\n\n\nPANAS_16\n\n\nAfraid\n\n\n\n\nPANAS_17\n\n\nAshamed\n\n\n\n\nPANAS_19\n\n\nNervous\n\n\n\n\nPANAS_20\n\n\nJittery\n\n\n\n\n\n\n\nItem location table\nItem location figure\nItem threshold table\nItem threshold figure\n\n\n\n\nCodeRIdifTableLR(df, dif$Group)\n\n\n\n\n\nItem locations\nStandard errors\n\n\n Item \n    Earlier Start \n    Later Start \n    MaxDiff \n    All \n    SE_Earlier Start \n    SE_Later Start \n    SE_All \n  \n\n\n\n PANAS_11 \n    0.069 \n    0.072 \n    0.003 \n    0.073 \n    0.140 \n    0.124 \n    0.093 \n  \n\n PANAS_12 \n    0.33 \n    0.346 \n    0.016 \n    0.342 \n    0.155 \n    0.133 \n    0.101 \n  \n\n PANAS_13 \n    0.69 \n    0.966 \n    0.276 \n    0.826 \n    0.185 \n    0.189 \n    0.129 \n  \n\n PANAS_14 \n    0.118 \n    0.054 \n    0.064 \n    0.084 \n    0.142 \n    0.121 \n    0.092 \n  \n\n PANAS_16 \n    0.447 \n    0.362 \n    0.085 \n    0.401 \n    0.160 \n    0.134 \n    0.102 \n  \n\n PANAS_17 \n    0.66 \n    0.822 \n    0.162 \n    0.751 \n    0.184 \n    0.172 \n    0.125 \n  \n\n PANAS_19 \n    -0.028 \n    -0.122 \n    0.094 \n    -0.083 \n    0.136 \n    0.117 \n    0.088 \n  \n\n PANAS_20 \n    0.351 \n    0.348 \n    0.003 \n    0.352 \n    0.160 \n    0.138 \n    0.104 \n  \n\n\nNote: \n\n Values highlighted in red are above the chosen cutoff 0.5 logits. Background color brown and blue indicate the lowest and highest values among the DIF groups.\n\n\n\n\n\n\n\nCodeRIdifFigureLR(df, dif$Group) + theme_rise()\n\n\n\n\n\n\n\n\n\n\nCodeRIdifThreshTblLR(df, dif$Group)\n\n\n\n\n\nThreshold locations\nStandard errors\n\n\n Item threshold \n    Earlier Start \n    Later Start \n    MaxDiff \n    All \n    SE_Earlier Start \n    SE_Later Start \n    SE_All \n  \n\n\nPANAS_11\n\n c1 \n    -1.245 \n    -1.241 \n    0.004 \n    -1.240 \n    0.098 \n    0.094 \n    0.068 \n  \n\n c2 \n    -0.365 \n    -0.221 \n    0.144 \n    -0.284 \n    0.107 \n    0.100 \n    0.073 \n  \n\n c3 \n    0.281 \n    0.096 \n    0.185 \n    0.180 \n    0.131 \n    0.114 \n    0.086 \n  \n\n c4 \n    1.604 \n    1.655 \n    0.051 \n    1.637 \n    0.224 \n    0.188 \n    0.144 \n  \nPANAS_12\n\n c1 \n    -0.484 \n    -0.362 \n    0.122 \n    -0.418 \n    0.092 \n    0.091 \n    0.065 \n  \n\n c2 \n    0.241 \n    -0.198 \n    0.439 \n    -0.005 \n    0.126 \n    0.108 \n    0.082 \n  \n\n c3 \n    0.479 \n    0.456 \n    0.023 \n    0.467 \n    0.169 \n    0.129 \n    0.103 \n  \n\n c4 \n    1.086 \n    1.489 \n    0.403 \n    1.323 \n    0.233 \n    0.205 \n    0.153 \n  \nPANAS_13\n\n c1 \n    -0.067 \n    0.23 \n    0.297 \n    0.093 \n    0.092 \n    0.089 \n    0.064 \n  \n\n c2 \n    0.403 \n    0.115 \n    0.288 \n    0.248 \n    0.135 \n    0.118 \n    0.088 \n  \n\n c3 \n    0.889 \n    1.042 \n    0.153 \n    0.983 \n    0.197 \n    0.164 \n    0.126 \n  \n\n c4 \n    1.536 \n    2.476 \n    0.94 \n    1.979 \n    0.316 \n    0.384 \n    0.239 \n  \nPANAS_14\n\n c1 \n    -1.017 \n    -0.972 \n    0.045 \n    -0.990 \n    0.095 \n    0.094 \n    0.066 \n  \n\n c2 \n    -0.134 \n    -0.272 \n    0.138 \n    -0.205 \n    0.111 \n    0.103 \n    0.076 \n  \n\n c3 \n    0.456 \n    0.095 \n    0.361 \n    0.239 \n    0.146 \n    0.116 \n    0.091 \n  \n\n c4 \n    1.168 \n    1.366 \n    0.198 \n    1.294 \n    0.216 \n    0.173 \n    0.135 \n  \nPANAS_16\n\n c1 \n    -0.156 \n    -0.25 \n    0.094 \n    -0.202 \n    0.097 \n    0.091 \n    0.066 \n  \n\n c2 \n    0.009 \n    -0.091 \n    0.1 \n    -0.046 \n    0.130 \n    0.112 \n    0.085 \n  \n\n c3 \n    0.324 \n    0.354 \n    0.03 \n    0.344 \n    0.159 \n    0.132 \n    0.102 \n  \n\n c4 \n    1.611 \n    1.435 \n    0.176 \n    1.508 \n    0.253 \n    0.201 \n    0.157 \n  \nPANAS_17\n\n c1 \n    0.264 \n    0.368 \n    0.104 \n    0.324 \n    0.097 \n    0.089 \n    0.066 \n  \n\n c2 \n    0.389 \n    0.421 \n    0.032 \n    0.412 \n    0.146 \n    0.128 \n    0.096 \n  \n\n c3 \n    0.804 \n    0.955 \n    0.151 \n    0.894 \n    0.205 \n    0.181 \n    0.136 \n  \n\n c4 \n    1.182 \n    1.545 \n    0.363 \n    1.373 \n    0.288 \n    0.290 \n    0.204 \n  \nPANAS_19\n\n c1 \n    -1.339 \n    -1.263 \n    0.076 \n    -1.297 \n    0.100 \n    0.098 \n    0.070 \n  \n\n c2 \n    -0.388 \n    -0.27 \n    0.118 \n    -0.323 \n    0.108 \n    0.105 \n    0.075 \n  \n\n c3 \n    0.101 \n    -0.333 \n    0.434 \n    -0.143 \n    0.127 \n    0.111 \n    0.083 \n  \n\n c4 \n    1.512 \n    1.378 \n    0.134 \n    1.430 \n    0.207 \n    0.156 \n    0.125 \n  \nPANAS_20\n\n c1 \n    -0.906 \n    -0.877 \n    0.029 \n    -0.887 \n    0.093 \n    0.090 \n    0.065 \n  \n\n c2 \n    -0.189 \n    -0.257 \n    0.068 \n    -0.223 \n    0.108 \n    0.099 \n    0.073 \n  \n\n c3 \n    1.037 \n    0.585 \n    0.452 \n    0.760 \n    0.162 \n    0.123 \n    0.098 \n  \n\n c4 \n    1.463 \n    1.941 \n    0.478 \n    1.756 \n    0.277 \n    0.238 \n    0.180 \n  \n\n\nNote: \n\n Values highlighted in red are above the chosen cutoff 0.5 logits. Background color brown and blue indicate the lowest and highest values among the DIF groups.\n\n\n\n\n\n\n\nCodeRIdifThreshFigLR(df, dif$Group) + theme_rise()\n\n\n\n\n\n\n\n\n\n\nThe item threshold table shows that the top threshold for item 13 differs more than 0.5 logits between groups. In this set of 8 items with 4 thresholds each, it is unlikely to result in problematic differences in estimated person scores.\n\n5.6 Conditional ICC\nThe iarm package function for CICC plots also includes the option to use a DIF variable. This gives us information about DIF across the latent continuum when class intervals are created based on latent score groups. If your analysis contains two groups and the lines for the groups cross each other, DIF is non-uniform.\n\nCodeRIciccPlot(df, dif = \"yes\", dif_var = dif$Sex)\n\n\n\n\n\n\n\n\n5.7 Logistic Ordinal Regression DIF\nThe lordif package (Choi et al., 2011) does not use a Rasch measurement model, it only offers a choice between the Graded Response Model (GRM) and the Generalized Partial Credit Model (GPCM). Both of these are 2PL models, meaning that they estimate a discrimination parameter for each item in addition to the item threshold parameters. lordif relies on the mirt package.\nThere are several nice features available in the lordif package. First, we get a χ2 test of uniform or non-uniform DIF. Second, there are three possible methods/criteria for flagging items with potential DIF. One of these uses a likelihood ratio (LR) χ2 test, while the other two are indicators of DIF size/magnitude, either using a pseudo R2 statistic (“McFadden”, “Nagelkerke”, or “CoxSnell”) or a Beta criterion. For further details, see ?lordif in your R console or the paper describing the package (Choi et al., 2011).\nBelow is some sample code to get you started with lordif.\n\nCodelibrary(lordif)\n\ng_dif &lt;- lordif(as.data.frame(df), as.numeric(dif$Sex), # make sure that the data is in a dataframe-object and that the DIF variable is numeric\n                criterion = c(\"Chisqr\"), \n                alpha = 0.01, \n                beta.change = 0.1,\n                model = \"GPCM\",\n                R2.change = 0.02)\n\ng_dif_sum &lt;- summary(g_dif)\n\n\n\nCode# threshold values for colorizing the table below\nalpha = 0.01\nbeta.change = 0.1\nR2.change = 0.02\n\ng_dif_sum$stats %&gt;% \n  as.data.frame() %&gt;% \n  select(!all_of(c(\"item\",\"df12\",\"df13\",\"df23\"))) %&gt;% \n  round(3) %&gt;% \n  add_column(itemnr = names(df), .before = \"ncat\") %&gt;% \n  mutate(across(c(chi12,chi13,chi23), ~ cell_spec(.x,\n                               color = case_when(\n                                 .x &lt; alpha ~ \"red\",\n                                 TRUE ~ \"black\"\n                               )))) %&gt;%\n  mutate(across(starts_with(\"pseudo\"), ~ cell_spec(.x,\n                               color = case_when(\n                                 .x &gt; R2.change ~ \"red\",\n                                 TRUE ~ \"black\"\n                               )))) %&gt;%\n  mutate(beta12 =  cell_spec(beta12,\n                               color = case_when(\n                                 beta12 &gt; beta.change ~ \"red\",\n                                 TRUE ~ \"black\"\n                               ))) %&gt;% \n  kbl_rise()\n\n\n\n itemnr \n    ncat \n    chi12 \n    chi13 \n    chi23 \n    beta12 \n    pseudo12.McFadden \n    pseudo13.McFadden \n    pseudo23.McFadden \n    pseudo12.Nagelkerke \n    pseudo13.Nagelkerke \n    pseudo23.Nagelkerke \n    pseudo12.CoxSnell \n    pseudo13.CoxSnell \n    pseudo23.CoxSnell \n  \n\n\n PANAS_11 \n    5 \n    0.323 \n    0 \n    0 \n    0.002 \n    0 \n    0.004 \n    0.003 \n    0 \n    0.005 \n    0.005 \n    0 \n    0.005 \n    0.005 \n  \n\n PANAS_12 \n    5 \n    0.013 \n    0.019 \n    0.192 \n    0.008 \n    0.001 \n    0.002 \n    0 \n    0.001 \n    0.002 \n    0 \n    0.001 \n    0.002 \n    0 \n  \n\n PANAS_13 \n    5 \n    0.106 \n    0.057 \n    0.077 \n    0.007 \n    0.001 \n    0.001 \n    0.001 \n    0.001 \n    0.002 \n    0.001 \n    0.001 \n    0.002 \n    0.001 \n  \n\n PANAS_14 \n    5 \n    0.182 \n    0.401 \n    0.83 \n    0.002 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n  \n\n PANAS_16 \n    5 \n    0.64 \n    0.17 \n    0.068 \n    0.001 \n    0 \n    0.001 \n    0.001 \n    0 \n    0.001 \n    0.001 \n    0 \n    0.001 \n    0.001 \n  \n\n PANAS_17 \n    5 \n    0 \n    0 \n    0.32 \n    0.032 \n    0.008 \n    0.008 \n    0 \n    0.011 \n    0.011 \n    0 \n    0.01 \n    0.01 \n    0 \n  \n\n PANAS_19 \n    5 \n    0.178 \n    0.25 \n    0.33 \n    0.002 \n    0 \n    0 \n    0 \n    0 \n    0.001 \n    0 \n    0 \n    0.001 \n    0 \n  \n\n PANAS_20 \n    5 \n    0.68 \n    0.349 \n    0.164 \n    0.001 \n    0 \n    0 \n    0 \n    0 \n    0.001 \n    0.001 \n    0 \n    0.001 \n    0.001 \n  \n\n\n\n\nWe can review the results regarding uniform/non-uniform DIF by looking at the chi* columns. Uniform DIF is indicated by column chi12 and non-uniform DIF by chi23, while column chi13 represents “an overall test of”total DIF effect” (Choi et al., 2011).\nWhile the table indicates significant chi2-tests for items 11 and 17, the magnitude estimates are low for these items.\nThere are some plots available as well, using the base R plot() function. For some reason the plots won’t render in this Quarto document, so I will try to sort that out at some point.\nCodeplot(g_dif) # use option `graphics.off()` to get the plots rendered one by one\n#plot(g_dif, graphics.off())\n\n\n\n\n5.8 Partial gamma DIF\nThe iarm package provides a function to assess DIF by partial gamma (Bjorner et al., 1998). It should be noted that this function only shows a single partial gamma value per item, so if you have more than two groups in your comparison, you will want to also use other methods to understand your results better.\nThere are some recommended cutoff-values mentioned in the paper above:\nNo or negligible DIF:\n\nGamma within the interval -0.21 to 0.21, or\n\nGamma not significantly different from 0\n\nSlight to moderate DIF:\n\nGamma within the interval -0.31 to 0.31 (and outside -0.21 to 0.21), or\n\nnot significantly outside the interval -0.21 to 0.21\n\nModerate to large DIF:\n\nGamma outside the interval -0.31 to 0.31, and\n\nsignificantly outside the interval -0.21 to 0.21\n\n\nCodeRIpartgamDIF(df, dif$Sex)\n\n\n\n Item \n    Partial gamma \n    SE \n    Lower CI \n    Upper CI \n    Adjusted p-value (BH) \n  \n\n PANAS_17 \n    0.234 \n    0.054 \n    0.127 \n    0.341 \n    0 \n  \n\n\n\nWe can see “slight” DIF for item 17, with a statistically significant gamma of .23."
  },
  {
    "objectID": "raschrvignette/RaschRvign.html#rasch-analysis-3",
    "href": "raschrvignette/RaschRvign.html#rasch-analysis-3",
    "title": "easyRasch vignette",
    "section": "\n6 Rasch analysis 3",
    "text": "6 Rasch analysis 3\nWhile there were no significant issues with DIF for any item/subgroup combination, we need to address the previously identified problem:\n\nItems 16 and 19 have the largest residual correlation.\n\nWe’ll remove item 19 since item 16 has better targeting.\n\nCoderemoved.items &lt;- c(removed.items,\"PANAS_19\")\n\ndf &lt;- df_backup %&gt;% \n  select(!any_of(removed.items))\n\ndf500 &lt;- df500_backup %&gt;% \n  select(!any_of(removed.items))\n\n\n\n\n\n\n\n\nitemnr\n\n\nitem\n\n\n\n\n\nPANAS_11\n\n\nDistressed\n\n\n\n\nPANAS_12\n\n\nUpset\n\n\n\n\nPANAS_13\n\n\nHostile\n\n\n\n\nPANAS_14\n\n\nIrritable\n\n\n\n\nPANAS_16\n\n\nAfraid\n\n\n\n\nPANAS_17\n\n\nAshamed\n\n\n\n\nPANAS_20\n\n\nJittery\n\n\n\n\n\n\n\nItem-restscore bootstrap\nItem fit\nCICC\nResidual correlations\nLRT\nTargeting\nItem hierarchy\n\n\n\n\nCodeRIbootRestscore(df, iterations = 250, samplesize = 800)\n\n\n\n Item \n    Item-restscore result \n    % of iterations \n    Conditional MSQ infit \n    Relative average item location \n  \n\n\n PANAS_20 \n    underfit \n    48.0 \n    1.16 \n    1.35 \n  \n\n PANAS_11 \n    underfit \n    13.2 \n    1.13 \n    1.07 \n  \n\n PANAS_12 \n    overfit \n    99.6 \n    0.78 \n    1.34 \n  \n\n PANAS_17 \n    overfit \n    10.4 \n    1.00 \n    1.74 \n  \n\n PANAS_16 \n    overfit \n    7.6 \n    0.94 \n    1.40 \n  \n\n\nNote: \n\n Results based on 250 bootstrap iterations with n = 800 and 7 items. Conditional mean-square infit based on complete responders only (n = 1851).\n\n\n\n\n\n\n\nCodesimfit3 &lt;- RIgetfit(df500, iterations = 200, cpu = 8)\nRIitemfit(df500, simfit3)\n\n\n\n Item \n    InfitMSQ \n    Infit thresholds \n    Infit diff \n    Relative location \n  \n\n\n PANAS_11 \n    1.103 \n    [0.838, 1.147] \n    no misfit \n    1.10 \n  \n\n PANAS_12 \n    0.742 \n    [0.855, 1.146] \n    0.113 \n    1.37 \n  \n\n PANAS_13 \n    1.119 \n    [0.866, 1.146] \n    no misfit \n    1.85 \n  \n\n PANAS_14 \n    0.914 \n    [0.871, 1.161] \n    no misfit \n    1.20 \n  \n\n PANAS_16 \n    0.97 \n    [0.841, 1.154] \n    no misfit \n    1.51 \n  \n\n PANAS_17 \n    1.043 \n    [0.856, 1.173] \n    no misfit \n    1.67 \n  \n\n PANAS_20 \n    1.155 \n    [0.854, 1.154] \n    0.001 \n    1.26 \n  \n\n\nNote: \n\n MSQ values based on conditional calculations (n = 500 complete cases).                                Simulation based thresholds from 200 simulated datasets.\n\n\n\n\n\n\n\nCodeRIciccPlot(df)\n\n\n\n\n\n\n\n\n\n\nCodesimcor3 &lt;- RIgetResidCor(df, iterations = 400, cpu = 8)\nRIresidcorr(df, cutoff = simcor3$p99)\n\n\n\n   \n    PANAS_11 \n    PANAS_12 \n    PANAS_13 \n    PANAS_14 \n    PANAS_16 \n    PANAS_17 \n    PANAS_20 \n  \n\n\n PANAS_11 \n     \n     \n     \n     \n     \n     \n     \n  \n\n PANAS_12 \n    -0.18 \n     \n     \n     \n     \n     \n     \n  \n\n PANAS_13 \n    -0.15 \n    -0.11 \n     \n     \n     \n     \n     \n  \n\n PANAS_14 \n    -0.22 \n    0.01 \n    -0.04 \n     \n     \n     \n     \n  \n\n PANAS_16 \n    -0.14 \n    -0.06 \n    -0.26 \n    -0.26 \n     \n     \n     \n  \n\n PANAS_17 \n    -0.2 \n    -0.09 \n    -0.14 \n    -0.22 \n    0 \n     \n     \n  \n\n PANAS_20 \n    -0.12 \n    -0.29 \n    -0.16 \n    -0.13 \n    -0.15 \n    -0.17 \n     \n  \n\n\nNote: \n\n Relative cut-off value is -0.056, which is 0.091 above the average correlation (-0.147).                                Correlations above the cut-off are highlighted in red text.\n\n\n\n\n\n\n\nCodeRIbootLRT(df, iterations = 1000, samplesize = 400, cpu = 8)\n\n\nConditional Likelihood Ratio Tests:\n\nConditional Likelihood Ratio Tests:\n\nConditional Likelihood Ratio Tests:\n\n\n\n\nResult\nn\nPercent\n\n\n\nNot statistically significant\n68\n6.8\n\n\nStatistically significant\n932\n93.2\n\n\n\n\n\n\n\n\nCodeRItargeting(df, bins = 45)\n\n\n\n\n\n\n\n\n\n\nCodeRIitemHierarchy(df)\n\n\n\n\n\n\n\n\n\n\n\n6.1 Analysis 3 comments\nThere are still problematic residual correlations and some items show misfit (most notably item 12 being overfit), but we will end this sample analysis here and move on to show other functions.\nThere are several item thresholds that are very closely located, as shown in the item hierarchy figure. This is not ideal, since it will inflate reliability estimates. However, we will not modify the response categories for this analysis, we only note that this is not workable and should be dealt with by trying variations of merged response categories to achieve better separation of threshold locations without disordering."
  },
  {
    "objectID": "raschrvignette/RaschRvign.html#rasch-analysis-4",
    "href": "raschrvignette/RaschRvign.html#rasch-analysis-4",
    "title": "RISEkbmRasch vignette",
    "section": "\n7 Rasch analysis 4",
    "text": "7 Rasch analysis 4\n\nCoderemoved.items &lt;- c(removed.items,\"PANAS_12\")\n\ndf2 &lt;- df2 %&gt;% \n  select(!any_of(removed.items))\n\n\n\n\n\n\n\n\nitemnr\n\n\nitem\n\n\n\n\n\nPANAS_11\n\n\nDistressed\n\n\n\n\nPANAS_13\n\n\nHostile\n\n\n\n\nPANAS_14\n\n\nIrritable\n\n\n\n\nPANAS_16\n\n\nAfraid\n\n\n\n\nPANAS_17\n\n\nAshamed\n\n\n\n\nPANAS_20\n\n\nJittery\n\n\n\n\n\n\n\nItem fit\nResidual correlations\nTargeting\nItem hierarchy\n\n\n\n\nCodeRIitemfitPCM2(df2, 350, 32, 8)\n\n\n\n   \n    OutfitMSQ \n    InfitMSQ \n    OutfitZSTD \n    InfitZSTD \n  \n\n\n PANAS_11 \n    0.926 \n    0.932 \n    -0.954 \n    -0.805 \n  \n\n PANAS_13 \n    0.907 \n    0.894 \n    -0.498 \n    -1.348 \n  \n\n PANAS_14 \n    0.85 \n    0.83 \n    -1.744 \n    -2.353 \n  \n\n PANAS_16 \n    0.795 \n    0.8 \n    -2 \n    -2.317 \n  \n\n PANAS_17 \n    0.802 \n    0.834 \n    -1.513 \n    -1.65 \n  \n\n PANAS_20 \n    0.96 \n    0.946 \n    -0.401 \n    -0.759 \n  \n\n\n\n\n\n\n\nCodeRIresidcorr(df2, cutoff = 0.2)\n\n\n\n   \n    PANAS_11 \n    PANAS_13 \n    PANAS_14 \n    PANAS_16 \n    PANAS_17 \n    PANAS_20 \n  \n\n\n PANAS_11 \n     \n     \n     \n     \n     \n     \n  \n\n PANAS_13 \n    -0.17 \n     \n     \n     \n     \n     \n  \n\n PANAS_14 \n    -0.22 \n    -0.03 \n     \n     \n     \n     \n  \n\n PANAS_16 \n    -0.15 \n    -0.26 \n    -0.24 \n     \n     \n     \n  \n\n PANAS_17 \n    -0.22 \n    -0.14 \n    -0.21 \n    0.01 \n     \n     \n  \n\n PANAS_20 \n    -0.17 \n    -0.2 \n    -0.16 \n    -0.19 \n    -0.2 \n     \n  \n\n\nNote: \n\n Relative cut-off value (highlighted in red) is 0.03, which is 0.2 above the average correlation.\n\n\n\n\n\n\n\nCodeRItargeting(df2)\n\n\n\n\n\n\n\n\n\n\nCodeRIitemHierarchy(df2)\n\n\n\n\n\n\n\n\n\n\n\n7.1 Analysis 4 comments\nThere are several item thresholds that are very closely located, as shown in the item hierarchy figure. This is not ideal, since it will inflate reliability estimates.\nHowever, we will not modify the response categories for this sample/simple analysis, we only note that this is not ideal."
  },
  {
    "objectID": "raschrvignette/RaschRvign.html#reliability",
    "href": "raschrvignette/RaschRvign.html#reliability",
    "title": "easyRasch vignette",
    "section": "\n7 Reliability",
    "text": "7 Reliability\n\nCodeRItif(df)\n\n\n\n\n\n\n\nThe figure above shows the Test Information Function (TIF), which indicates the reliability of all items making up the test/scale (not the reliability of the sample).\nThe default cutoff value used in RItif() is TIF = 3.33, which roughly corresponds to person separation index (PSI) = 0.7 (although this is not always the case). PSI is similar to reliability coefficients such as omega and alpha, ranging from 0 to 1. You can change the TIF cutoff by using the option cutoff, for instance cutoff = 2.5 (TIF values range from 1 and up).\nWhile 11.8% of respondents had a floor effect based on the raw sum scored data, the figure above shows us that 41.8% are located below the point where the items produce a PSI of 0.7 or higher. Again, note that this figure shows the reliability of the test/scale, not the sample. If you want to add the sample reliability use option samplePSI = TRUE. More details are available in the documentation ?RItif."
  },
  {
    "objectID": "raschrvignette/RaschRvign.html#person-fit",
    "href": "raschrvignette/RaschRvign.html#person-fit",
    "title": "easyRasch vignette",
    "section": "\n8 Person fit",
    "text": "8 Person fit\nWe can also look at how the respondents fit the Rasch model with these items. By default, RIpfit() outputs a histogram and a hex heatmap with the person infit ZSTD statistic, using +/- 1.96 as cutoff values. This is currently the only person fit method implemented in the easyRasch package, and the curious analyst is suggested to look at the package PerFit for more tools.\n\nCodeRIpfit(df)\n\n\n\n\n\n\n\n\n\n\n\n\n\nYou can export the person fit values to a new variable in the dataframe by specifying output = \"dataframe\", or if you just want the row numbers for respondents with deviant infit values, output = \"rowid\".\nYou can also specify a grouping variable to visualize the person fit for different groups.\n\nCodeRIpfit(df, group = dif$Sex, output = \"heatmap\")\n\n\n\n\n\n\n\nPerson fit is a useful way to identify respondents with unexpected response patterns and investigate this further.\n\n8.1 PerFit sample code\nWhile none of the functions in the PerFit package has been implemented in easyRasch, this is some code to get you started if you are interested in using it. There are multiple methods/functions available for polytomous and dichotomous data, see the package documentation.\nFor this example, we’ll use the non-parametric U3 statistic generalized to polytomous items (Emons, 2008) and the smaller sample.\n\n\nU3poly\nCutoff information\nFlagged respondents\n\n\n\n\nCodelibrary(PerFit)\npfit_u3poly &lt;- U3poly(matrix = df500, \n                      Ncat = 5, # make sure to input number of response categories, not thresholds\n                      IRT.PModel = \"PCM\")\n\n\n\n\n\nCodecutoff(pfit_u3poly)\n\n$Cutoff\n[1] 0.4546\n\n$Cutoff.SE\n[1] 0.0301\n\n$Prop.flagged\n[1] 0.088\n\n$Tail\n[1] \"upper\"\n\n$Cutoff.CI\n  2.5%  97.5% \n0.3759 0.4898 \n\nattr(,\"class\")\n[1] \"PerFit.cutoff\"\n\n\n\n\n\nCodeflagged.resp(pfit_u3poly) %&gt;% \n  pluck(\"Scores\") %&gt;% \n  as.data.frame() %&gt;% \n  arrange(desc(PFscores))\n\n   FlaggedID It1 It4 It7 It2 It5 It3 It6 PFscores\n1         89   0   0   0   0   0   0   1   1.0000\n2        191   0   0   0   0   0   0   1   1.0000\n3        255   0   0   0   0   0   0   2   1.0000\n4        303   0   0   0   0   0   0   1   1.0000\n5        398   0   0   0   0   0   3   0   0.9360\n6        499   0   0   0   0   0   3   0   0.9360\n7         54   0   0   0   0   0   2   0   0.8965\n8        332   0   0   0   0   0   1   0   0.8643\n9        373   0   0   0   0   0   1   0   0.8643\n10       455   0   0   0   0   0   1   0   0.8643\n11       465   0   0   0   0   0   1   0   0.8643\n12       392   0   2   0   4   4   0   4   0.8242\n13         7   4   0   0   0   0   0   4   0.7530\n14       482   4   4   0   4   0   4   0   0.7505\n15        27   1   0   0   0   4   0   0   0.7007\n16       283   0   0   4   0   0   0   0   0.6717\n17       327   0   0   4   0   0   0   0   0.6717\n18       147   0   0   0   0   1   0   0   0.6494\n19       258   0   0   0   0   1   0   0   0.6494\n20       361   0   0   0   0   1   0   0   0.6494\n21       324   1   0   0   0   4   0   1   0.6442\n22       132   0   0   0   2   0   0   0   0.6244\n23       180   1   4   0   4   0   0   0   0.6131\n24        23   4   4   4   4   4   0   0   0.6013\n25       437   4   0   0   0   0   0   0   0.5870\n26        83   0   4   4   4   2   2   4   0.5840\n27       459   3   0   0   3   3   0   4   0.5636\n28        92   3   0   2   0   0   0   4   0.5627\n29       244   0   1   0   0   0   3   0   0.4999\n30       390   1   4   3   2   0   3   4   0.4831\n31       216   2   0   0   0   0   2   3   0.4734\n32       236   4   0   1   0   0   2   0   0.4674\n33       204   0   0   0   1   0   0   0   0.4623\n34       411   0   0   0   1   0   0   0   0.4623\n35       352   0   0   1   0   0   2   0   0.4606\n36        75   1   2   2   0   0   0   4   0.4588\n37        84   0   3   0   1   0   3   0   0.4586\n38        45   0   0   2   0   0   0   0   0.4564\n39        98   0   0   2   0   0   0   0   0.4564\n40       111   0   0   2   0   0   0   0   0.4564\n41       247   0   0   2   0   0   0   0   0.4564\n42       263   0   0   2   0   0   0   0   0.4564\n43       406   0   0   2   0   0   0   0   0.4564\n44       431   0   0   2   0   0   0   0   0.4564\n45       380   4   4   0   3   3   2   4   0.4537\n\n\n\n\n\nThe dataframe shown under the tab Flagged respondents above contains a variable named FlaggedID which represents the row id’s. This variable is useful if one wants to filter out respondents with deviant response patterns (person misfit). There are indications that persons with misfit may affect results of Andersen’s LR-test for DIF (Artner, 2016).\n\n8.2 Item fit without aberrant responses\nWe can remove the misfitting persons to see how that affects item fit. Let’s also compare with the misfitting respondents identified by RIpfit(), also using the smaller sample.\n\nCodemisfits &lt;- flagged.resp(pfit_u3poly) %&gt;% \n  pluck(\"Scores\") %&gt;% \n  as.data.frame() %&gt;% \n  pull(FlaggedID)\n\nmisfits2 &lt;- RIpfit(df500, output = \"rowid\")\n\n\n\n\nAll respondents\nU3 misfit removed\nZSTD misfit removed\n\n\n\n\nCodeRIitemfit(df500, simcut = simfit3)\n\n\n\n Item \n    InfitMSQ \n    Infit thresholds \n    Infit diff \n    Relative location \n  \n\n\n PANAS_11 \n    1.103 \n    [0.838, 1.147] \n    no misfit \n    1.10 \n  \n\n PANAS_12 \n    0.742 \n    [0.855, 1.146] \n    0.113 \n    1.37 \n  \n\n PANAS_13 \n    1.119 \n    [0.866, 1.146] \n    no misfit \n    1.85 \n  \n\n PANAS_14 \n    0.914 \n    [0.871, 1.161] \n    no misfit \n    1.20 \n  \n\n PANAS_16 \n    0.97 \n    [0.841, 1.154] \n    no misfit \n    1.51 \n  \n\n PANAS_17 \n    1.043 \n    [0.856, 1.173] \n    no misfit \n    1.67 \n  \n\n PANAS_20 \n    1.155 \n    [0.854, 1.154] \n    0.001 \n    1.26 \n  \n\n\nNote: \n\n MSQ values based on conditional calculations (n = 500 complete cases).                                Simulation based thresholds from 200 simulated datasets.\n\n\n\n\n\n\n\nCodeRIitemfit(df500[-misfits,], simcut = simfit3)\n\n\n\n Item \n    InfitMSQ \n    Infit thresholds \n    Infit diff \n    Relative location \n  \n\n\n PANAS_11 \n    1.169 \n    [0.838, 1.147] \n    0.022 \n    1.25 \n  \n\n PANAS_12 \n    0.792 \n    [0.855, 1.146] \n    0.063 \n    1.58 \n  \n\n PANAS_13 \n    1.015 \n    [0.866, 1.146] \n    no misfit \n    2.25 \n  \n\n PANAS_14 \n    1.023 \n    [0.871, 1.161] \n    no misfit \n    1.40 \n  \n\n PANAS_16 \n    0.958 \n    [0.841, 1.154] \n    no misfit \n    1.79 \n  \n\n PANAS_17 \n    1 \n    [0.856, 1.173] \n    no misfit \n    2.30 \n  \n\n PANAS_20 \n    1.108 \n    [0.854, 1.154] \n    no misfit \n    1.43 \n  \n\n\nNote: \n\n MSQ values based on conditional calculations (n = 447 complete cases).                                Simulation based thresholds from 200 simulated datasets.\n\n\n\n\n\n\n\nCodeRIitemfit(df500[-misfits2,], simcut = simfit3)\n\n\n\n Item \n    InfitMSQ \n    Infit thresholds \n    Infit diff \n    Relative location \n  \n\n\n PANAS_11 \n    1.088 \n    [0.838, 1.147] \n    no misfit \n    1.21 \n  \n\n PANAS_12 \n    0.761 \n    [0.855, 1.146] \n    0.094 \n    1.52 \n  \n\n PANAS_13 \n    1.159 \n    [0.866, 1.146] \n    0.013 \n    1.95 \n  \n\n PANAS_14 \n    0.938 \n    [0.871, 1.161] \n    no misfit \n    1.30 \n  \n\n PANAS_16 \n    0.94 \n    [0.841, 1.154] \n    no misfit \n    1.68 \n  \n\n PANAS_17 \n    1.008 \n    [0.856, 1.173] \n    no misfit \n    1.89 \n  \n\n PANAS_20 \n    1.125 \n    [0.854, 1.154] \n    no misfit \n    1.36 \n  \n\n\nNote: \n\n MSQ values based on conditional calculations (n = 457 complete cases).                                Simulation based thresholds from 200 simulated datasets."
  },
  {
    "objectID": "raschrvignette/RaschRvign.html#item-parameters",
    "href": "raschrvignette/RaschRvign.html#item-parameters",
    "title": "easyRasch vignette",
    "section": "\n9 Item parameters",
    "text": "9 Item parameters\nTo allow others (and oneself) to use the item parameters estimated for estimation of person locations/thetas, we should make the item parameters available. The function will also write a csv-file with the item threshold locations. Estimations of person locations/thetas can be done with the thetaEst() function from the catR package. This is implemented in the function RIestThetasOLD(), see below for details.\nFirst, we’ll output the parameters into a table.\n\nCodeRIitemparams(df)\n\n\n\n   \n    Threshold 1 \n    Threshold 2 \n    Threshold 3 \n    Threshold 4 \n    Item location \n  \n\n\n PANAS_11 \n    -1.63 \n    -0.67 \n    -0.22 \n    1.21 \n    -0.33 \n  \n\n PANAS_12 \n    -0.80 \n    -0.39 \n    0.06 \n    0.89 \n    -0.06 \n  \n\n PANAS_13 \n    -0.29 \n    -0.15 \n    0.56 \n    1.53 \n    0.42 \n  \n\n PANAS_14 \n    -1.38 \n    -0.59 \n    -0.16 \n    0.87 \n    -0.32 \n  \n\n PANAS_16 \n    -0.59 \n    -0.44 \n    -0.06 \n    1.08 \n    0 \n  \n\n PANAS_17 \n    -0.06 \n    0.01 \n    0.48 \n    0.93 \n    0.34 \n  \n\n PANAS_20 \n    -1.27 \n    -0.61 \n    0.35 \n    1.32 \n    -0.05 \n  \n\n\nNote: \n\n Item location is the average of the thresholds for each item.\n\n\n\n\nThe parameters can also be output to a dataframe or a file, using the option output = \"dataframe\" or output = \"file\"."
  },
  {
    "objectID": "raschrvignette/RaschRvign.html#ordinal-sum-score-to-interval-score",
    "href": "raschrvignette/RaschRvign.html#ordinal-sum-score-to-interval-score",
    "title": "easyRasch vignette",
    "section": "\n10 Ordinal sum score to interval score",
    "text": "10 Ordinal sum score to interval score\nThis table shows the corresponding “raw” ordinal sum score values and logit scores, with standard errors for each logit value. Interval scores are estimated using WL based on a simulated dataset using the item parameters estimated from the input dataset. The choice of WL as default is due to the lower bias compared to ML estimation (Warm, 1989).\n(An option will hopefully be added at some point to create this table based on only item parameters.)\n\nCodeRIscoreSE(df)\n\n\n\n Ordinal sum score \n    Logit score \n    Logit std.error \n  \n\n\n 0 \n    -3.642 \n    0.620 \n  \n\n 1 \n    -2.543 \n    0.716 \n  \n\n 2 \n    -2.032 \n    0.653 \n  \n\n 3 \n    -1.693 \n    0.578 \n  \n\n 4 \n    -1.437 \n    0.515 \n  \n\n 5 \n    -1.228 \n    0.468 \n  \n\n 6 \n    -1.050 \n    0.432 \n  \n\n 7 \n    -0.893 \n    0.406 \n  \n\n 8 \n    -0.750 \n    0.386 \n  \n\n 9 \n    -0.618 \n    0.371 \n  \n\n 10 \n    -0.493 \n    0.361 \n  \n\n 11 \n    -0.373 \n    0.353 \n  \n\n 12 \n    -0.258 \n    0.348 \n  \n\n 13 \n    -0.144 \n    0.346 \n  \n\n 14 \n    -0.031 \n    0.346 \n  \n\n 15 \n    0.083 \n    0.348 \n  \n\n 16 \n    0.198 \n    0.352 \n  \n\n 17 \n    0.317 \n    0.359 \n  \n\n 18 \n    0.441 \n    0.368 \n  \n\n 19 \n    0.572 \n    0.381 \n  \n\n 20 \n    0.712 \n    0.397 \n  \n\n 21 \n    0.866 \n    0.418 \n  \n\n 22 \n    1.037 \n    0.447 \n  \n\n 23 \n    1.233 \n    0.484 \n  \n\n 24 \n    1.464 \n    0.534 \n  \n\n 25 \n    1.747 \n    0.601 \n  \n\n 26 \n    2.118 \n    0.681 \n  \n\n 27 \n    2.666 \n    0.745 \n  \n\n 28 \n    3.804 \n    0.638 \n  \n\n\n\n\n\n10.1 Ordinal/interval figure\nThe figure below can also be generated to illustrate the relationship between ordinal sum score and logit interval score. The errorbars default to show the standard error at each point, multiplied by 1.96.\n\nCodeRIscoreSE(df, output = \"figure\")\n\n\n\n\n\n\n\n\n10.2 Estimating interval level person scores\nBased on the Rasch analysis output of item parameters, we can estimate each individuals location or score (also known as “theta”). RIestThetas() by default uses WLE estimation based on item parameters from a partial credit model (PCM) and outputs a dataframe with person locations (WLE) and measurement error (SEM) on the logit scale.\n\nCodethetas &lt;- RIestThetas(df)\n\nhead(thetas)\n\n         WLE       SEM\n1 -2.5430672 0.7160122\n2 -2.0319918 0.6533338\n3 -0.1439296 0.3460439\n4 -3.6420132 0.6202853\n5 -2.0319918 0.6533338\n6  0.1980813 0.3520345\n\n\nEach individual has a standard error of measurement (SEM) associated with their estimated location/score. This is included in the output of the RIestThetas() function as the SEM variable, as seen above. We can review the distribution of measurement error with a figure.\nWe can take a look at the distribution of person locations (thetas) using a histogram.\n\nCodehist(thetas$WLE, \n     col = \"#009ca6\", \n     main = \"Histogram of person locations (thetas)\", \n     breaks = 20)\n\n\n\n\n\n\n\nRIestThetasOLD() can be used with a pre-specified item (threshold) location matrix. The choice of WL as default is due to the lower bias compared to ML estimation (Warm, 1989). Similarly to RIscoreSE() you can (and may indeed need to) change the range of logit scores, using the option theta_range. The default is c(-7,7), which should hopefully work in most circumstances.\nIf you would like to use an existing item threshold location matrix, this code may be helpful:\n\nCodeitemParameters &lt;- read_csv(\"itemParameters.csv\") %&gt;% \n  as.matrix()\nitemParameters\n\n     Threshold 1 Threshold 2 Threshold 3 Threshold 4\n[1,]     -1.2382     -0.3288      0.0666      1.4649\n[2,]      0.0623      0.1480      0.8241      1.7680\n[3,]     -0.9915     -0.2566      0.1231      1.1222\n[4,]     -0.2179     -0.1217      0.2099      1.3248\n[5,]      0.2868      0.3061      0.7319      1.1655\n[6,]     -0.8929     -0.2860      0.6297      1.5673\n\n\nAs you can see, this is a matrix object (not a dataframe), with each item as a row, and the threshold locations as columns."
  },
  {
    "objectID": "raschrvignette/RaschRvign.html#figure-design",
    "href": "raschrvignette/RaschRvign.html#figure-design",
    "title": "easyRasch vignette",
    "section": "\n11 Figure design",
    "text": "11 Figure design\nMost of the figures created by the functions can be styled (colors, fonts, etc) by adding theme settings to them. You can use the standard ggplot function theme() and related theme-functions. As usual it is possible to “stack” theme functions, as seen in the example below.\nYou can also change coloring, axis limits/breaks, etc, just by adding ggplot options with a + sign.\nA custom theme function, theme_rise(), is included in the easyRasch package. It might be easier to use if you are not familiar with theme().\nFor instance, you might like to change the font to “Lato” for the item hierarchy figure, and make the background transparent.\n\nCodeRIitemHierarchy(df) +\n  theme_minimal() + # first apply the minimal theme to make the background transparent\n  theme_rise(fontfamily = \"Lato\") # then apply theme_rise, which simplifies making changes to all plot elements\n\n\n\n\n\n\n\nAs of package version 0.1.30.0, the RItargeting() function allows more flexibility in styling too, by having an option to return a list object with the three separate plots. See the NEWS file for more details. Since the RItargeting() function uses the patchwork library to combine plots, you can also make use of the many functions that patchwork includes. For instance, you can set a title with a specific theme:\n\nCodeRItargeting(df) + plot_annotation(title = \"Targeting\", theme = theme_rise(fontfamily = \"Arial\"))\n\n\n\n\n\n\n\nIn order to change font for text inside plots (such as “t1” for thresholds) you will need to add an additional line of code.\nupdate_geom_defaults(\"text\", list(family = \"Lato\"))\nPlease note that the line of code above updates the default settings for geom_text() for the whole session. Also, some functions, such as RIloadLoc(), make use of geom_text_repel(), for which you would need to change the code above from “text” to “text_repel”.\nA simple way to only change font family and font size would be to use theme_minimal(base_family = \"Calibri\", base_size = 14). Please see the reference page for default ggplot themes for alternatives to theme_minimal()."
  },
  {
    "objectID": "raschrvignette/RaschRvign.html#software-used",
    "href": "raschrvignette/RaschRvign.html#software-used",
    "title": "RISEkbmRasch vignette",
    "section": "\n13 Software used",
    "text": "13 Software used\nThe grateful package is a nice way to give credit to the packages used in making the analysis. The package can create both a bibliography file and a table object, which is handy for automatically creating a reference list based on the packages used (or at least explicitly loaded).\n\nCodelibrary(grateful)\npkgs &lt;- cite_packages(cite.tidyverse = TRUE, \n                      output = \"table\",\n                      bib.file = \"grateful-refs.bib\",\n                      include.RStudio = TRUE,\n                      out.dir = getwd())\n# If kbl() is used to generate this table, the references will not be added to the Reference list.\nformattable(pkgs, \n            table.attr = 'class=\\\"table table-striped\\\" style=\"font-size: 13px; font-family: Lato; width: 80%\"')\n\n\n\n\n\nPackage\n\n\nVersion\n\n\nCitation\n\n\n\n\n\nbase\n\n\n4.2.3\n\n\nR Core Team (2023)\n\n\n\n\ncar\n\n\n3.1.2\n\n\nFox & Weisberg (2019)\n\n\n\n\neRm\n\n\n1.0.4\n\n\nMair & Hatzinger (2007b); Mair & Hatzinger (2007a); Hatzinger & Rusch (2009); Rusch et al. (2013); Koller et al. (2015); Debelak & Koller (2019)\n\n\n\n\nforeach\n\n\n1.5.2\n\n\nMicrosoft & Weston (2022)\n\n\n\n\nformattable\n\n\n0.2.1\n\n\nRen & Russell (2021)\n\n\n\n\nfurrr\n\n\n0.3.1\n\n\nVaughan & Dancho (2022)\n\n\n\n\nggrepel\n\n\n0.9.4\n\n\nSlowikowski (2023)\n\n\n\n\nglue\n\n\n1.6.2\n\n\nHester & Bryan (2022)\n\n\n\n\nkableExtra\n\n\n1.3.4\n\n\nZhu (2021)\n\n\n\n\nknitr\n\n\n1.42\n\n\nXie (2014); Xie (2015); Xie (2023)\n\n\n\n\nmatrixStats\n\n\n0.63.0\n\n\nBengtsson (2022)\n\n\n\n\nmirt\n\n\n1.41\n\n\nChalmers (2012)\n\n\n\n\npatchwork\n\n\n1.1.3\n\n\nPedersen (2023)\n\n\n\n\npsych\n\n\n2.3.6\n\n\nWilliam Revelle (2023)\n\n\n\n\npsychotree\n\n\n0.16.0\n\n\nTrepte & Verbeet (2010); Strobl et al. (2011); Strobl et al. (2015a); Komboz et al. (2018); Wickelmaier & Zeileis (2018)\n\n\n\n\nreshape\n\n\n0.8.9\n\n\nWickham (2007)\n\n\n\n\nRISEkbmRasch\n\n\n0.1.30.2\n\n\nJohansson (2023)\n\n\n\n\nrmarkdown\n\n\n2.22\n\n\nXie et al. (2018); Xie et al. (2020); Allaire et al. (2023)\n\n\n\n\nTAM\n\n\n4.1.4\n\n\n@\n\n\n\n\ntidyverse\n\n\n2.0.0\n\n\nWickham et al. (2019)"
  },
  {
    "objectID": "raschrvignette/RaschRvign.html#additional-credits",
    "href": "raschrvignette/RaschRvign.html#additional-credits",
    "title": "easyRasch vignette",
    "section": "\n13 Additional credits",
    "text": "13 Additional credits\nThanks to my colleagues at RISE for providing feedback and testing the package on Windows and MacOS platforms. Also, thanks to Mike Linacre and Jeanette Melin for providing useful feedback to improve this vignette."
  },
  {
    "objectID": "powerviz.html",
    "href": "powerviz.html",
    "title": "Power analysis for multilevel models",
    "section": "",
    "text": "I recently needed to do a power analysis for a longitudinal study, and since I always document my explorative work in a Quarto file it seemed reasonable to take a few minutes to share my experiences.\nThe starting point for me was the “Basic example” made by the author of the powerlmm package.\nYou may need to install the package from the GitHub website:\nCodelibrary(powerlmm)\nlibrary(tidyverse)\n\n# define ggplot theme\ntheme_rise &lt;- function(fontfamily = \"Lato\", axissize = 13, titlesize = 15,\n                       margins = 12, axisface = \"plain\", stripsize = 12,\n                       panelDist = 0.6, legendSize = 11, legendTsize = 12) {\n  theme_minimal() +\n  theme(\n    text = element_text(family = fontfamily),\n    axis.title.x = element_text(\n      margin = margin(t = margins),\n      size = axissize\n    ),\n    axis.title.y = element_text(\n      margin = margin(r = margins),\n      size = axissize\n    ),\n    plot.title = element_text(\n      face = \"bold\",\n      size = titlesize\n    ),\n    axis.title = element_text(\n      face = axisface\n    ),\n    plot.caption = element_text(\n      face = \"italic\"\n    ),\n    legend.text = element_text(family = fontfamily, size = legendSize),\n    legend.title = element_text(family = fontfamily, size = legendTsize),\n    legend.background = element_rect(color = \"lightgrey\"),\n    strip.text = element_text(size = stripsize),\n    strip.background = element_rect(color = \"lightgrey\"),\n    panel.spacing = unit(panelDist, \"cm\", data = NULL)\n  )\n}\nI made some changes in the example regarding time points, clusters and icc’s. Below is a test run for one combination of parameter settings.\nCode# dropout per treatment group\nd &lt;- per_treatment(control = dropout_weibull(0.25, 1),\n              treatment = dropout_weibull(0.25, 1))\n# Setup design\np &lt;- study_parameters(n1 = 5, # time points\n                      n2 = 10, # subjects per cluster\n                      n3 = 4, # clusters per treatment arm\n                      icc_pre_subject = 0.5,\n                      icc_pre_cluster = 0.2,\n                      icc_slope = 0.05,\n                      var_ratio = 0.02,\n                      dropout = d,\n                      cohend = -0.8)\n# Power\npowerAnalysis &lt;- get_power(p)\n\npowerAnalysis$power\n\n[1] 0.7626795"
  },
  {
    "objectID": "powerviz.html#creating-a-function",
    "href": "powerviz.html#creating-a-function",
    "title": "Power analysis for multilevel models",
    "section": "\n1 Creating a function",
    "text": "1 Creating a function\nWe want to be able to vary some of the parameters and compare the resulting power analysis output.\nFor this example, we will create a function that allows us to vary three of the input parameters:\n\nthe number of participants per cluster\nattrition/dropout rate\neffect size\n\n\nCodepowerFunc &lt;- function(samplesize = 15, dropoutrate = 0.25, effectsize = -0.8) {\n  d &lt;- per_treatment(\n    control = dropout_weibull(dropoutrate, 1),\n    treatment = dropout_weibull(dropoutrate, 1)\n  )\n\n  p &lt;- study_parameters(\n    n1 = 5, # time points\n    n2 = samplesize, # subjects per cluster\n    n3 = 4, # clusters per treatment arm\n    icc_pre_subject = 0.5,\n    icc_pre_cluster = 0.2,\n    icc_slope = 0.05,\n    var_ratio = 0.02,\n    dropout = d,\n    cohend = effectsize\n  )\n  paout &lt;- get_power(p)\n  return(paout$power)\n}\n\n\nLet’s test the function.\n\nCodepowerFunc()\n\n[1] 0.8937192"
  },
  {
    "objectID": "powerviz.html#specifying-parameter-variations",
    "href": "powerviz.html#specifying-parameter-variations",
    "title": "Power analysis for multilevel models",
    "section": "\n2 Specifying parameter variations",
    "text": "2 Specifying parameter variations\nNext, we will define variables with the parameter variations we want to look at.\n\nCode# set sample sizes\nsampleSizes &lt;- c(10, 12, 14, 16, 18, 20)\n# set dropout rates\ndropoutRates &lt;- c(0.20, 0.25, 0.30)\n# set effect sizes\neffectSizes &lt;- c(-0.6, -0.7, -0.8)\n\n\nAnd now we utilize the magic of expand_grid() to create a dataframe with all combinations of the three variables.\n\nCode# make a dataframe with all combinations\ncombinations &lt;- expand_grid(sampleSizes, dropoutRates, effectSizes)"
  },
  {
    "objectID": "powerviz.html#power-analysis",
    "href": "powerviz.html#power-analysis",
    "title": "Power analysis for multilevel models",
    "section": "\n3 Power analysis",
    "text": "3 Power analysis\nWe’ll make use of pmap() from the purrr:map() family since we need to input more than two variables.\n\nCode# use pmap to iterate over all combinations\npowerList &lt;- pmap(\n  list(\n    combinations$sampleSizes,\n    combinations$dropoutRates,\n    combinations$effectSizes\n  ),\n  powerFunc\n) %&gt;%\n  # set readable names for each output list object, to later separate into variables\n  set_names(paste0(\n    combinations$sampleSizes, \"_\",\n    combinations$dropoutRates, \"_\",\n    combinations$effectSizes\n  ))\n\n\nThe names we set at the end of the chunk above will help us to create separate variables next.\n\nCode# combine into dataframe for visualization\ndf.power &lt;- powerList %&gt;%\n  bind_rows() %&gt;%\n  t() %&gt;%\n  as.data.frame() %&gt;%\n  rownames_to_column(\"split_this\") %&gt;%\n  rename(power = V1) %&gt;%\n  separate(split_this,\n    into = c(\"samplesize\", \"dropoutrate\", \"effectsize\"),\n    sep = \"_\"\n  ) %&gt;%\n  mutate(across(everything(), ~ as.numeric(.x)))\n\nglimpse(df.power)\n\nRows: 54\nColumns: 4\n$ samplesize  &lt;dbl&gt; 10, 10, 10, 10, 10, 10, 10, 10, 10, 12, 12, 12, 12, 12, 12…\n$ dropoutrate &lt;dbl&gt; 0.20, 0.20, 0.20, 0.25, 0.25, 0.25, 0.30, 0.30, 0.30, 0.20…\n$ effectsize  &lt;dbl&gt; -0.6, -0.7, -0.8, -0.6, -0.7, -0.8, -0.6, -0.7, -0.8, -0.6…\n$ power       &lt;dbl&gt; 0.5415203, 0.6703958, 0.7798685, 0.5241826, 0.6515921, 0.7…"
  },
  {
    "objectID": "powerviz.html#visualization",
    "href": "powerviz.html#visualization",
    "title": "Power analysis for multilevel models",
    "section": "\n4 Visualization",
    "text": "4 Visualization\n\nCodedf.power %&gt;%\n  mutate(dropoutrate = factor(dropoutrate,\n                              labels = c(\"20%\",\"25%\",\"30%\")),\n         effectsize = fct_rev(factor(effectsize))) %&gt;%\n  \n  ggplot(aes(\n    x = samplesize,\n    y = power,\n    color = dropoutrate,\n    group = dropoutrate\n  )) +\n  geom_point() +\n  geom_line() +\n  scale_x_continuous(breaks = sampleSizes) +\n  scale_y_continuous(\n    limits = c(0, 1),\n    breaks = c(0, 0.2, 0.4, 0.6, 0.8, 1)\n  ) +\n  labs(\n    x = \"Cluster sample size\",\n    y = \"Statistical power\",\n    title = \"Power analysis\",\n    subtitle = \"For different effect sizes (Cohen's d)\",\n    caption = \"Note. There are four clusters per treatment arm and five time points.\"\n  ) +\n  theme_minimal(\n    base_family = \"Lato\",\n    base_size = 14\n  ) +\n  facet_wrap(~effectsize) +\n  geom_hline(\n    yintercept = 0.8,\n    linetype = 2\n  ) +\n  scale_color_viridis_d('Dropout rate') +\n  theme_rise()\n\n\n\n\n\n\n\nI added a reference line for power = 0.80."
  },
  {
    "objectID": "datawrangling.html",
    "href": "datawrangling.html",
    "title": "Data wrangling for psychometrics in R",
    "section": "",
    "text": "While the easyRasch R package simplifies the process of doing Rasch analysis in R, users still need to be able to import and often make modifications to their data. This demands some knowledge in R data wrangling. For a comprehensive treatment of this topic, please see the “R for data science” book. This guide focuses on common data wrangling issues that occur in psychometric analyses.\nI will rely heavily on library(tidyverse) functions in most examples.\nThis page will be updated intermittently. The planned content includes:"
  },
  {
    "objectID": "datawrangling.html#dividing-a-dataset",
    "href": "datawrangling.html#dividing-a-dataset",
    "title": "Data wrangling for psychometrics in R",
    "section": "2 Dividing a dataset",
    "text": "2 Dividing a dataset"
  },
  {
    "objectID": "datawrangling.html#recoding-response-categories",
    "href": "datawrangling.html#recoding-response-categories",
    "title": "Data wrangling for psychometrics in R",
    "section": "3 Recoding response categories",
    "text": "3 Recoding response categories\nMany options are available. I have settled on primarily using car::recode() from library(car) since I find the syntax to be logical and consistent. Another option to consider is dplyr::case_when().\nThe basic syntax of car::recode() (henceforth referred to as only recode()) is this:\nrecode(variable_to_recode, \"newvalue1=oldvalue1;newvalue2=oldvalue2\", as.factor = FALSE/TRUE)\nAs you can see, semi-colon ; is used to separate recodings. When numbers are recoded, you just write them out as 1=0. When characters are involved, you need the single quote symbol to enclose the character string, i.e 'Never'=0 for recoding to numerics (which also necessitates the option as.factor = FALSE if you want the recoded variable to be numeric), or 'Never'='Nevver' when you need to correct misspelled response options.\n\n\n\n\n\n\nTip\n\n\n\nRemember to define your preferred recoding function in your script to avoid headaches due to unexpected error messages. This is done by using the assignment operator &lt;-. Example below.\nrecode &lt;- car::recode\n\n\nThere are some more or less clever ways to use recode(). You can simply copy&paste code for each variable, such as:\ndf$q1 &lt;- recode(df$q1, \"1=0;2=1;3=2\", as.factor = FALSE)\ndf$q2 &lt;- recode(df$q2, \"1=0;2=1;3=2\", as.factor = FALSE)\ndf$q3 &lt;- recode(df$q3, \"1=0;2=1;3=2\", as.factor = FALSE)\ndf$q4 &lt;- recode(df$q4, \"1=0;2=1;3=2\", as.factor = FALSE)\ndf$q5 &lt;- recode(df$q5, \"1=0;2=1;3=2\", as.factor = FALSE)\nSuch an approach might actually be sensible if each variable needed different recodings. But when you want to make the same changes to many variables, there are of course more efficient strategies. The code above could be rewritten as:\ndf %&gt;% \n  mutate(across(q1:q5, ~ car::recode(.x, \"1=0;2=1;3=2\", as.factor = FALSE)))\nOr, in this case, where we only want to recode our scale from the range of 1-3 to 0-2, i.e. subtract one:\ndf %&gt;% \n  mutate(across(q1:q5, ~ .x - 1))\nThe syntax in these two examples is related to how tidyverse uses unnamed functions by the combination of ~ and .x, where the latter becomes a placeholder for the variables defined in the first term of across().\nIt is good practice to check that your recoding worked as intended. The first step I recommend is RItileplot().\ndf %&gt;% \n  mutate(across(q1:q5, ~ car::recode(.x, \"1=0;2=1;3=2\", as.factor = FALSE))) %&gt;% \n  RItileplot()\nIf you are trying out different ways to merge response categories to resolve issues with disordered thresholds, you may also want to review the probability curves before committing your recode to a new data object.\ndf %&gt;% \n  mutate(across(q1:q5, ~ car::recode(.x, \"1=0;2=1;3=2\", as.factor = FALSE))) %&gt;% \n  RIitemCats(legend = \"left\")\nBelow is an example where we have multiple cell contents that we want to recode to NA to ensure that R interprets them as missing data. We want to recode three different things:\n\nall the numbers from 990 to 999 (usually a way to differentiate between types of missing data)\nblank cells\n“Don’t know” responses\n\ndf$q45 &lt;- recode(df$q45,\"990:999=NA;''=NA;'Don't know'=NA\")\nThe : means that all numbers from 990 to 999 will be recoded into NA."
  },
  {
    "objectID": "datawrangling.html#item-split",
    "href": "datawrangling.html#item-split",
    "title": "Data wrangling for psychometrics in R",
    "section": "4 Item split",
    "text": "4 Item split\nIt can be desirable to split an item due to issues with DIF. This refers to taking a variable and creating two (or more) replacement variables, one for each demographic group. For each variable, data will be missing for the other group. Often this is relevant for gender DIF, which we will use in this example.\nThis example assumes that there is a (DIF) vector variable dif.gender with the gender data, which has the same length as the number of rows in the dataset df. We’ll create a new dataframe to store the dataset with the item split. Item q10 is the one we want to split.\ndf.q10split &lt;- df %&gt;% \n  add_column(gender = dif.gender) %&gt;% \n  mutate(q10f = if_else(gender == \"Female\", q10, NA), # create variable q10f when gender is \"Female\"\n         q10m = if_else(gender == \"Male\", q10, NA)\n         ) %&gt;% \n  select(!gender) %&gt;% # remove gender and q10 variables from the dataset\n  select(!q10)\n\n# check the data\nRItileplot(df.q10split)\nThe if_else() function used within mutate() has three inputs/options in the example above:\n\ncondition (logical statement)\nassignment if the condition true\nassignment if false"
  },
  {
    "objectID": "datawrangling.html#item-merge",
    "href": "datawrangling.html#item-merge",
    "title": "Data wrangling for psychometrics in R",
    "section": "5 Item merge",
    "text": "5 Item merge\nSometimes it is desirable to merge two items into one. This is often done when there is a high residual correlation between two items. This is called a testlet. The items are merged into a new variable, and the original items are removed from the dataset.\nIt is usually a good idea to create a new dataframe with the merged variable, in case you need to go back to the original dataset.\ndf2 &lt;- df %&gt;% \n  mutate(sdq2_15 = sdq2 + sdq15) %&gt;% # create variable by adding them\n  select(!sdq2) %&gt;% \n  select(!sdq15)\nThen you should check the ICC curves for the merged item:\nRIitemCats(df2, item = \"sdq2_15\")"
  },
  {
    "objectID": "datawrangling.html#links",
    "href": "datawrangling.html#links",
    "title": "Data wrangling for psychometrics in R",
    "section": "8 Links",
    "text": "8 Links\nhttps://github.com/Cghlewis/data-wrangling-functions/wiki"
  },
  {
    "objectID": "datawrangling.html#checking-response-distribution-prior-to-dif-analysis",
    "href": "datawrangling.html#checking-response-distribution-prior-to-dif-analysis",
    "title": "Data wrangling for psychometrics in R",
    "section": "6 Checking response distribution prior to DIF analysis",
    "text": "6 Checking response distribution prior to DIF analysis\nThis example assumes that you have previously created a DIF variable dif.gender for gender using two factor levels, using the labels Female and Male.\nThe reason for doing this is making sure that there are no empty cells (particularly in lower response categories) in either group prior to running the DIF analysis, since this could lead to DIF being indicated incorrectly.\ndifplots &lt;- df %&gt;% \n  add_column(dif = dif.gender) %&gt;% \n  split(.$dif) %&gt;%\n  map(~ RItileplot(.x %&gt;% select(!dif)) + labs(title = .x$dif))\n  \nlibrary(patchwork)\ndifplots$Female + difplots$Male\nThe last part will make the tile plots show up side by side, labeled with the gender variable as it is coded in the dataset. You will need to adapt the code to the factor labels in your own dataset. You can also access the plots using difplots[[1]] for the first plot. There will be one plot for each level of your factor variable."
  },
  {
    "objectID": "compLatent.html",
    "href": "compLatent.html",
    "title": "Using Rasch and WLS to account for measurement uncertainties in regression models",
    "section": "",
    "text": "There are several reasons for putting this blog post together. The larger issue is to investigate ways to analyze relationships between measures that have varying levels of measurement uncertainty across their respective range or continuum. This is relevant to any kind of latent variable measurement that uses multiple indicators/items/questions as a way to assess a latent variable.\nIn classical test theory (CTT; i.e. factor analysis) the assumption is generally that the measurement error is a single value, constant across the scale and across participants. Modern test theory (Rasch Measurement Theory (RMT) or Item Response Theory (IRT)) has tools to describe the varying uncertainties of the measure itself, and also allows for estimation of measurement uncertainty for each individual, based on the item properties.\nIn the “business-as-usual” approach, no matter which types of measurement uncertainty are ignored. The simple case of have two variables and an ordinary least squares (OLS) linear regression model will take the input from each variable as a perfect measurement. Bayesians may have another take on this, and I will look into that at a later point, so for now this is relevant for frequentist statistics.\n“Business-as-usual” also makes use of ordinal sum scores disguised as interval scores. There is a seemingly wide-spread idea that estimated person interval scores is not significantly different from ordinal sum scores, and we’ll look further into that as well by including ordinal sum scores.\nOne way to take measurement uncertainty into account in a linear regression model is to use Weighted Least Squares (WLS) instead of OLS. However, this approach only allows weights for one variable. The weights will be based on the measurement uncertainty for the interval scores.\nAdditionally, we will use Quantile Regression, with and without weights."
  },
  {
    "objectID": "compLatent.html#data",
    "href": "compLatent.html#data",
    "title": "Using Rasch and WLS to account for measurement uncertainties in regression models",
    "section": "\n2 Data",
    "text": "2 Data\nData were collected for a project evaluating multiple work environment questionnaires. Two of the analyzed scales will be used in our example, as a foundation for simulated data. More information is available at the GitHub repository and website.\nThe two questionnaires cover the domains of “recovery” and “agency”, where the latter refers to workers’ perceived control over their work situation. Our analyses will look at how “agency” affects “recovery”. We will retain an interval scale score for the “recovery” scale throughout, while varying the “agency” between ordinal sum score, interval score, and interval score with weights, and check how these three reproduce the true simulated relationship between the two scales.\nVariables will be named SEM for standard error of measurement, Theta for interval theta/scores/locations, and SumScore for ordinal sum scores.\nWe’ll also add analyses using structural equation modeling and weighted sum scores (based on confirmatory factor analysis factor loadings).\n\nCodelibrary(RISEkbmRasch)\nlibrary(easystats)\nlibrary(quantreg)\nlibrary(faux)\nlibrary(broom.mixed)\nlibrary(lavaan)\nlibrary(DescTools)\nlibrary(lme4)\n\n### some commands exist in multiple packages, here we define preferred ones that are frequently used\nselect <- dplyr::select\ncount <- dplyr::count\nrecode <- car::recode\nrename <- dplyr::rename"
  },
  {
    "objectID": "compLatent.html#simulating-data-n-800",
    "href": "compLatent.html#simulating-data-n-800",
    "title": "Using Rasch and WLS to account for measurement uncertainties in regression models",
    "section": "\n3 Simulating data n = 800",
    "text": "3 Simulating data n = 800\nWe’ll pretend that we have n = 400 that are measured at two time points, with a specified correlation between measurements of 0.5. The difference in group means is set to 1 logit, and the standard deviation will be 1 logit for both points of measurement.\n\nCodeset.seed(1523)\nn <- 100\ndata <- rnorm_multi(n = n, \n                  mu = c(-0.5, +0.5),\n                  sd = c(1, 1),\n                  r = c(0.5), \n                  varnames = c(\"pre\", \"post\"),\n                  empirical = FALSE)\n\ncor_test(\"pre\", \"post\", data = data)\n\nParameter1 | Parameter2 |    r |       95% CI | t(98) |         p\n-----------------------------------------------------------------\npre        |       post | 0.55 | [0.39, 0.67] |  6.49 | < .001***\n\nObservations: 100\n\nCode# light wrangling to get long format and an id variable\ndata_long <- data %>% \n  add_column(id = 1:(nrow(.))) %>% \n  pivot_longer(cols = c(\"pre\", \"post\"), \n               names_to = \"time\", \n               values_to = \"theta\") %>% \n  mutate(theta = round(theta, 4),\n         time = dplyr::recode(time, \"pre\" = 0, \"post\" = 1))\n#write_csv(data, \"data800simPrePost.csv\")\n\n\n\n3.1 Scatter plot\n\nCodedata %>% \n  ggplot(aes(x = pre, y = post)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = TRUE) +\n  theme_bw()\n\n\n\n\n\n3.2 Reference LMM results from the simulated theta values\n\nCodelmm0 <- lmer(scale(theta) ~ time + (1 | id), data = data_long) \n\nlmm0 %>%\n  tidy(conf.int = TRUE) %>% \n  mutate(across(where(is.numeric), round, 4))\n\n# A tibble: 4 × 8\n  effect   group    term         estimate std.error statistic conf.low conf.high\n  <chr>    <chr>    <chr>           <dbl>     <dbl>     <dbl>    <dbl>     <dbl>\n1 fixed    <NA>     (Intercept)    -0.436    0.0902     -4.83   -0.613    -0.259\n2 fixed    <NA>     time            0.872    0.0857     10.2     0.704     1.04 \n3 ran_pars id       sd__(Interc…    0.668   NA          NA      NA        NA    \n4 ran_pars Residual sd__Observa…    0.606   NA          NA      NA        NA    \n\nCodeicc(lmm0)\n\n# Intraclass Correlation Coefficient\n\n    Adjusted ICC: 0.548\n  Unadjusted ICC: 0.444\n\nCodeStdCoef(lmm0)\n\n            Estimate* Std. Error*  df\n(Intercept) 0.0000000  0.00000000 196\ntime        0.4370146  0.04297825 196\nattr(,\"class\")\n[1] \"coefTable\" \"matrix\"   \n\n\nThe standardized coefficient of time is 0.424. This will be our reference value for comparison with the other analyses since ordinal sum scores and estimated theta scores will not be on the same scale.\n\n3.3 Simulating polytomous data\nPrepare the input item parameters.\n\nCode# item parameters\nagencyParams <- read_csv(\"AgencyItemParameters.csv\") %>% \n  as.matrix()\n\n# put item parameters for agency into a list object\ncolnames(agencyParams) <- NULL\n\ntlist <- lapply(1:nrow(agencyParams), function(i) {\n  as.list(agencyParams[i, ])\n})\n\n# simulate polytomous data\nsimData <- SimPartialScore(\n  deltaslist = tlist,\n  thetavec = data_long$theta\n) %>%\n  as.data.frame()\n\n\nA brief look at the simulated data.\n\nCode# distribution\nRIbarstack(simData)\n\n\n\n\n\nCode# reliability/test information\nRItif(simData, cutoff = 2.5, samplePSI = TRUE)\n\n\n\n\nTIF 2.5 = PSI 0.6\n\n3.4 Classical test theory\nConfirmatory factor analysis (CFA) is used to estimate model fit.\n\nCode# specify model\ncfa1 <- 'agencyCFA =~ V1 + V2 + V3 + V4'\n# estimate model\ncfa1.fit <- cfa(cfa1, \n                ordered = TRUE, \n                estimator = \"DWLS\",\n                data = simData)\n# get fit indices etc\nsummary(cfa1.fit, fit.measures = TRUE)\n\nlavaan 0.6.15 ended normally after 13 iterations\n\n  Estimator                                       DWLS\n  Optimization method                           NLMINB\n  Number of model parameters                        20\n\n  Number of observations                           200\n\nModel Test User Model:\n                                                      \n  Test statistic                                 1.426\n  Degrees of freedom                                 2\n  P-value (Chi-square)                           0.490\n\nModel Test Baseline Model:\n\n  Test statistic                               441.330\n  Degrees of freedom                                 6\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    1.000\n  Tucker-Lewis Index (TLI)                       1.004\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.000\n  90 Percent confidence interval - lower         0.000\n  90 Percent confidence interval - upper         0.127\n  P-value H_0: RMSEA <= 0.050                    0.639\n  P-value H_0: RMSEA >= 0.080                    0.209\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.023\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model        Unstructured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(>|z|)\n  agencyCFA =~                                        \n    V1                1.000                           \n    V2                0.918    0.109    8.403    0.000\n    V3                1.179    0.138    8.542    0.000\n    V4                0.942    0.110    8.576    0.000\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(>|z|)\n   .V1                0.000                           \n   .V2                0.000                           \n   .V3                0.000                           \n   .V4                0.000                           \n    agencyCFA         0.000                           \n\nThresholds:\n                   Estimate  Std.Err  z-value  P(>|z|)\n    V1|t1            -0.613    0.095   -6.438    0.000\n    V1|t2             0.088    0.089    0.987    0.323\n    V1|t3             0.842    0.101    8.310    0.000\n    V1|t4             2.326    0.265    8.791    0.000\n    V2|t1            -0.842    0.101   -8.310    0.000\n    V2|t2            -0.088    0.089   -0.987    0.323\n    V2|t3             0.824    0.101    8.180    0.000\n    V2|t4             1.881    0.178   10.583    0.000\n    V3|t1            -0.896    0.103   -8.694    0.000\n    V3|t2            -0.292    0.090   -3.240    0.001\n    V3|t3             0.332    0.091    3.661    0.000\n    V3|t4             1.645    0.150   10.980    0.000\n    V4|t1            -1.282    0.121  -10.576    0.000\n    V4|t2             0.151    0.089    1.692    0.091\n    V4|t3             1.036    0.109    9.547    0.000\n    V4|t4             2.054    0.205   10.020    0.000\n\nVariances:\n                   Estimate  Std.Err  z-value  P(>|z|)\n   .V1                0.507                           \n   .V2                0.585                           \n   .V3                0.315                           \n   .V4                0.563                           \n    agencyCFA         0.493    0.074    6.682    0.000\n\nScales y*:\n                   Estimate  Std.Err  z-value  P(>|z|)\n    V1                1.000                           \n    V2                1.000                           \n    V3                1.000                           \n    V4                1.000                           \n\nCode# get standardized factor loadings\ninspect(cfa1.fit,what=\"std\")$lambda\n\n   agnCFA\nV1  0.702\nV2  0.644\nV3  0.828\nV4  0.661\n\nCode# save factor loadings to a vector\ncfa1_loadings <- inspect(cfa1.fit,what=\"std\")$lambda %>% \n  as.data.frame() %>% \n  pull(agencyCFA)\n\n# CTT reliability\npsych::alpha(simData)\n\n\nReliability analysis   \nCall: psych::alpha(x = simData)\n\n  raw_alpha std.alpha G6(smc) average_r S/N   ase mean   sd median_r\n      0.76      0.76    0.72      0.45 3.2 0.027  1.6 0.83     0.45\n\n    95% confidence boundaries \n         lower alpha upper\nFeldt     0.71  0.76  0.81\nDuhachek  0.71  0.76  0.82\n\n Reliability if an item is dropped:\n   raw_alpha std.alpha G6(smc) average_r S/N alpha se  var.r med.r\nV1      0.70      0.71    0.62      0.44 2.4    0.035 0.0071  0.46\nV2      0.72      0.73    0.64      0.47 2.7    0.033 0.0010  0.46\nV3      0.66      0.67    0.57      0.40 2.0    0.041 0.0024  0.40\nV4      0.73      0.73    0.65      0.48 2.7    0.033 0.0047  0.51\n\n Item statistics \n     n raw.r std.r r.cor r.drop mean   sd\nV1 200  0.77  0.77  0.65   0.57  1.4 1.11\nV2 200  0.75  0.74  0.61   0.53  1.6 1.09\nV3 200  0.83  0.81  0.74   0.64  1.9 1.20\nV4 200  0.71  0.74  0.60   0.52  1.5 0.91\n\nNon missing response frequency for each item\n      0    1    2    3    4 miss\nV1 0.27 0.26 0.26 0.19 0.01    0\nV2 0.20 0.26 0.33 0.17 0.03    0\nV3 0.18 0.20 0.24 0.32 0.05    0\nV4 0.10 0.46 0.29 0.13 0.02    0\n\nCodepsych::omega(simData)\n\n\n\n\nOmega \nCall: omegah(m = m, nfactors = nfactors, fm = fm, key = key, flip = flip, \n    digits = digits, title = title, sl = sl, labels = labels, \n    plot = plot, n.obs = n.obs, rotate = rotate, Phi = Phi, option = option, \n    covar = covar)\nAlpha:                 0.76 \nG.6:                   0.72 \nOmega Hierarchical:    0.7 \nOmega H asymptotic:    0.89 \nOmega Total            0.78 \n\nSchmid Leiman Factor loadings greater than  0.2 \n      g   F1* F2* F3*   h2   u2   p2\nV1 0.73               0.49 0.51 1.08\nV2 0.52  0.39         0.43 0.57 0.62\nV3 0.66  0.45         0.64 0.36 0.69\nV4 0.66               0.41 0.59 1.04\n\nWith Sums of squares  of:\n   g  F1*  F2*  F3* \n1.67 0.36 0.00 0.00 \n\ngeneral/max  4.69   max/min =   Inf\nmean percent general =  0.86    with sd =  0.24 and cv of  0.28 \nExplained Common Variance of the general factor =  0.82 \n\nThe degrees of freedom are -3  and the fit is  0 \nThe number of observations was  200  with Chi Square =  0  with prob <  NA\nThe root mean square of the residuals is  0 \nThe df corrected root mean square of the residuals is  NA\n\nCompare this with the adequacy of just a general factor and no group factors\nThe degrees of freedom for just the general factor are 2  and the fit is  0.06 \nThe number of observations was  200  with Chi Square =  11.68  with prob <  0.0029\nThe root mean square of the residuals is  0.08 \nThe df corrected root mean square of the residuals is  0.13 \n\nRMSEA index =  0.155  and the 10 % confidence intervals are  0.078 0.248\nBIC =  1.08 \n\nMeasures of factor score adequacy             \n                                                 g   F1* F2*   F3*\nCorrelation of scores with factors            0.86  0.60   0  0.08\nMultiple R square of scores with factors      0.73  0.36   0  0.01\nMinimum correlation of factor score estimates 0.46 -0.28  -1 -0.99\n\n Total, General and Subset omega for each subset\n                                                 g  F1* F2*  F3*\nOmega total for total scores and subscales    0.78 0.69  NA 0.66\nOmega general for total scores and subscales  0.70 0.46  NA 0.66\nOmega group for total scores and subscales    0.08 0.23  NA 0.00\n\n\n\n3.5 Estimate person thetas/latent scores\nNext step is to estimate person thetas/scores/locations based on the pre-specified item parameters.\n\nCode# estimate person thetas\npersonThetas <- RIestThetas(simData,itemParams = agencyParams)\n# estimate person theta SEM\npersonSEM <- map_vec(personThetas, ~ catR::semTheta(.x, it = agencyParams, method = \"WL\", model = \"PCM\"))\n# add person thetas to the simulated data\nsimData$estTheta <- personThetas\nsimData$estSEM <- personSEM\n# add ordinal sum scores to the simulated data\nsimData <- simData %>% \n  mutate(SumScore = V1+V2+V3+V4)\n# add weighted ordinal sum scores to the simulated data\nsimData$weightedSumScore <- simData$V1*cfa1_loadings[1] + \n  simData$V2*cfa1_loadings[2] + \n  simData$V3*cfa1_loadings[3] + \n  simData$V4*cfa1_loadings[4]\n\n# add simulated theta values to the simData\nsimData$simTheta <- data_long$theta\n# add id variable\nsimData$id <- data_long$id\n# create grouping variable for quintiles\nsimData$quintile <- as.factor(ntile(simData$simTheta, 5))\n\n# add time variable to data\nsimData$time <- data_long$time\n\nwrite_csv(simData, \"simData.csv\")\n\n\n\nCodehist(simData$simTheta, col = \"lightblue\")\nhist(simData$estTheta, col = \"lightpink\")\nhist(simData$SumScore, col = \"lightgreen\")\nhist(simData$weightedSumScore, col = \"darkgreen\")\n\n\n\n\n\nHistograms of generated and estimated theta values, ordinal sum scores, and weighted ordinal sum scores\n\n\n\n\n\n\nHistograms of generated and estimated theta values, ordinal sum scores, and weighted ordinal sum scores\n\n\n\n\n\n\n\n\nHistograms of generated and estimated theta values, ordinal sum scores, and weighted ordinal sum scores\n\n\n\n\n\n\nHistograms of generated and estimated theta values, ordinal sum scores, and weighted ordinal sum scores\n\n\n\n\n\n\n\nCode# density plot for all four of the above histograms, facet_wrapped by time\nsimData %>% \n  mutate(across(c(\"simTheta\", \"estTheta\", \"SumScore\", \"weightedSumScore\"), ~ scale(.x))) %>%\n  pivot_longer(cols = c(\"simTheta\", \"estTheta\", \"SumScore\", \"weightedSumScore\"), \n               names_to = \"variable\", \n               values_to = \"value\") %>% \n  ggplot(aes(x = value, fill = variable)) +\n  geom_density(alpha = 0.5) +\n  facet_wrap(~time) +\n  theme_bw() +\n  scale_fill_viridis_d() +\n  labs(x = \"Value\", y = \"Density\", fill = \"Variable\",\n       subtitle = \"Split by timepoint 0 and 1\")\n\n\n\nDensity plot of scaled values for all four variables"
  },
  {
    "objectID": "compLatent.html#statistical-analyses",
    "href": "compLatent.html#statistical-analyses",
    "title": "Using Rasch and WLS to account for measurement uncertainties in regression models",
    "section": "\n4 Statistical analyses",
    "text": "4 Statistical analyses\n\n4.1 LMM with estimated theta\n\nCodelmm1 <- lmer(scale(estTheta) ~ time + (1 | id), data = simData) \n\nlmm1 %>%\n  tidy(conf.int = TRUE) %>% \n  mutate(across(where(is.numeric), round, 4))\n\n# A tibble: 4 × 8\n  effect   group    term         estimate std.error statistic conf.low conf.high\n  <chr>    <chr>    <chr>           <dbl>     <dbl>     <dbl>    <dbl>     <dbl>\n1 fixed    <NA>     (Intercept)    -0.347    0.094      -3.70   -0.532    -0.163\n2 fixed    <NA>     time            0.695    0.0998      6.96    0.499     0.890\n3 ran_pars id       sd__(Interc…    0.620   NA          NA      NA        NA    \n4 ran_pars Residual sd__Observa…    0.706   NA          NA      NA        NA    \n\nCodeicc(lmm1)\n\n# Intraclass Correlation Coefficient\n\n    Adjusted ICC: 0.436\n  Unadjusted ICC: 0.383\n\nCodeStdCoef(lmm1)\n\n            Estimate* Std. Error*  df\n(Intercept) 0.0000000   0.0000000 196\ntime        0.3481564   0.0500326 196\nattr(,\"class\")\n[1] \"coefTable\" \"matrix\"   \n\n\n0.37 (SE = 0.026) compared with 0.42.\n\n4.2 Weighted LMM with estimated theta\nWe’ll use the estimated theta SEM as weights.\n\nCode# plot range of estSEM across estTheta\nsimData %>% \n  ggplot(aes(x = estTheta, y = estSEM)) +\n  geom_point() +\n  geom_line(group = 1) +\n  theme_bw() +\n  labs(x = \"Estimated theta\", y = \"Estimated theta SEM\")\n\n\n\nPlot of estimated theta SEM against estimated theta\n\n\n\n\n\nCodelmm2 <- lmer(scale(estTheta) ~ time + (1 | id), data = simData,\n             weights = estSEM) \n\nlmm2 %>%\n  tidy(conf.int = TRUE) %>% \n  mutate(across(where(is.numeric), round, 4))\n\n# A tibble: 4 × 8\n  effect   group    term         estimate std.error statistic conf.low conf.high\n  <chr>    <chr>    <chr>           <dbl>     <dbl>     <dbl>    <dbl>     <dbl>\n1 fixed    <NA>     (Intercept)    -0.429    0.101      -4.27   -0.626    -0.232\n2 fixed    <NA>     time            0.798    0.0992      8.04    0.603     0.992\n3 ran_pars id       sd__(Interc…    0.731   NA          NA      NA        NA    \n4 ran_pars Residual sd__Observa…    0.571   NA          NA      NA        NA    \n\nCodeicc(lmm2)\n\n# Intraclass Correlation Coefficient\n\n    Adjusted ICC: 0.622\n  Unadjusted ICC: 0.524\n\nCodeStdCoef(lmm2)\n\n            Estimate* Std. Error*  df\n(Intercept)  0.000000  0.00000000 196\ntime         0.399825  0.04970946 196\nattr(,\"class\")\n[1] \"coefTable\" \"matrix\"   \n\n\n0.414 (SE = 0.026) compared with 0.42.\n\n4.3 LMM with ordinal sum score\n\nCodelmm3 <- lmer(scale(SumScore) ~ time + (1 | id), data = simData)\n\nlmm3 %>%\n  tidy(conf.int = TRUE) %>% \n  mutate(across(where(is.numeric), round, 4))\n\n# A tibble: 4 × 8\n  effect   group    term         estimate std.error statistic conf.low conf.high\n  <chr>    <chr>    <chr>           <dbl>     <dbl>     <dbl>    <dbl>     <dbl>\n1 fixed    <NA>     (Intercept)    -0.336    0.0944     -3.57   -0.522    -0.152\n2 fixed    <NA>     time            0.673    0.101       6.64    0.474     0.872\n3 ran_pars id       sd__(Interc…    0.614   NA          NA      NA        NA    \n4 ran_pars Residual sd__Observa…    0.717   NA          NA      NA        NA    \n\nCodeicc(lmm3)\n\n# Intraclass Correlation Coefficient\n\n    Adjusted ICC: 0.423\n  Unadjusted ICC: 0.375\n\nCodeStdCoef(lmm3)\n\n            Estimate* Std. Error*  df\n(Intercept) 0.0000000  0.00000000 196\ntime        0.3373715  0.05083355 196\nattr(,\"class\")\n[1] \"coefTable\" \"matrix\"   \n\n\n0.373 (SE = 0.026) compared with 0.42.\n\n4.4 LMM with weighted ordinal sum score\n\nCodelmm4 <- lmer(scale(weightedSumScore) ~ time + (1 | id), data = simData)\n\nlmm4 %>%\n  tidy(conf.int = TRUE) %>% \n  mutate(across(where(is.numeric), round, 4))\n\n# A tibble: 4 × 8\n  effect   group    term         estimate std.error statistic conf.low conf.high\n  <chr>    <chr>    <chr>           <dbl>     <dbl>     <dbl>    <dbl>     <dbl>\n1 fixed    <NA>     (Intercept)    -0.333    0.0945     -3.53   -0.518    -0.148\n2 fixed    <NA>     time            0.666    0.102       6.54    0.467     0.866\n3 ran_pars id       sd__(Interc…    0.611   NA          NA      NA        NA    \n4 ran_pars Residual sd__Observa…    0.720   NA          NA      NA        NA    \n\nCodeicc(lmm4)\n\n# Intraclass Correlation Coefficient\n\n    Adjusted ICC: 0.419\n  Unadjusted ICC: 0.372\n\nCodeStdCoef(lmm4)\n\n            Estimate* Std. Error*  df\n(Intercept) 0.0000000  0.00000000 196\ntime        0.3339848  0.05107553 196\nattr(,\"class\")\n[1] \"coefTable\" \"matrix\"   \n\n\n0.373 (SE = 0.026) compared with 0.424."
  },
  {
    "objectID": "compLatent.html#summary",
    "href": "compLatent.html#summary",
    "title": "Using Rasch and WLS to account for measurement uncertainties in regression models",
    "section": "\n5 Summary",
    "text": "5 Summary\n\nCode# rbind together lmm0 to lmm4 in one dataframe\nlmm_summary <- rbind(tidy(lmm0), tidy(lmm1), tidy(lmm2), tidy(lmm3), tidy(lmm4)) %>% \n  add_column(model = rep(c(\"lmm0\", \"lmm1estTheta\", \"lmm2weightedTheta\", \"lmm3sumscore\", \"lmm4weighted Sumscore\"),each = 4))\nlmm_summary %>% \n  filter(term == \"time\") %>% \n  write_csv(paste0(n,\"_lmm_summary.csv\"))\n\n\n\nCode# check ICC vs correlation pre/post for estTheta\nsimData %>% \n  mutate(time = factor(time, labels = c(\"pre\",\"post\"))) %>% \n  pivot_wider(names_from = time, values_from = estTheta, id_cols = \"id\") %>% \n  cor_test(\"pre\",\"post\")\n\nParameter1 | Parameter2 |    r |       95% CI | t(98) |         p\n-----------------------------------------------------------------\npre        |       post | 0.44 | [0.26, 0.58] |  4.80 | < .001***\n\nObservations: 100"
  },
  {
    "objectID": "raschrvignette/RaschRvign.html#sec-grateful",
    "href": "raschrvignette/RaschRvign.html#sec-grateful",
    "title": "easyRasch vignette",
    "section": "\n12 Software used",
    "text": "12 Software used\nThe grateful package is a nice way to give credit to the packages used in making the analysis. The package can create both a bibliography file and a table object, which is handy for automatically creating a reference list based on the packages used (or at least explicitly loaded).\n\nCodelibrary(grateful)\npkgs &lt;- cite_packages(cite.tidyverse = TRUE, \n                      output = \"table\",\n                      bib.file = \"grateful-refs.bib\",\n                      include.RStudio = TRUE,\n                      out.dir = getwd())\n# If kbl() is used to generate this table, the references will not be added to the Reference list.\nformattable(pkgs, \n            table.attr = 'class=\\\"table table-striped\\\" style=\"font-size: 13px; font-family: Lato; width: 80%\"')\n\n\n\n\nPackage\n\n\nVersion\n\n\nCitation\n\n\n\n\n\nbase\n\n\n4.4.3\n\n\nR Core Team (2025)\n\n\n\n\ncar\n\n\n3.1.2\n\n\nFox & Weisberg (2019)\n\n\n\n\neasyRasch\n\n\n0.3.9\n\n\nJohansson (2025b)\n\n\n\n\neRm\n\n\n1.0.10\n\n\nMair & Hatzinger (2007b); Mair & Hatzinger (2007a); Hatzinger & Rusch (2009); Rusch et al. (2013); Koller et al. (2015); Debelak & Koller (2019)\n\n\n\n\nforeach\n\n\n1.5.2\n\n\nMicrosoft & Weston (2022)\n\n\n\n\nformattable\n\n\n0.2.1\n\n\nRen & Russell (2021)\n\n\n\n\nggrepel\n\n\n0.9.6\n\n\nSlowikowski (2024)\n\n\n\n\nglue\n\n\n1.8.0\n\n\nHester & Bryan (2024)\n\n\n\n\ngtsummary\n\n\n2.4.0\n\n\nSjoberg et al. (2021)\n\n\n\n\niarm\n\n\n0.4.3\n\n\nMueller (2022)\n\n\n\n\nkableExtra\n\n\n1.4.0\n\n\nZhu (2024)\n\n\n\n\nknitr\n\n\n1.48\n\n\nXie (2014); Xie (2015); Xie (2024)\n\n\n\n\nlordif\n\n\n0.3.3\n\n\nChoi et al. (2016)\n\n\n\n\nmatrixStats\n\n\n1.4.1\n\n\nBengtsson (2024)\n\n\n\n\nmice\n\n\n3.16.0\n\n\nvan Buuren & Groothuis-Oudshoorn (2011)\n\n\n\n\nmirt\n\n\n1.45.1\n\n\nChalmers (2012)\n\n\n\n\npatchwork\n\n\n1.3.2\n\n\nPedersen (2025)\n\n\n\n\nPerFit\n\n\n1.4.6\n\n\nTendeiro et al. (2016)\n\n\n\n\npsych\n\n\n2.5.6\n\n\nWilliam Revelle (2025)\n\n\n\n\npsychotree\n\n\n0.16.2\n\n\nTrepte & Verbeet (2010); Strobl et al. (2011); Strobl et al. (2015a); Komboz et al. (2018); Wickelmaier & Zeileis (2018)\n\n\n\n\nRASCHplot\n\n\n0.1.0\n\n\nBuchardt et al. (2022)\n\n\n\n\nreshape\n\n\n0.8.10\n\n\nWickham (2007)\n\n\n\n\nrmarkdown\n\n\n2.29\n\n\nXie et al. (2018); Xie et al. (2020); Allaire et al. (2024)\n\n\n\n\ntidyverse\n\n\n2.0.0\n\n\nWickham et al. (2019)"
  },
  {
    "objectID": "datawrangling.html#removing-respondents-with-missing-data",
    "href": "datawrangling.html#removing-respondents-with-missing-data",
    "title": "Data wrangling for psychometrics in R",
    "section": "1 Removing respondents with missing data",
    "text": "1 Removing respondents with missing data\nThere may be situations where you want to specify a minimum number of items that a respondent must have answered in order to be included in the analysis. This is done by using the filter() function from library(dplyr). The syntax is as follows:\nmin.responses &lt;- 5 # set the minimum number of responses\n\ndf2 &lt;- df %&gt;% \n  filter(length(itemlabels$itemnr) - rowSums(is.na(.[itemlabels$itemnr])) &gt;=     min.responses)\nThe object itemlabels$itemnr is a vector (column in a dataframe) with the short item labels as they are used in the dataframe df which contain the data, with items as columns using the corresponding labels.\nThe length() function is used to count the number of items in the vector, and rowSums(is.na()) is used to count the number of missing values in each row. Then a simple subtraction is done, total number of items minus number of missing responses. The &gt;= operator is used to compare the number of responses to the minimum number of responses specified in the object min.responses. The filter() function removes all rows that do not meet the criteria. And the new dataset is saved to df2."
  },
  {
    "objectID": "parameterizedQ.html",
    "href": "parameterizedQ.html",
    "title": "Automating reports with Quarto",
    "section": "",
    "text": "Let’s say you have survey data from a group of 28 municipalities. You have worked out a Quarto file to generate nice tables and figures and want to produce 28 different reports that all use the same Quarto file for each of the municipalities. Additionally, you want one collective report using the complete dataset.\nInstead of creating 28+1 different .qmd files and running them manually, this can be easily automated with parameterization. This will save huge amounts of time, for instance when you find that typo in one figure and need to re-render all 29 reports. Or, even better, when you get next years survey results and can re-use the whole setup and instantly generate your new reports!"
  },
  {
    "objectID": "parameterizedQ.html#background",
    "href": "parameterizedQ.html#background",
    "title": "Automating reports with Quarto",
    "section": "",
    "text": "Let’s say you have survey data from a group of 28 municipalities. You have worked out a Quarto file to generate nice tables and figures and want to produce 28 different reports that all use the same Quarto file for each of the municipalities. Additionally, you want one collective report using the complete dataset.\nInstead of creating 28+1 different .qmd files and running them manually, this can be easily automated with parameterization. This will save huge amounts of time, for instance when you find that typo in one figure and need to re-render all 29 reports. Or, even better, when you get next years survey results and can re-use the whole setup and instantly generate your new reports!"
  },
  {
    "objectID": "parameterizedQ.html#setting-up",
    "href": "parameterizedQ.html#setting-up",
    "title": "Automating reports with Quarto",
    "section": "\n2 Setting up",
    "text": "2 Setting up\nIn our simple case, we will only use one parameter, the name of the municipality. Of course there could be any number of parameters that you may want to customize, which you are likely to easily be able to do based on this example.\nWe need to add two rows to the qmd-file YAML:\n---\nparams:\n  municipality: \"All municipalities\"\n---\nThis creates the object params$municipality and gives it a default value.\nNext, we create our little script, in a file called render.R, starting with a vector of municipalities. This could of course be read from a file, and our example will only contain four.\nmunicipalities &lt;- c(\"All municipalities\",\"Vallentuna\",\"Vaxholm\",\"Södertälje\",\"Botkyrka\")\nlibrary(glue)\nlibrary(quarto)\nlibrary(purrr)\n\nwalk(1:length(municipalities), function(i) {\n  muni &lt;- municipalities[i]\n  \n  outfile &lt;- glue(\"{Sys.Date()}_{muni}.html\") # gives the filename a date and the municipality name\n  \n  quarto_render(input = \"yourQuartoTemplate.qmd\", \n                execute_params = list(\"grupp\" = muni), \n                output_file = outfile,\n                output_format = \"html\")\n})\n\nAs you can see, we make use of purrr::walk, which means you may be able to use furrr::future_walk to enable parallel processing?! I have only tried this briefly, but it didn’t work. Could be worth looking into if you need to generate a lot of reports often.\n\n\n2.1 Final settings\nNow, we just need to make sure that our Quarto file uses the params. For simplicity, I expect that you read your data from a file into a dataframe. We will make use of a simple if and else combined with dplyr::filter() from the tidyverse.\n# read data\ndf &lt;- read_csv(\"yourDataFile.csv\")\n\nif (params$municipality == \"All municipalities\") {\n  df &lt;- df\n} else {\n  df &lt;- df %&gt;% \n  filter(municipality == params$municipality)\n}\nSave your files and run the render.R script! Files will be output to the same directory as your .qmd file.\nIf you’d like to use the municipality in the first headline of the report, you can add a line like this to the Quarto file:\n\nCode## `r params$municipality` {.unnumbered}\n\n\n\n\n\n\n\n\nNote\n\n\n\nA far more complex example can be found here (and some of my code was adapted from there): https://github.com/Pecners/sra_pullout/blob/main/render.R"
  },
  {
    "objectID": "parameterizedQ.html#a-slightly-more-complex-example",
    "href": "parameterizedQ.html#a-slightly-more-complex-example",
    "title": "Automating reports with Quarto",
    "section": "\n3 A slightly more complex example",
    "text": "3 A slightly more complex example\nThe earlier example only used a single vector of municipalities as a parameter. In a current project I’m working on the municipalities also want to specify who they want to compare themselves with, and the timespan in years that they want displayed in the report. We keep this information in a MS Excel file with three variables: focusMuni, compMuni, yearsMuni, which is read into a dataframe called DIDparams in the script below. To avoid issues with blank spaces in the cells, we’ll remove them when importing. This is done with the gsub() function.\n# we need two additional packages for this\nlibrary(readxl)\nlibrary(dplyr)\n\nDIDparams &lt;- read_excel(\"DIDreportParameters.xls\") %&gt;%\n  mutate(across(everything(), ~ gsub(\" \",\"\",.x)))\nThen we loop through each row in the DIDparams dataframe. Each cell in the variables compMuni and YearsMuni will contain multiple values. We use strsplit() and unlist() in a pipe to convert these into vectors. Finally, we want to make sure that the variable containing years is coded as numeric.\nwalk(1:nrow(DIDparams), function(i) {\n  this &lt;- DIDparams[i,]\n\n  outfile &lt;- glue(\"{Sys.Date()}_DIDrapport_{this$fokusKommun}.html\")\n\n  quarto_render(input = \"DIDreport.qmd\",\n                execute_params = list(\"focusMuni\" = this$focusMuni, # only contains one value\n                                      \"compMuni\" = this$compMuni %&gt;% strsplit(\",\") %&gt;% unlist(),\n                                      \"yearsMuni\" = this$yearsMuni %&gt;% strsplit(\",\") %&gt;% unlist() %&gt;% as.numeric()\n                                      ),\n                output_file = outfile,\n                output_format = \"html\")\n})"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Magnus Johansson is a licensed psychologist with a PhD in behavior analysis. I work as a scientist at RISE Research Institutes of Sweden, primarily with psychometrics, measurement, prevention and public health. I am also an affiliated researcher at Karolinska Institutet & Stockholm Health Care Services, Centre for Psychiatry Research, Department of Clinical Neuroscience.\nYou can also find me on:\n\nBluesky: @pgmj.bsky.social\nORCID: 0000-0003-1669-592X\nMastodon: @pgmj\nOSF: Open Science Framework\n\nand of course on GitHub\n\n\n\n\n\nReuseCC BY 4.0"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Home",
    "section": "",
    "text": "When you work with open source tools such as R it comes with a community of users asking and answering questions, as well as many making helpful tutorials or blog posts freely available.\nI constantly learn from others and like to give something back from my own journey of learning, exploring and hopefully innovating and contributing back to the community. I have the opportunity to work with a lot of different data scenarios. Most often psychometrics is involved, as well as data analysis and visualization.\nMy ambition is to get better at documenting my own learning process and sharing it through this website. Content will be added intermittently under the Blog headline.\n\n\n\n\n\nReuseCC BY 4.0"
  },
  {
    "objectID": "raschrvignette/RaschRvign.html#session-info",
    "href": "raschrvignette/RaschRvign.html#session-info",
    "title": "easyRasch vignette",
    "section": "\n14 Session info",
    "text": "14 Session info\n\nCodesessionInfo()\n\nR version 4.4.3 (2025-02-28)\nPlatform: aarch64-apple-darwin20\nRunning under: macOS Sequoia 15.6.1\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\ntime zone: Europe/Stockholm\ntzcode source: internal\n\nattached base packages:\n [1] parallel  grid      stats4    stats     graphics  grDevices utils    \n [8] datasets  methods   base     \n\nother attached packages:\n [1] PerFit_1.4.6      ltm_1.2-0         polycor_0.8-1     msm_1.7.1        \n [5] MASS_7.3-64       lordif_0.3-3      rms_6.8-1         Hmisc_5.2-3      \n [9] RASCHplot_0.1.0   knitr_1.48        readxl_1.4.5      car_3.1-2        \n[13] carData_3.0-5     grateful_0.2.4    easyRasch_0.3.9   doParallel_1.0.17\n[17] iterators_1.0.14  furrr_0.3.1       future_1.67.0     foreach_1.5.2    \n[21] ggdist_3.3.3      janitor_2.2.1     iarm_0.4.3        hexbin_1.28.4    \n[25] catR_3.17         glue_1.8.0        ggrepel_0.9.6     patchwork_1.3.2  \n[29] reshape_0.8.10    matrixStats_1.4.1 psychotree_0.16-2 psychotools_0.7-4\n[33] partykit_1.2-24   mvtnorm_1.3-3     libcoin_1.0-10    psych_2.5.6      \n[37] mirt_1.45.1       lattice_0.22-6    eRm_1.0-10        lubridate_1.9.4  \n[41] forcats_1.0.0     stringr_1.5.2     dplyr_1.1.4       purrr_1.1.0      \n[45] readr_2.1.5       tidyr_1.3.1       tibble_3.3.0      ggplot2_3.5.2    \n[49] tidyverse_2.0.0   kableExtra_1.4.0  formattable_0.2.1\n\nloaded via a namespace (and not attached):\n  [1] bitops_1.0-8         RColorBrewer_1.1-3   tools_4.4.3         \n  [4] backports_1.5.0      utf8_1.2.4           R6_2.5.1            \n  [7] DT_0.33              vegan_2.6-8          sm_2.2-6.0          \n [10] mgcv_1.9-1           jomo_2.7-6           permute_0.9-7       \n [13] withr_3.0.2          gridExtra_2.3        progressr_0.14.0    \n [16] archive_1.1.8        quantreg_5.98        cli_3.6.3           \n [19] gt_1.0.0             sandwich_3.1-1       labeling_0.4.3      \n [22] sass_0.4.9           polspline_1.1.25     pbapply_1.7-2       \n [25] systemfonts_1.2.3    commonmark_1.9.2     foreign_0.8-88      \n [28] relimp_1.0-5         svglite_2.1.3        R.utils_2.12.3      \n [31] parallelly_1.45.1    sessioninfo_1.2.3    rstudioapi_0.17.1   \n [34] generics_0.1.3       shape_1.4.6.1        vroom_1.6.5         \n [37] distributional_0.4.0 qvcalc_1.0.3         Matrix_1.7-2        \n [40] abind_1.4-5          R.methodsS3_1.8.2    lifecycle_1.0.4     \n [43] multcomp_1.4-26      yaml_2.3.10          snakecase_0.11.1    \n [46] inum_1.0-5           gtsummary_2.4.0      ggstance_0.3.7      \n [49] irtoys_0.2.2         promises_1.3.2       crayon_1.5.3        \n [52] mitml_0.4-5          cowplot_1.1.3        pillar_1.10.1       \n [55] boot_1.3-31          fda_6.1.8            vcdExtra_0.8-5      \n [58] admisc_0.35          future.apply_1.20.0  codetools_0.2-20    \n [61] pan_1.9              beepr_2.0            data.table_1.17.8   \n [64] vcd_1.4-12           vctrs_0.6.5          Rdpack_2.6.1        \n [67] testthat_3.2.1.1     cellranger_1.1.0     gtable_0.3.6        \n [70] ks_1.14.2            cachem_1.1.0         xfun_0.46           \n [73] rbibutils_2.2.16     mime_0.12            pracma_2.4.4        \n [76] fds_1.8              reformulas_0.4.1     pcaPP_2.0-4         \n [79] survival_3.8-3       audio_0.1-11         TH.data_1.1-2       \n [82] nlme_3.1-167         bit64_4.0.5          rprojroot_2.1.1     \n [85] Deriv_4.1.3          KernSmooth_2.23-26   rpart_4.1.24        \n [88] colorspace_2.1-1     gnm_1.1-5            nnet_7.3-20         \n [91] mnormt_2.1.1         tidyselect_1.2.1     bit_4.0.5           \n [94] compiler_4.4.3       curl_7.0.0           glmnet_4.1-8        \n [97] htmlTable_2.4.3      mice_3.16.0          SparseM_1.84-2      \n[100] expm_0.999-9         xml2_1.3.6           checkmate_2.3.2     \n[103] scales_1.4.0         lmtest_0.9-40        digest_0.6.37       \n[106] rainbow_3.8          minqa_1.2.7          rmarkdown_2.29      \n[109] ca_0.71.1            htmltools_0.5.8.1    pkgconfig_2.0.3     \n[112] base64enc_0.1-3      SimDesign_2.20.0     lme4_1.1-37         \n[115] fastmap_1.2.0        rlang_1.1.6          htmlwidgets_1.6.4   \n[118] shiny_1.10.0         farver_2.1.2         zoo_1.8-12          \n[121] jsonlite_1.8.9       mclust_6.1.1         dcurver_0.9.2       \n[124] R.oo_1.26.0          RCurl_1.98-1.16      magrittr_2.0.3      \n[127] Formula_1.2-5        Rcpp_1.0.14          stringi_1.8.7       \n[130] brio_1.1.5           plyr_1.8.9           listenv_0.9.1       \n[133] splines_4.4.3        hms_1.1.3            ggpubr_0.6.0        \n[136] ggsignif_0.6.4       markdown_1.13        reshape2_1.4.4      \n[139] GPArotation_2024.3-1 evaluate_1.0.1       renv_1.0.7          \n[142] deSolve_1.40         nloptr_2.1.1         tzdb_0.4.0          \n[145] httpuv_1.6.15        MatrixModels_0.5-3   cards_0.7.0         \n[148] broom_1.0.10         xtable_1.8-4         rstatix_0.7.2       \n[151] later_1.4.1          viridisLite_0.4.2    memoise_2.0.1       \n[154] cluster_2.1.8        corrplot_0.92        timechange_0.3.0    \n[157] globals_0.18.0       hdrcde_3.4           here_1.0.2"
  },
  {
    "objectID": "ttestChange.html",
    "href": "ttestChange.html",
    "title": "Analyzing individual change with t-tests",
    "section": "",
    "text": "When one uses Rasch or Item Response Theory to estimate measurement values on the latent scale it is also easy to estimate an “individualized” measurement error for each value estimated. It has been suggested that a useful way to use this output when analyzing data with two repeated measurement points is to conduct one paired t-test for each individual, comparing the pre/post measurements and using their respective measurement error as the “population standard error” (Hobart et al., 2010).\nThis strategy results in measurably detectable change for individuals across two time points. Whether this change is meaningful requires external validation to determine. Since measurement error is not equal across the continuum, how large the measurably detactable change is will also vary depending on the pre-measurement value.\nAll the usual limitations and caveats with t-tests apply. This post is not an endorsement of using multiple t-tests as a primary strategy of analysis, but it may be an interesting perspective that takes measurement error into account and focuses on individual outcomes.\n\n\n\n\n\n\nNote\n\n\n\nThis is work in progress and will be updated and expanded on. The first aim is to share code to do individual t-tests of theta+SEM and summarise the results. Later on, we’ll look at other methods to analyze longitudinal data, both looking at group level and individual level change.\n\n\nThe reliability of the scale/test will of course heavily influence the possibility of detecting change over time. In the Rasch Measurement Theory and Item Response Theory paradigm, reliability is not a single point value that is constant across the latent continuum (REF to separate post on reliability). Depending on the item threshold locations, the test has varying reliability.\nLater in this post, I will present a figure describing the Test Information Function, and then a figure with the SEM values across the latent continuum.\nAs shown in the separate post on reliability (LINK), SEM * 1.96 will contain the true value in 95% of cases.\n\nCodelibrary(tidyverse)\nlibrary(eRm)\nlibrary(janitor)\nlibrary(RISEkbmRasch)\nlibrary(lme4)\nlibrary(modelsummary)\nlibrary(broom.mixed)\nlibrary(marginaleffects)\nlibrary(faux)\nlibrary(ggdist)\n\n### some commands exist in multiple packages, here we define preferred ones that are frequently used\nselect &lt;- dplyr::select\ncount &lt;- dplyr::count\nrecode &lt;- car::recode\nrename &lt;- dplyr::rename\n\ntheme_rise &lt;- function(fontfamily = \"Lato\", axissize = 13, titlesize = 15,\n                       margins = 12, axisface = \"plain\", panelDist = 0.6, ...) {\n  theme_minimal() +\n  theme(\n    text = element_text(family = fontfamily),\n    axis.title.x = element_text(\n      margin = margin(t = margins),\n      size = axissize\n    ),\n    axis.title.y = element_text(\n      margin = margin(r = margins),\n      size = axissize\n    ),\n    plot.title = element_text(\n      face = \"bold\",\n      size = titlesize\n    ),\n    axis.title = element_text(\n      face = axisface\n    ),\n    plot.caption = element_text(\n      face = \"italic\"\n    ),\n    legend.text = element_text(family = fontfamily),\n    legend.background = element_rect(color = \"lightgrey\"),\n    strip.background = element_rect(color = \"lightgrey\"),\n    panel.spacing = unit(panelDist, \"cm\", data = NULL),\n    panel.border = element_rect(color = \"grey\", fill = NA),\n    ...\n  )\n}\n\ntheme_set(theme_rise())"
  },
  {
    "objectID": "ttestChange.html#introduction",
    "href": "ttestChange.html#introduction",
    "title": "Analyzing individual change with t-tests",
    "section": "",
    "text": "When one uses Rasch or Item Response Theory to estimate measurement values on the latent scale it is also easy to estimate an “individualized” measurement error for each value estimated. It has been suggested that a useful way to use this output when analyzing data with two repeated measurement points is to conduct one paired t-test for each individual, comparing the pre/post measurements and using their respective measurement error as the “population standard error” (Hobart et al., 2010).\nThis strategy results in measurably detectable change for individuals across two time points. Whether this change is meaningful requires external validation to determine. Since measurement error is not equal across the continuum, how large the measurably detactable change is will also vary depending on the pre-measurement value.\nAll the usual limitations and caveats with t-tests apply. This post is not an endorsement of using multiple t-tests as a primary strategy of analysis, but it may be an interesting perspective that takes measurement error into account and focuses on individual outcomes.\n\n\n\n\n\n\nNote\n\n\n\nThis is work in progress and will be updated and expanded on. The first aim is to share code to do individual t-tests of theta+SEM and summarise the results. Later on, we’ll look at other methods to analyze longitudinal data, both looking at group level and individual level change.\n\n\nThe reliability of the scale/test will of course heavily influence the possibility of detecting change over time. In the Rasch Measurement Theory and Item Response Theory paradigm, reliability is not a single point value that is constant across the latent continuum (REF to separate post on reliability). Depending on the item threshold locations, the test has varying reliability.\nLater in this post, I will present a figure describing the Test Information Function, and then a figure with the SEM values across the latent continuum.\nAs shown in the separate post on reliability (LINK), SEM * 1.96 will contain the true value in 95% of cases.\n\nCodelibrary(tidyverse)\nlibrary(eRm)\nlibrary(janitor)\nlibrary(RISEkbmRasch)\nlibrary(lme4)\nlibrary(modelsummary)\nlibrary(broom.mixed)\nlibrary(marginaleffects)\nlibrary(faux)\nlibrary(ggdist)\n\n### some commands exist in multiple packages, here we define preferred ones that are frequently used\nselect &lt;- dplyr::select\ncount &lt;- dplyr::count\nrecode &lt;- car::recode\nrename &lt;- dplyr::rename\n\ntheme_rise &lt;- function(fontfamily = \"Lato\", axissize = 13, titlesize = 15,\n                       margins = 12, axisface = \"plain\", panelDist = 0.6, ...) {\n  theme_minimal() +\n  theme(\n    text = element_text(family = fontfamily),\n    axis.title.x = element_text(\n      margin = margin(t = margins),\n      size = axissize\n    ),\n    axis.title.y = element_text(\n      margin = margin(r = margins),\n      size = axissize\n    ),\n    plot.title = element_text(\n      face = \"bold\",\n      size = titlesize\n    ),\n    axis.title = element_text(\n      face = axisface\n    ),\n    plot.caption = element_text(\n      face = \"italic\"\n    ),\n    legend.text = element_text(family = fontfamily),\n    legend.background = element_rect(color = \"lightgrey\"),\n    strip.background = element_rect(color = \"lightgrey\"),\n    panel.spacing = unit(panelDist, \"cm\", data = NULL),\n    panel.border = element_rect(color = \"grey\", fill = NA),\n    ...\n  )\n}\n\ntheme_set(theme_rise())"
  },
  {
    "objectID": "ttestChange.html#simulating-longitudinal-data",
    "href": "ttestChange.html#simulating-longitudinal-data",
    "title": "Analyzing individual change with t-tests",
    "section": "\n2 Simulating longitudinal data",
    "text": "2 Simulating longitudinal data\n3 time points for later use. First, we’ll focus on t1 and t3 and run the t-tests focusing on individual change.\n\nCodeset.seed(1523)\n\ndata_g1 &lt;- rnorm_multi(n = 250, \n                  mu = c(-0.25, 0, 0.25),\n                  sd = c(1, 1, 1),\n                  r = c(0.5), \n                  varnames = c(\"t_1\", \"t_2\", \"t_3\"),\n                  empirical = FALSE) %&gt;% \n  add_column(group = \"treatment\") %&gt;% \n  rownames_to_column(\"id\")\n\ndata_g2 &lt;- rnorm_multi(n = 250, \n                  mu = c(-0.25, -0.15, -0.05),\n                  sd = c(1, 1, 1),\n                  r = c(0.5), \n                  varnames = c(\"t_1\", \"t_2\", \"t_3\"),\n                  empirical = FALSE) %&gt;% \n  add_column(group = \"control\") %&gt;% \n  mutate(id = c(251:500))\n\nd &lt;- rbind(data_g1,data_g2)\n\nd_long &lt;- d %&gt;% \n  pivot_longer(starts_with(\"t\")) %&gt;% \n  mutate(time = as.numeric(gsub(\"t_\",\"\",name))) %&gt;% \n  select(!name)\n\n\n\n2.1 Simulate response data\n\nCode# item list for simulation\ntlist &lt;- list(\n  t1 = list(1.2, 1.8, 2.4),\n  t2 = list(-1.3, -0.5, 0.5),\n  t3 = list(-0.3, 0.3, 1.2),\n  t4 = list(0.1, 0.6, 1.6),\n  t5 = list(-0.3, 0.7, 1.5),\n  t6 = list(-1.6, -1, -0.3),\n  t7 = list(1, 1.8, 2.5),\n  t8 = list(-1.3, -0.7, 0.4),\n  t9 = list(-0.8, 1.4, 1.9),\n  t10 = list(0.25, 1.25, 2.15)\n)\n\ninputDeltas &lt;- tibble(\n  q1 = c(1.2, 1.8, 2.4),\n  q2 = c(-1.3, -0.5, 0.5),\n  q3 = c(-0.3, 0.3, 1.2),\n  q4 = c(0.1, 0.6, 1.6),\n  q5 = c(-0.3, 0.7, 1.5),\n  q6 = c(-1.6, -1, -0.3),\n  q7 = c(1, 1.8, 2.5),\n  q8 = c(-1.3, -0.7, 0.4),\n  q9 = c(-0.8, 1.4, 1.9),\n  q10 = c(0.25, 1.25, 2.15)\n) %&gt;%\n  t() %&gt;%\n  as.data.frame() %&gt;%\n  mutate(Average = rowMeans(., na.rm = T)) %&gt;% \n  mutate_if(is.double, round, digits = 2) %&gt;% \n  rownames_to_column(var = \"Item\") %&gt;% \n  dplyr::rename(T1 = V1,\n         T2 = V2,\n         T3 = V3)\n\n# simulate response data based on the above defined item thresholds\ntd &lt;- SimPartialScore(\n  deltaslist = tlist,\n  thetavec = d_long$value\n) %&gt;%\n  as.data.frame()\n\n\n\n2.2 Estimating item threshold locations\nWe could use the parameters from the input, but to add some real world usage to the setup, we’ll stack the response data and estimate the item threshold locations using the eRm package with the Partial Credig Model and Conditional Maximum Likelihood.\n\nCodeerm_item_parameters &lt;- RIitemparams(td, output = \"dataframe\") %&gt;% \n  select(!Location) %&gt;% \n  rename(T1 = `Threshold 1`,\n         T2 = `Threshold 2`,\n         T3 = `Threshold 3`) %&gt;% \n  as.matrix()\n\n# center item parameters to sum = 0\nerm_item_parameters &lt;- erm_item_parameters - mean(erm_item_parameters)\n\n\n\n2.3 Estimating person locations and SEM\nUsing Weighted Likelihood (Warm, 1989) to minimize bias.\n\nCode# estimate thetas/person locations, based on eRm estimated item thresholds\nerm_thetas &lt;- RIestThetas(td, itemParams = erm_item_parameters)\n\n# estimate measurement error (SEM)\nerm_sem &lt;- map_vec(erm_thetas, ~ catR::semTheta(.x, it = erm_item_parameters, method = \"WL\", model = \"PCM\"))\n\nd_long$erm_thetas &lt;- erm_thetas\nd_long$erm_sem &lt;- erm_sem"
  },
  {
    "objectID": "ttestChange.html#change",
    "href": "ttestChange.html#change",
    "title": "Analyzing individual change with t-tests",
    "section": "\n3 Change",
    "text": "3 Change\nHow many have changed 0.5+ logits (the mean change simulated, also corresponding to 0.5 SD) in the generated data? This could serve as a reference.\n\nCoded &lt;- d %&gt;% \n  mutate(change = t_3 - t_1)\n\nsummary(d$change)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n-2.4126 -0.4427  0.2565  0.2682  0.9909  3.0004 \n\nCodesd(d$change)\n\n[1] 1.00556\n\nCodesd(d$t_3)\n\n[1] 1.02455\n\nCodeggplot(d, aes(x = change, fill = group, color = group)) +\n  #geom_histogram() +\n  stat_dotsinterval(color = \"black\", slab_color = \"white\", slab_linewidth = 0.5) +\n  facet_wrap(~ group) +\n  guides(fill = \"none\", color = \"none\")\n\n\n\n\n\n\nCoded %&gt;% \n  mutate(change_grp = case_when(change &gt; 0.5 ~ \"Improved &gt; 0.5\",\n                                change &lt; -0.5 ~ \"Worsened &gt; 0.5\",\n                                TRUE ~ \"In between\")) %&gt;% \n  group_by(group) %&gt;% \n  count(change_grp) %&gt;% \n  kbl_rise(tbl_width = 40) %&gt;% \n  row_spec(c(1,4),bold=TRUE)\n\n\n\n group \n    change_grp \n    n \n  \n\n\n control \n    Improved &gt; 0.5 \n    82 \n  \n\n control \n    In between \n    95 \n  \n\n control \n    Worsened &gt; 0.5 \n    73 \n  \n\n treatment \n    Improved &gt; 0.5 \n    124 \n  \n\n treatment \n    In between \n    85 \n  \n\n treatment \n    Worsened &gt; 0.5 \n    41 \n  \n\n\n\n\n\n3.1 Test information function\n\nCodeRItif(td, samplePSI = TRUE)"
  },
  {
    "objectID": "ttestChange.html#t-tests-of-individual-change",
    "href": "ttestChange.html#t-tests-of-individual-change",
    "title": "Analyzing individual change with t-tests",
    "section": "\n4 t-tests of individual change",
    "text": "4 t-tests of individual change\nOnly using t1 and t3 estimated person locations/thetas and SEM values.\n\nCoded_wide_tt &lt;- d_long %&gt;% \n  pivot_wider(values_from = c(erm_thetas,erm_sem),\n              id_cols = c(id,group),\n              names_from = time\n              ) %&gt;% \n  mutate(id = as.numeric(id))\n\n# basic formula for t-test\n# (d_wide_tt$erm_thetas_1[1] - d_wide_tt$erm_thetas_3[1]) / sqrt(d_wide_tt$erm_sem_1[1] + d_wide_tt$erm_sem_3[1])\n\n# t-tests for all rows in dataframe\nt_values &lt;- c()\nfor (i in 1:nrow(d_wide_tt)) {\n t_values[i] &lt;- (d_wide_tt$erm_thetas_3[i] - d_wide_tt$erm_thetas_1[i]) / sqrt(d_wide_tt$erm_sem_3[i] + d_wide_tt$erm_sem_1[i])\n}\n\ndata.frame(t_values = t_values,\n           group = rep(c(\"Intervention\",\"Control\"), each = 250)) %&gt;% \n  mutate(result = case_when(t_values &gt; 1.96 ~ \"Improved\",\n                            t_values &lt; -1.96 ~ \"Worsened\",\n                            TRUE ~ \"No detectable change\")) %&gt;% \n  group_by(group) %&gt;% \n  count(result) %&gt;% \n  kbl_rise(tbl_width = 40) %&gt;% \n  row_spec(c(1,4),bold=TRUE)\n\n\n\n group \n    result \n    n \n  \n\n\n Control \n    Improved \n    12 \n  \n\n Control \n    No detectable change \n    229 \n  \n\n Control \n    Worsened \n    9 \n  \n\n Intervention \n    Improved \n    27 \n  \n\n Intervention \n    No detectable change \n    219 \n  \n\n Intervention \n    Worsened \n    4 \n  \n\n\n\n\n\n4.1 “Recovery” of n changed\nAgain, strongly affected by precision/reliability"
  },
  {
    "objectID": "simcutoffs.html",
    "href": "simcutoffs.html",
    "title": "Simulation based cutoff values for Rasch item fit and residual correlations",
    "section": "",
    "text": "It has long been known that rule-of-thumb cutoff values in psychometrics are not optimal (and often misguiding), and that simulations are helpful for determining appropriate cutoff values based on the properties of the sample and items being analyzed. This actually goes for factor analysis, as well as item response theory and Rasch measurement theory. In recent years, we have seen a number of interactive Shiny apps made available to help solve this issue, which is clearly a step in the right direction.\nMy approach here is to write functions to help users easily run these simulations on their computers in R, when they do their psychometric analysis.\nThis blog post will be incorporated into the easyRasch package vignette at some point, but as I just put quite a bit of time into developing these functions I wanted to write something about it. Also, I have not put all that much time into testing, so I hope to hear about any problems you may run into if you try these functions out.\nFinally, I want to thank Karl Bang Christensen for a very nice presentation on item fit and simulations at the Scandinavian Applied Measurement Conference, which made me think about these things a lot until I came up with the idea to implement these functions."
  },
  {
    "objectID": "simcutoffs.html#background",
    "href": "simcutoffs.html#background",
    "title": "Simulation based cutoff values for Rasch item fit and residual correlations",
    "section": "",
    "text": "It has long been known that rule-of-thumb cutoff values in psychometrics are not optimal (and often misguiding), and that simulations are helpful for determining appropriate cutoff values based on the properties of the sample and items being analyzed. This actually goes for factor analysis, as well as item response theory and Rasch measurement theory. In recent years, we have seen a number of interactive Shiny apps made available to help solve this issue, which is clearly a step in the right direction.\nMy approach here is to write functions to help users easily run these simulations on their computers in R, when they do their psychometric analysis.\nThis blog post will be incorporated into the easyRasch package vignette at some point, but as I just put quite a bit of time into developing these functions I wanted to write something about it. Also, I have not put all that much time into testing, so I hope to hear about any problems you may run into if you try these functions out.\nFinally, I want to thank Karl Bang Christensen for a very nice presentation on item fit and simulations at the Scandinavian Applied Measurement Conference, which made me think about these things a lot until I came up with the idea to implement these functions."
  },
  {
    "objectID": "simcutoffs.html#residual-correlations",
    "href": "simcutoffs.html#residual-correlations",
    "title": "Simulation based cutoff values for Rasch item fit and residual correlations",
    "section": "\n3 Residual correlations",
    "text": "3 Residual correlations\nThis is the simpler of the two functions that have been added. It is based on the simulation study by Christensen et al. (2017).\n\nCodelibrary(easyRasch)\n\n\nFor simplicity, we will use the dataset pcmdat2 included in the eRm package, which has polytomous data. Dichotomous data also works with all functions in this text. The functions will determine the proper model based on the data structure.\n\nCodesimres1 &lt;- RIgetResidCor(pcmdat2, iterations = 1000, cpu = 8)\n\n\nWhile we are mostly interested in the 99% percentile from the simulation (p99 below), we will save the output of the simulation into the list object res_cor.\n\nCodeglimpse(simres1)\n\nList of 8\n $ results          :'data.frame':  1000 obs. of  3 variables:\n  ..$ mean: num [1:1000] -0.246 -0.26 -0.248 -0.262 -0.252 ...\n  ..$ max : num [1:1000] -0.0915 -0.175 -0.0972 -0.1732 -0.1871 ...\n  ..$ diff: num [1:1000] 0.1543 0.0851 0.1506 0.089 0.0649 ...\n $ actual_iterations: num 1000\n $ sample_n         : int 300\n $ sample_summary   : 'summaryDefault' Named num [1:6] -3.873 0.107 0.704 0.684 1.337 ...\n  ..- attr(*, \"names\")= chr [1:6] \"Min.\" \"1st Qu.\" \"Median\" \"Mean\" ...\n $ max_diff         : num 0.229\n $ sd_diff          : num 0.0336\n $ p95              : Named num 0.152\n  ..- attr(*, \"names\")= chr \"95%\"\n $ p99              : Named num 0.174\n  ..- attr(*, \"names\")= chr \"99%\"\n\n\nThe object contains a dataframe with mean and max values for residual correlations within each dataset simulated, and the difference between the mean and the max. This difference is the key metric we are interested in.\nThe max value will increase with the number of iterations, and it seems to be a bit spurious, so I would suggest to use the 99% percentile value as the cutoff. Based on the simulations I have made so far, it seems that 1000 iterations may be sufficient for a reasonably accurate 99% percentile value (p99) value. For more detailed information on the topic I refer to Christensen et al. (2017).\n\nCodeRIresidcorr(pcmdat2, cutoff = simres1$p99)\n\n\n\n   \n    I1 \n    I2 \n    I3 \n    I4 \n  \n\n\n I1 \n     \n     \n     \n     \n  \n\n I2 \n    -0.13 \n     \n     \n     \n  \n\n I3 \n    -0.29 \n    -0.33 \n     \n     \n  \n\n I4 \n    -0.38 \n    -0.44 \n    0.2 \n     \n  \n\n\nNote: \n\n Relative cut-off value (highlighted in red) is -0.052, which is 0.174 above the average correlation (-0.226).\n\n\n\n\nWe can see that items 3 and 4 have a residual correlation between them that is quite a bit above the threshold value.\nSince the results of the simulation is included in the output object, we can investigate it further.\n\nCodehist(simres1$results$diff, breaks = 50, col = \"lightblue\")\nabline(v = simres1$p99, col = \"red\")\nabline(v = simres1$p95, col = \"orange\")\n\n\n\n\n\n\n\n\nCodesummary(simres1$results$diff)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n0.01344 0.06429 0.08356 0.08805 0.10804 0.22923"
  },
  {
    "objectID": "simcutoffs.html#item-fit",
    "href": "simcutoffs.html#item-fit",
    "title": "Simulation based cutoff values for Rasch item fit and residual correlations",
    "section": "\n3 Item fit",
    "text": "3 Item fit\nWhile conditional item fit is a huge step towards getting correct item fit values, no matter what the sample size is (Buchardt et al., 2023; Müller, 2020), we also need to know the range of values that are plausible for items fitting the Rasch model in the current context. The specific properties of the sample and the items is used to simulate multiple datasets that fit the Rasch model, to understand the variation in item fit that can be expected. The simulation results can then be compared with the observed data.\n\n\n\n\n\n\nNote\n\n\n\nWe will use 1000 simulations, but no extensive tests have been made (yet) to determine a reasonable number of simulations. The more items/thresholds and the bigger your sample size, the more time this simulation will take. If you have more cpu cores, this will reduce the time (please remember to not use all cores, leave 1-2 unused). To give some reference, the code below (n = 300, 4 items, 4 response categories) using 500 iterations took 7.7 seconds on 8 cpu cores and 11.7 seconds on 4 cores. 1000 iterations on 8 cores took 12.8 seconds (based on single runs).\n\n\n\nCodeitemfit_sim &lt;- RIgetfit(pcmdat2, iterations = 1000, cpu = 8)\n\n\nThe output is a list object consisting of one dataframe per iteration. We can look at iteration 1:\n\nCodeitemfit_sim[[1]]\n\n  Item InfitMSQ OutfitMSQ\n1   V1    0.938     0.908\n2   V2    1.004     1.064\n3   V3    1.012     1.048\n4   V4    1.066     1.065\n\n\nThis object can in turn be used with three different functions. Most importantly, you can use it with RIitemfit() to set cutoff values for both MSQ and ZSTD based on your sample size and item parameters. This uses the 1st and 99th percentile values for each individual item.\n\nCodeRIitemfit(pcmdat2, simcut = itemfit_sim)\n\n\n\n Item \n    InfitMSQ \n    Infit thresholds \n    OutfitMSQ \n    Outfit thresholds \n  \n\n\n I1 \n    1.08 \n    [0.874, 1.141] \n    1.069 \n    [0.844, 1.192] \n  \n\n I2 \n    1.155 \n    [0.854, 1.164] \n    1.084 \n    [0.792, 1.269] \n  \n\n I3 \n    0.828 \n    [0.866, 1.146] \n    0.802 \n    [0.806, 1.217] \n  \n\n I4 \n    1.007 \n    [0.837, 1.165] \n    1.024 \n    [0.813, 1.198] \n  \n\n\nNote: \n\n MSQ values based on conditional calculations (n = 300).                                Simulation based thresholds based on 1000 simulated datasets.\n\n\n\n\nThe function RIgetfitPlot() uses the package ggdist to plot the distribution of fit values from the simulation results.\n\nCodeRIgetfitPlot(itemfit_sim)\n\n\n\n\n\n\n\nAs a side note, a graphical representation of conditional item fit across class intervals can be helpful to further analyze our observed data.\n\nCodelibrary(RASCHplot) # devtools::install_github(\"ERRTG/RASCHplot\")\nCICCplot(PCM(pcmdat2), which.item = 1:4, grid.items = TRUE)"
  },
  {
    "objectID": "simcutoffs.html#session-info",
    "href": "simcutoffs.html#session-info",
    "title": "Simulation based cutoff values for Rasch item fit and residual correlations",
    "section": "\n6 Session info",
    "text": "6 Session info\n\nCodesessionInfo() \n\nR version 4.4.1 (2024-06-14)\nPlatform: aarch64-apple-darwin20\nRunning under: macOS Sonoma 14.6.1\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\ntime zone: Europe/Stockholm\ntzcode source: internal\n\nattached base packages:\n [1] parallel  grid      stats4    stats     graphics  grDevices utils    \n [8] datasets  methods   base     \n\nother attached packages:\n [1] RASCHplot_0.1.0    ggdist_3.3.2       doParallel_1.0.17  iterators_1.0.14  \n [5] foreach_1.5.2      RISEkbmRasch_0.2.4 janitor_2.2.0      iarm_0.4.3        \n [9] hexbin_1.28.3      catR_3.17          glue_1.7.0         ggrepel_0.9.5     \n[13] patchwork_1.2.0    reshape_0.8.9      matrixStats_1.3.0  psychotree_0.16-1 \n[17] psychotools_0.7-4  partykit_1.2-21    mvtnorm_1.2-5      libcoin_1.0-10    \n[21] psych_2.4.6.26     mirt_1.42          lattice_0.22-6     eRm_1.0-6         \n[25] lubridate_1.9.3    forcats_1.0.0      stringr_1.5.1      dplyr_1.1.4       \n[29] purrr_1.0.2        readr_2.1.5        tidyr_1.3.1        tibble_3.2.1      \n[33] ggplot2_3.5.1      tidyverse_2.0.0    kableExtra_1.4.0   formattable_0.2.1 \n\nloaded via a namespace (and not attached):\n  [1] later_1.3.2          splines_4.4.1        R.oo_1.26.0         \n  [4] cellranger_1.1.0     rpart_4.1.23         lifecycle_1.0.4     \n  [7] rstatix_0.7.2        rprojroot_2.0.4      globals_0.16.3      \n [10] MASS_7.3-61          backports_1.5.0      magrittr_2.0.3      \n [13] vcd_1.4-12           Hmisc_5.1-3          rmarkdown_2.27      \n [16] yaml_2.3.10          httpuv_1.6.15        sessioninfo_1.2.2   \n [19] pbapply_1.7-2        RColorBrewer_1.1-3   abind_1.4-5         \n [22] audio_0.1-11         quadprog_1.5-8       R.utils_2.12.3      \n [25] nnet_7.3-19          listenv_0.9.1        testthat_3.2.1.1    \n [28] RPushbullet_0.3.4    vegan_2.6-6.1        parallelly_1.38.0   \n [31] svglite_2.1.3        permute_0.9-7        codetools_0.2-20    \n [34] DT_0.33              xml2_1.3.6           tidyselect_1.2.1    \n [37] farver_2.1.2         base64enc_0.1-3      jsonlite_1.8.8      \n [40] progressr_0.14.0     Formula_1.2-5        survival_3.7-0      \n [43] systemfonts_1.1.0    tools_4.4.1          gnm_1.1-5           \n [46] Rcpp_1.0.13          mnormt_2.1.1         gridExtra_2.3       \n [49] xfun_0.46            here_1.0.1           mgcv_1.9-1          \n [52] distributional_0.4.0 ca_0.71.1            withr_3.0.1         \n [55] beepr_2.0            fastmap_1.2.0        fansi_1.0.6         \n [58] digest_0.6.36        mime_0.12            timechange_0.3.0    \n [61] R6_2.5.1             colorspace_2.1-1     R.methodsS3_1.8.2   \n [64] inum_1.0-5           utf8_1.2.4           generics_0.1.3      \n [67] data.table_1.15.4    SimDesign_2.16       htmlwidgets_1.6.4   \n [70] pkgconfig_2.0.3      gtable_0.3.5         lmtest_0.9-40       \n [73] brio_1.1.5           htmltools_0.5.8.1    carData_3.0-5       \n [76] scales_1.3.0         corrplot_0.92        snakecase_0.11.1    \n [79] knitr_1.48           rstudioapi_0.16.0    reshape2_1.4.4      \n [82] tzdb_0.4.0           checkmate_2.3.2      nlme_3.1-165        \n [85] curl_5.2.1           zoo_1.8-12           cachem_1.1.0        \n [88] relimp_1.0-5         vcdExtra_0.8-5       foreign_0.8-87      \n [91] pillar_1.9.0         vctrs_0.6.5          promises_1.3.0      \n [94] ggpubr_0.6.0         car_3.1-2            xtable_1.8-4        \n [97] Deriv_4.1.3          cluster_2.1.6        dcurver_0.9.2       \n[100] GPArotation_2024.3-1 htmlTable_2.4.3      evaluate_0.24.0     \n[103] cli_3.6.3            compiler_4.4.1       rlang_1.1.4         \n[106] future.apply_1.11.2  ggsignif_0.6.4       labeling_0.4.3      \n[109] plyr_1.8.9           stringi_1.8.4        viridisLite_0.4.2   \n[112] munsell_0.5.1        Matrix_1.7-0         qvcalc_1.0.3        \n[115] hms_1.1.3            future_1.34.0        shiny_1.9.1         \n[118] broom_1.0.6          memoise_2.0.1        ggstance_0.3.7      \n[121] readxl_1.4.3"
  },
  {
    "objectID": "simcutoffs.html#conditional-item-fit",
    "href": "simcutoffs.html#conditional-item-fit",
    "title": "Simulation based cutoff values for Rasch item fit and residual correlations",
    "section": "\n4 Conditional item fit",
    "text": "4 Conditional item fit\nWhile conditional item fit is a huge step towards getting correct item fit values (Buchardt et al., 2023; Christensen & Kreiner, 2012; Müller, 2020), we also need to know the range of values that are plausible for items fitting the Rasch model in the current context. The specific properties of the sample and the items is used to simulate multiple datasets that fit the Rasch model, to understand the variation in item fit that can be expected. The simulation results can then be compared with the observed data.\n\n\n\n\n\n\nNote\n\n\n\nWe will use 1000 simulations, but no extensive tests have been made (yet) to determine a reasonable number of simulations. The more items/thresholds and the bigger your sample size, the more time this simulation will take. If you have more cpu cores, this will reduce the time (please remember to not use all cores, leave 1-2 unused).\nTo give some reference, the code below (n = 300, 4 items, 4 response categories per item) using 500 iterations took 3.7 seconds on 4 cpu cores. 1000 iterations on 8 cores took 4.2 seconds (based on single runs). I recommend library(tictoc) to make timing easy.\n\n\n\nCodesimfit1 &lt;- RIgetfit(pcmdat2, iterations = 1000, cpu = 8)\n\n\nThe output is a list object consisting of one dataframe per iteration. We can look at iteration 1:\n\nCodesimfit1[[1]]\n\n  Item InfitMSQ OutfitMSQ\n1   I1    0.962     0.969\n2   I2    0.964     0.946\n3   I3    1.041     1.027\n4   I4    1.020     1.023\n\n\nThis object can in turn be used with three different functions. Most importantly, you can use it with RIitemfit() to automatically set the simulation based cutoff values for infit & outfit MSQ based on your sample size and item parameters. RIitemfit() uses the .05 and 99.5 percentile values for each individual item. The cutoff limit might be added as a user option later. You can optionally sort the table output according to misfit by using sort = \"infit\".\n\nCodeRIitemfit(pcmdat2, simfit1)\n\n\n\n Item \n    InfitMSQ \n    Infit thresholds \n    OutfitMSQ \n    Outfit thresholds \n    Infit diff \n    Outfit diff \n  \n\n\n I1 \n    1.08 \n    [0.853, 1.138] \n    1.069 \n    [0.835, 1.201] \n    no misfit \n    no misfit \n  \n\n I2 \n    1.155 \n    [0.838, 1.169] \n    1.084 \n    [0.794, 1.256] \n    no misfit \n    no misfit \n  \n\n I3 \n    0.828 \n    [0.849, 1.174] \n    0.802 \n    [0.792, 1.238] \n    0.021 \n    no misfit \n  \n\n I4 \n    1.007 \n    [0.838, 1.171] \n    1.024 \n    [0.808, 1.22] \n    no misfit \n    no misfit \n  \n\n\nNote: \n\n MSQ values based on conditional calculations (n = 300).                                Simulation based thresholds based on 1000 simulated datasets.\n\n\n\n\nThe function RIgetfitPlot() uses the package ggdist to plot the distribution of fit values from the simulation results. You can get the observed conditional item fit included in the plot by using the option data = yourdataframe.\n\nCodeRIgetfitPlot(simfit1, pcmdat2)"
  },
  {
    "objectID": "simcutoffs.html#visualizing-conditional-fit",
    "href": "simcutoffs.html#visualizing-conditional-fit",
    "title": "Simulation based cutoff values for Rasch item fit and residual correlations",
    "section": "\n5 Visualizing conditional fit",
    "text": "5 Visualizing conditional fit\nAs a side note, a graphical representation of conditional item fit across class intervals (Buchardt et al., 2023) can be helpful to further analyze our observed data.\n\nCodelibrary(RASCHplot) # devtools::install_github(\"ERRTG/RASCHplot\")\nCICCplot(PCM(pcmdat2), which.item = 3)\n\n$I3"
  },
  {
    "objectID": "simcutoffs.html#simulation-methodology",
    "href": "simcutoffs.html#simulation-methodology",
    "title": "Simulation based cutoff values for Rasch item fit and residual correlations",
    "section": "\n2 Simulation methodology",
    "text": "2 Simulation methodology\nThe process is similar for both functions:\n\nEstimate item parameters (using conditional maximum likelihood) and person parameters (thetas, using weighted likelihood).\nResample thetas with replacement (parametric bootstrapping) and simulate response data (that fit a Rasch model) based on the resampled thetas and previously estimated item parameters\nCalculate conditional item fit/residual correlations for the simulated response data\nRepeat 2 & 3 across n iterations.\n\nThe simulation then results in an empirical distribution of plausible values."
  },
  {
    "objectID": "est_comp.html",
    "href": "est_comp.html",
    "title": "Comparing Rasch packages/estimators",
    "section": "",
    "text": "I’m doing comparisons of the bias in the estimators for the Partial Credit Model (PCM) implemented in Rasch R packages and thought I would share a bit of code and simulated data. There will be more extensive simulations and comparisons coming along, varying targeting and sample distribution.\nFor this example we have a simple setup:\n\n250 datasets\neach dataset has 9 items and 4 thresholds (5 response categories)\neach dataset has 720 respondents (20 per threshold estimated, which should be plenty)\n\nThe .rds data file contains a list object with the 250 datasets, and each dataset is accompanied by a matrix of item threshold parameters and a vector of theta values used to generate the response data.\n\nthe vector of theta values is normally distributed with a mean and SD closely matching that of the item parameters\n\nWe have four packages in the comparison:\n\n\neRm - Conditional Maximum Likelihood (CML)\n\nTAM - Marginal ML (MML)\n\npairwise - Pairwise CML (PCML)\n\nmirt - fixed quadrature expectation-maximization algorithm\n\nOther estimation methods are available in TAM and mirt, but we’ll just test these for now.\nThe datafile is available in the Github repo."
  },
  {
    "objectID": "est_comp.html#background",
    "href": "est_comp.html#background",
    "title": "Comparing Rasch packages/estimators",
    "section": "",
    "text": "I’m doing comparisons of the bias in the estimators for the Partial Credit Model (PCM) implemented in Rasch R packages and thought I would share a bit of code and simulated data. There will be more extensive simulations and comparisons coming along, varying targeting and sample distribution.\nFor this example we have a simple setup:\n\n250 datasets\neach dataset has 9 items and 4 thresholds (5 response categories)\neach dataset has 720 respondents (20 per threshold estimated, which should be plenty)\n\nThe .rds data file contains a list object with the 250 datasets, and each dataset is accompanied by a matrix of item threshold parameters and a vector of theta values used to generate the response data.\n\nthe vector of theta values is normally distributed with a mean and SD closely matching that of the item parameters\n\nWe have four packages in the comparison:\n\n\neRm - Conditional Maximum Likelihood (CML)\n\nTAM - Marginal ML (MML)\n\npairwise - Pairwise CML (PCML)\n\nmirt - fixed quadrature expectation-maximization algorithm\n\nOther estimation methods are available in TAM and mirt, but we’ll just test these for now.\nThe datafile is available in the Github repo."
  },
  {
    "objectID": "est_comp.html#data-import",
    "href": "est_comp.html#data-import",
    "title": "Comparing Rasch packages/estimators",
    "section": "\n2 Data import",
    "text": "2 Data import\n\nCodelibrary(tidyverse)\nlibrary(eRm)\nlibrary(janitor)\nlibrary(TAM)\nlibrary(mirt)\nlibrary(pairwise)\nlibrary(ggdist)\nlibrary(doParallel)\nlibrary(tinytable)\n\n### some commands exist in multiple packages, here we define preferred ones that are frequently used\nselect &lt;- dplyr::select\ncount &lt;- dplyr::count\nrecode &lt;- car::recode\nrename &lt;- dplyr::rename\n\n# read 250 simulated datasets, stored in a list() object\nsim_data_4t &lt;- readRDS(\"sim_data_4t.rds\")"
  },
  {
    "objectID": "est_comp.html#estimation",
    "href": "est_comp.html#estimation",
    "title": "Comparing Rasch packages/estimators",
    "section": "\n3 Estimation",
    "text": "3 Estimation\nWe’ll make use of library(doParallel) for multicore processing.\n\nCoderegisterDoParallel(cores = 8)\niterations = 250\n\nsim_results_4t &lt;- list()\n\nsim_results_4t &lt;- foreach(i = 1:iterations) %dopar% {\n\n  input_params &lt;- sim_data_4t[[i]]$input_params\n  input_thetas &lt;- sim_data_4t[[i]]$input_thetas\n  testData &lt;- sim_data_4t[[i]]$data\n\n  # check mean/centrality (item parameters should have been centered before data generation)\n  input_params_correction &lt;- mean(input_params)\n\n  # eRm\n  erm_out &lt;- PCM(testData)\n  erm_params &lt;- thresholds(erm_out)[[3]][[1]][,-1] %&gt;%\n    as.data.frame() %&gt;%\n    set_names(c(\"T1\",\"T2\",\"T3\",\"T4\")) %&gt;%\n    as.matrix()\n\n  erm_params_c &lt;- erm_params - mean(erm_params) + input_params_correction\n\n  # TAM\n  tam_out &lt;- tam(as.matrix(testData), irtmodel = \"PCM\", verbose = FALSE)\n\n  tam_params &lt;- tam_out$item_irt %&gt;%\n    as.data.frame() %&gt;%\n    select(starts_with(\"tau\")) %&gt;%\n    set_names(c(\"T1\",\"T2\",\"T3\",\"T4\")) %&gt;%\n    as.matrix()\n\n  tam_params_c &lt;- tam_params - mean(tam_params) + input_params_correction\n\n  # mirt\n  mirt_out &lt;- mirt(data = testData, model = 1, itemtype = \"Rasch\", verbose = FALSE)\n  mirt_params &lt;- coef(mirt_out, simplify = TRUE, IRTpars = TRUE)$items %&gt;%\n    as.data.frame() %&gt;%\n    select(!a) %&gt;%\n    set_names(c(\"T1\",\"T2\",\"T3\",\"T4\")) %&gt;%\n    as.matrix()\n  mirt_params_c &lt;- mirt_params - mean(mirt_params) + input_params_correction\n\n  # PAIR\n  pair_out &lt;- pair(testData)\n  pair_params &lt;- deltapar(pair_out) %&gt;%\n    as.data.frame()\n\n  pair_params &lt;- pair_params[,-1] %&gt;%\n    set_names(c(\"T1\",\"T2\",\"T3\",\"T4\")) %&gt;%\n    as.matrix()\n\n  pair_params_c &lt;- pair_params - mean(pair_params) + input_params_correction\n\n  # combine item parameters and calculate lowest to highest threshold distances\n  input_params %&gt;%\n    as.data.frame() %&gt;%\n    rename(T1 = V1,\n           T2 = V2,\n           T3 = V3,\n           T4 = V4) %&gt;%\n    add_column(Item = c(1:9),\n               Type = \"Input\") %&gt;%\n    bind_rows(\n      erm_params_c %&gt;%\n        as.data.frame() %&gt;%\n        add_column(Item = c(1:9),\n                   Type = \"eRm\")\n    ) %&gt;%\n    bind_rows(\n      tam_params_c %&gt;%\n        as.data.frame() %&gt;%\n        add_column(Item = c(1:9),\n                   Type = \"TAM\")\n    ) %&gt;%\n    bind_rows(\n      mirt_params_c %&gt;%\n        as.data.frame() %&gt;%\n        add_column(Item = c(1:9),\n                   Type = \"mirt\")\n    ) %&gt;%\n    bind_rows(\n      pair_params_c %&gt;%\n        as.data.frame() %&gt;%\n        add_column(Item = c(1:9),\n                   Type = \"PAIR\")\n    ) %&gt;%\n    remove_rownames()\n}"
  },
  {
    "objectID": "est_comp.html#results",
    "href": "est_comp.html#results",
    "title": "Comparing Rasch packages/estimators",
    "section": "\n4 Results",
    "text": "4 Results\n\nCode# collect item threshold parameters to a single dataframe\nresults_params_4t &lt;- map_df(1:iterations, ~ sim_results_4t[[.x]] %&gt;%\n                               mutate(iteration = .x))\n\n\n\n4.1 Summary table MAE\nMean Absolute Error\n\nCode# calculate absolute differences between input and estimated item thresholds per estimator, average by item\nresults_params_diff_4t &lt;- results_params_4t %&gt;%\n  pivot_wider(\n    values_from = c(\"T1\", \"T2\", \"T3\", \"T4\"),\n    names_from = \"Type\",\n    id_cols = c(\"iteration\", \"Item\")\n  ) %&gt;%\n  group_by(iteration, Item) %&gt;%\n  summarise(\n    diff_eRm = sum(abs(T1_Input - T1_eRm) + abs(T2_Input - T2_eRm) + abs(T3_Input - T3_eRm) + abs(T4_Input - T4_eRm))/4,\n    diff_TAM = sum(abs(T1_Input - T1_TAM) + abs(T2_Input - T2_TAM) + abs(T3_Input - T3_TAM) + abs(T4_Input - T4_TAM))/4,\n    diff_mirt = sum(abs(T1_Input - T1_mirt) + abs(T2_Input - T2_mirt) + abs(T3_Input - T3_mirt) + abs(T4_Input - T4_mirt))/4,\n    diff_PAIR = sum(abs(T1_Input - T1_PAIR) + abs(T2_Input - T2_PAIR) + abs(T3_Input - T3_PAIR) + abs(T4_Input - T4_PAIR))/4\n  ) %&gt;%\n  ungroup() %&gt;%\n  pivot_longer(\n    cols = c(diff_eRm, diff_TAM,diff_mirt,diff_PAIR),\n    names_to = c(NA, \"Estimator\"),\n    names_sep = \"_\",\n    values_to = \"diff\"\n  )\n\n# produce table with summary stats per estimator/package\nresults_params_diff_4t %&gt;%\n  group_by(Estimator) %&gt;%\n  summarise(\n    Median = median(diff),\n    MAD = mad(diff),\n    IQR = IQR(diff),\n    Mean = mean(diff),\n    SD = sd(diff)\n  ) %&gt;%\n  mutate(across(where(is.numeric), ~ round(.x,3))) %&gt;%\n  rename(Package = Estimator) %&gt;%\n  arrange(Median) %&gt;%\n  tt()\n\n \n\n  \n    \n\ntinytable_0po126oqgr0buj0q9gye\n\n\n      \n\nPackage\n                Median\n                MAD\n                IQR\n                Mean\n                SD\n              \n\n\nmirt\n                  0.101\n                  0.043\n                  0.059\n                  0.106\n                  0.043\n                \n\neRm \n                  0.102\n                  0.043\n                  0.059\n                  0.106\n                  0.043\n                \n\nTAM \n                  0.140\n                  0.064\n                  0.087\n                  0.152\n                  0.069\n                \n\nPAIR\n                  0.146\n                  0.065\n                  0.091\n                  0.157\n                  0.070\n                \n\n\n\n\n    \n\n\n\n4.2 Summary table RMSE\nRoot Mean Squared Error\n\nCode# RMSE\nresults_params_4t %&gt;%\n  pivot_wider(\n    values_from = c(\"T1\", \"T2\", \"T3\", \"T4\"),\n    names_from = \"Type\",\n    id_cols = c(\"iteration\", \"Item\")\n  ) %&gt;%\n  group_by(iteration, Item) %&gt;% # sqrt(mean((data$actual - data$predicted)^2))\n  summarise(\n    rmse_eRm = sqrt(mean(((T1_Input - T1_eRm)^2) + ((T2_Input - T2_eRm)^2) + ((T3_Input - T3_eRm)^2) + ((T4_Input - T4_eRm))^2)),\n        rmse_TAM = sqrt(mean(((T1_Input - T1_TAM)^2) + ((T2_Input - T2_TAM)^2) + ((T3_Input - T3_TAM)^2) + ((T4_Input - T4_TAM))^2)),\n        rmse_mirt = sqrt(mean(((T1_Input - T1_mirt)^2) + ((T2_Input - T2_mirt)^2) + ((T3_Input - T3_mirt)^2) + ((T4_Input - T4_mirt))^2)),\n        rmse_PAIR = sqrt(mean(((T1_Input - T1_PAIR)^2) + ((T2_Input - T2_PAIR)^2) + ((T3_Input - T3_PAIR)^2) + ((T4_Input - T4_PAIR))^2))\n  ) %&gt;%\n  ungroup() %&gt;%\n  pivot_longer(\n    cols = c(rmse_eRm, rmse_TAM,rmse_mirt,rmse_PAIR),\n    names_to = c(NA, \"Estimator\"),\n    names_sep = \"_\",\n    values_to = \"rmse\"\n  ) %&gt;% \n  group_by(Estimator) %&gt;%\n  summarise(\n    Median = median(rmse),\n    MAD = mad(rmse),\n    IQR = IQR(rmse),\n    Mean = mean(rmse),\n    SD = sd(rmse)\n  ) %&gt;%\n  mutate(across(where(is.numeric), ~ round(.x,3))) %&gt;%\n  rename(Package = Estimator) %&gt;%\n  arrange(Median) %&gt;% \n  tt()\n\n \n\n  \n    \n\ntinytable_q26dyrtpg74yhyxvj2vb\n\n\n      \n\nPackage\n                Median\n                MAD\n                IQR\n                Mean\n                SD\n              \n\n\nmirt\n                  0.240\n                  0.103\n                  0.138\n                  0.252\n                  0.101\n                \n\neRm \n                  0.242\n                  0.102\n                  0.137\n                  0.253\n                  0.101\n                \n\nTAM \n                  0.330\n                  0.147\n                  0.200\n                  0.350\n                  0.148\n                \n\nPAIR\n                  0.344\n                  0.149\n                  0.204\n                  0.364\n                  0.151\n                \n\n\n\n\n    \n\n\n\n4.3 Thresholds summary MAE\n\nCode# table summarizing all thresholds and estimators\nresults_params_4t %&gt;%\n  pivot_wider(values_from = c(\"T1\", \"T2\", \"T3\", \"T4\"),\n              names_from = \"Type\",\n              id_cols = c(\"iteration\",\"Item\")\n  ) %&gt;%\n  group_by(iteration,Item) %&gt;%\n  summarise(\n    diff_eRm.T1 = abs(T1_eRm - T1_Input),\n    diff_eRm.T2 = abs(T2_eRm - T2_Input),\n    diff_eRm.T3 = abs(T3_eRm - T3_Input),\n    diff_eRm.T4 = abs(T4_eRm - T4_Input),\n    diff_TAM.T1 = abs(T1_TAM - T1_Input),\n    diff_TAM.T2 = abs(T2_TAM - T2_Input),\n    diff_TAM.T3 = abs(T3_TAM - T3_Input),\n    diff_TAM.T4 = abs(T4_TAM - T4_Input),\n    diff_mirt.T1 = abs(T1_mirt - T1_Input),\n    diff_mirt.T2 = abs(T2_mirt - T2_Input),\n    diff_mirt.T3 = abs(T3_mirt - T3_Input),\n    diff_mirt.T4 = abs(T4_mirt - T4_Input),\n    diff_PAIR.T1 = abs(T1_PAIR - T1_Input),\n    diff_PAIR.T2 = abs(T2_PAIR - T2_Input),\n    diff_PAIR.T3 = abs(T3_PAIR - T3_Input),\n    diff_PAIR.T4 = abs(T4_PAIR - T4_Input)\n  ) %&gt;%\n  ungroup() %&gt;%\n  pivot_longer(cols = starts_with(\"diff\"),\n               names_to = c(\"Estimator\",\"Threshold\"),\n               names_sep = \"\\\\.\",\n               values_to = \"diff\") %&gt;%\n  mutate(Package = gsub(\"diff_\",\"\",Estimator)) %&gt;%\n  group_by(Package, Threshold) %&gt;%\n  summarise(\n    Median = median(diff),\n    MAD = mad(diff),\n    IQR = IQR(diff),\n    Mean = mean(diff),\n    SD = sd(diff)\n  ) %&gt;%\n  mutate(across(where(is.numeric), ~ round(.x,3))) %&gt;%\n  arrange(Threshold,Median) %&gt;%\n  tt() %&gt;% \n  style_tt(i = c(1, 5, 9, 13), j = 2, rowspan = 4, alignv = \"t\")\n\n \n\n  \n    \n\ntinytable_bebptriuqhnmt4cb6k99\n\n\n      \n\nPackage\n                Threshold\n                Median\n                MAD\n                IQR\n                Mean\n                SD\n              \n\n\neRm \n                  T1\n                  0.106\n                  0.093\n                  0.130\n                  0.127\n                  0.097\n                \n\nmirt\n                  T1\n                  0.106\n                  0.094\n                  0.131\n                  0.126\n                  0.097\n                \n\nTAM \n                  T1\n                  0.133\n                  0.116\n                  0.160\n                  0.157\n                  0.118\n                \n\nPAIR\n                  T1\n                  0.142\n                  0.118\n                  0.166\n                  0.164\n                  0.121\n                \n\neRm \n                  T2\n                  0.072\n                  0.063\n                  0.088\n                  0.085\n                  0.064\n                \n\nmirt\n                  T2\n                  0.072\n                  0.062\n                  0.087\n                  0.085\n                  0.063\n                \n\nTAM \n                  T2\n                  0.123\n                  0.108\n                  0.153\n                  0.144\n                  0.106\n                \n\nPAIR\n                  T2\n                  0.129\n                  0.113\n                  0.158\n                  0.148\n                  0.109\n                \n\neRm \n                  T3\n                  0.072\n                  0.066\n                  0.094\n                  0.086\n                  0.066\n                \n\nmirt\n                  T3\n                  0.073\n                  0.067\n                  0.091\n                  0.085\n                  0.065\n                \n\nTAM \n                  T3\n                  0.119\n                  0.109\n                  0.153\n                  0.144\n                  0.110\n                \n\nPAIR\n                  T3\n                  0.120\n                  0.108\n                  0.155\n                  0.147\n                  0.113\n                \n\nmirt\n                  T4\n                  0.105\n                  0.091\n                  0.131\n                  0.127\n                  0.097\n                \n\neRm \n                  T4\n                  0.106\n                  0.093\n                  0.130\n                  0.128\n                  0.097\n                \n\nTAM \n                  T4\n                  0.138\n                  0.117\n                  0.164\n                  0.163\n                  0.121\n                \n\nPAIR\n                  T4\n                  0.144\n                  0.121\n                  0.171\n                  0.170\n                  0.129\n                \n\n\n\n\n    \n\n\n\n4.4 Figure\n\nCoderesults_params_4t %&gt;%\n  pivot_wider(values_from = c(\"T1\", \"T2\", \"T3\", \"T4\"),\n              names_from = \"Type\",\n              id_cols = c(\"iteration\",\"Item\")\n  ) %&gt;%\n  group_by(iteration,Item) %&gt;%\n  summarise(\n    diff_eRm.T1 = T1_eRm - T1_Input,\n    diff_eRm.T2 = T2_eRm - T2_Input,\n    diff_eRm.T3 = T3_eRm - T3_Input,\n    diff_eRm.T4 = T4_eRm - T4_Input,\n    diff_TAM.T1 = T1_TAM - T1_Input,\n    diff_TAM.T2 = T2_TAM - T2_Input,\n    diff_TAM.T3 = T3_TAM - T3_Input,\n    diff_TAM.T4 = T4_TAM - T4_Input,\n    diff_mirt.T1 = T1_mirt - T1_Input,\n    diff_mirt.T2 = T2_mirt - T2_Input,\n    diff_mirt.T3 = T3_mirt - T3_Input,\n    diff_mirt.T4 = T4_mirt - T4_Input,\n    diff_PAIR.T1 = T1_PAIR - T1_Input,\n    diff_PAIR.T2 = T2_PAIR - T2_Input,\n    diff_PAIR.T3 = T3_PAIR - T3_Input,\n    diff_PAIR.T4 = T4_PAIR - T4_Input\n  ) %&gt;%\n  ungroup() %&gt;%\n  pivot_longer(cols = starts_with(\"diff\"),\n               names_to = c(\"Estimator\",\"Threshold\"),\n               names_sep = \"\\\\.\",\n               values_to = \"diff\") %&gt;%\n  mutate(Package = gsub(\"diff_\",\"\",Estimator)) %&gt;%\n  mutate(Threshold = car::recode(Threshold,\"'T1'='Threshold 1';'T2'='Threshold 2';'T3'='Threshold 3';'T4'='Threshold 4';\")) %&gt;%\n  ggplot(aes(x = diff, y = Package, slab_fill = after_stat(level))) +\n  stat_dotsinterval(quantiles = 250, point_interval = \"median_qi\",\n                    layout = \"weave\", slab_color = NA, .width = c(.66,.95)) +\n  labs(caption = \"Point interval: median_qi (.66 and .95). Based on 250 simulated datasets with 9 items and 720 respondents each.\",\n       x = \"Bias (logit scale)\",\n       y = \"Package\",\n       title = \"Distribution of item threshold estimation bias\",\n       subtitle = \"Rasch Partial Credit Model\") +\n  scale_color_manual(guide = \"none\", values = scales::brewer_pal()(3)[-1], aesthetics = \"slab_fill\") +\n  facet_wrap(~Threshold, nrow = 1) +\n  geom_vline(xintercept = 0, linetype = \"dashed\") +\n  theme_minimal()"
  },
  {
    "objectID": "est_comp.html#smaller-sample",
    "href": "est_comp.html#smaller-sample",
    "title": "Comparing Rasch packages/estimators",
    "section": "\n5 Smaller sample",
    "text": "5 Smaller sample\nSince the pairwise method has been claimed to be good with small samples, at least compared to MML (Finch and French 2019). Notably, the paper mentions using the ltm package for MML estimation (which is referred to as “standard MLE” in the paper), for PCM estimation, but PCM is not available in ltm.\nI was interested to have a quick look at this. We’ll use the same 250 datasets again, but randomly select 108 respondents (3 per threshold estimated) from each dataset.\n\nCoderegisterDoParallel(cores = 8)\niterations = 250\nsamplesize = 108 # n = 3 per threshold (instead of 20/threshold)\n\nsim_results_4t2 &lt;- list()\n\nsim_results_4t2 &lt;- foreach(i = 1:iterations) %dopar% {\n\n  input_params &lt;- sim_data_4t[[i]]$input_params\n  testData &lt;- sim_data_4t[[i]]$data[sample(1:720, samplesize), ]\n\n  # check mean/centrality (item parameters should have been centered before data generation)\n  input_params_correction &lt;- mean(input_params)\n\n  # eRm\n  erm_out &lt;- PCM(testData)\n  erm_params &lt;- thresholds(erm_out)[[3]][[1]][,-1] %&gt;%\n    as.data.frame() %&gt;%\n    set_names(c(\"T1\",\"T2\",\"T3\",\"T4\")) %&gt;%\n    as.matrix()\n\n  erm_params_c &lt;- erm_params - mean(erm_params) + input_params_correction\n\n  # TAM\n  tam_out &lt;- tam(as.matrix(testData), irtmodel = \"PCM\", verbose = FALSE)\n\n  tam_params &lt;- tam_out$item_irt %&gt;%\n    as.data.frame() %&gt;%\n    select(starts_with(\"tau\")) %&gt;%\n    set_names(c(\"T1\",\"T2\",\"T3\",\"T4\")) %&gt;%\n    as.matrix()\n\n  tam_params_c &lt;- tam_params - mean(tam_params) + input_params_correction\n\n  # mirt\n  mirt_out &lt;- mirt(data = testData, model = 1, itemtype = \"Rasch\", verbose = FALSE)\n  mirt_params &lt;- coef(mirt_out, simplify = TRUE, IRTpars = TRUE)$items %&gt;%\n    as.data.frame() %&gt;%\n    select(!a) %&gt;%\n    set_names(c(\"T1\",\"T2\",\"T3\",\"T4\")) %&gt;%\n    as.matrix()\n  mirt_params_c &lt;- mirt_params - mean(mirt_params) + input_params_correction\n\n  # PAIR\n  pair_out &lt;- pair(testData, m = 5)\n  pair_params &lt;- deltapar(pair_out) %&gt;%\n    as.data.frame()\n\n  pair_params &lt;- pair_params[,-1] %&gt;%\n    set_names(c(\"T1\",\"T2\",\"T3\",\"T4\")) %&gt;%\n    as.matrix()\n\n  pair_params_c &lt;- pair_params - mean(pair_params) + input_params_correction\n\n  # combine item parameters and calculate lowest to highest threshold distances\n  input_params %&gt;%\n    as.data.frame() %&gt;%\n    rename(T1 = V1,\n           T2 = V2,\n           T3 = V3,\n           T4 = V4) %&gt;%\n    add_column(Item = c(1:9),\n               Type = \"Input\") %&gt;%\n    bind_rows(\n      erm_params_c %&gt;%\n        as.data.frame() %&gt;%\n        add_column(Item = c(1:9),\n                   Type = \"eRm\")\n    ) %&gt;%\n    bind_rows(\n      tam_params_c %&gt;%\n        as.data.frame() %&gt;%\n        add_column(Item = c(1:9),\n                   Type = \"TAM\")\n    ) %&gt;%\n    bind_rows(\n      mirt_params_c %&gt;%\n        as.data.frame() %&gt;%\n        add_column(Item = c(1:9),\n                   Type = \"mirt\")\n    ) %&gt;%\n    bind_rows(\n      pair_params_c %&gt;%\n        as.data.frame() %&gt;%\n        add_column(Item = c(1:9),\n                   Type = \"PAIR\")\n    ) %&gt;%\n    remove_rownames()\n}\n\n\n\n5.1 Results\n\nCode# collect item threshold parameters to a single dataframe\nresults_params_4t2 &lt;- map_df(1:iterations, ~ sim_results_4t2[[.x]] %&gt;%\n                               mutate(iteration = .x))\n\n\n\n5.1.1 Summary table MAE\n\nCode# calculate absolute differences between input and estimated item thresholds per estimator, average by item\nresults_params_diff_4t2 &lt;- results_params_4t2 %&gt;%\n  pivot_wider(\n    values_from = c(\"T1\", \"T2\", \"T3\", \"T4\"),\n    names_from = \"Type\",\n    id_cols = c(\"iteration\", \"Item\")\n  ) %&gt;%\n  group_by(iteration, Item) %&gt;%\n  summarise(\n    diff_eRm = sum(abs(T1_Input - T1_eRm) + abs(T2_Input - T2_eRm) + abs(T3_Input - T3_eRm) + abs(T4_Input - T4_eRm))/4,\n    diff_TAM = sum(abs(T1_Input - T1_TAM) + abs(T2_Input - T2_TAM) + abs(T3_Input - T3_TAM) + abs(T4_Input - T4_TAM))/4,\n    diff_mirt = sum(abs(T1_Input - T1_mirt) + abs(T2_Input - T2_mirt) + abs(T3_Input - T3_mirt) + abs(T4_Input - T4_mirt))/4,\n    diff_PAIR = sum(abs(T1_Input - T1_PAIR) + abs(T2_Input - T2_PAIR) + abs(T3_Input - T3_PAIR) + abs(T4_Input - T4_PAIR))/4\n  ) %&gt;%\n  ungroup() %&gt;%\n  pivot_longer(\n    cols = c(diff_eRm, diff_TAM,diff_mirt,diff_PAIR),\n    names_to = c(NA, \"Estimator\"),\n    names_sep = \"_\",\n    values_to = \"diff\"\n  )\n\n# produce table with summary stats per estimator/package\nresults_params_diff_4t2 %&gt;%\n  group_by(Estimator) %&gt;%\n  summarise(\n    Median = median(diff, na.rm = TRUE),\n    MAD = mad(diff, na.rm = TRUE),\n    IQR = IQR(diff, na.rm = TRUE),\n    Mean = mean(diff, na.rm = TRUE),\n    SD = sd(diff, na.rm = TRUE),\n  ) %&gt;%\n  mutate(across(where(is.numeric), ~ round(.x,3))) %&gt;%\n  rename(Package = Estimator) %&gt;%\n  arrange(Median) %&gt;%\n  tt()\n\n \n\n  \n    \n\ntinytable_eat8y8n90g0mp1tyxubx\n\n\n      \n\nPackage\n                Median\n                MAD\n                IQR\n                Mean\n                SD\n              \n\n\neRm \n                  0.273\n                  0.121\n                  0.167\n                  0.293\n                  0.127\n                \n\nmirt\n                  0.273\n                  0.121\n                  0.166\n                  0.292\n                  0.126\n                \n\nTAM \n                  0.283\n                  0.122\n                  0.168\n                  0.301\n                  0.131\n                \n\nPAIR\n                  0.297\n                  0.130\n                  0.182\n                  0.326\n                  0.184\n                \n\n\n\n\n    \n\n\n\n5.1.2 Summary table RMSE\n\nCode# RMSE\nresults_params_4t2 %&gt;%\n  pivot_wider(\n    values_from = c(\"T1\", \"T2\", \"T3\", \"T4\"),\n    names_from = \"Type\",\n    id_cols = c(\"iteration\", \"Item\")\n  ) %&gt;%\n  group_by(iteration, Item) %&gt;% # sqrt(mean((data$actual - data$predicted)^2))\n  summarise(\n    rmse_eRm = sqrt(mean(((T1_Input - T1_eRm)^2) + ((T2_Input - T2_eRm)^2) + ((T3_Input - T3_eRm)^2) + ((T4_Input - T4_eRm))^2)),\n        rmse_TAM = sqrt(mean(((T1_Input - T1_TAM)^2) + ((T2_Input - T2_TAM)^2) + ((T3_Input - T3_TAM)^2) + ((T4_Input - T4_TAM))^2)),\n        rmse_mirt = sqrt(mean(((T1_Input - T1_mirt)^2) + ((T2_Input - T2_mirt)^2) + ((T3_Input - T3_mirt)^2) + ((T4_Input - T4_mirt))^2)),\n        rmse_PAIR = sqrt(mean(((T1_Input - T1_PAIR)^2) + ((T2_Input - T2_PAIR)^2) + ((T3_Input - T3_PAIR)^2) + ((T4_Input - T4_PAIR))^2))\n  ) %&gt;%\n  ungroup() %&gt;%\n  pivot_longer(\n    cols = c(rmse_eRm, rmse_TAM,rmse_mirt,rmse_PAIR),\n    names_to = c(NA, \"Estimator\"),\n    names_sep = \"_\",\n    values_to = \"rmse\"\n  ) %&gt;% \n  group_by(Estimator) %&gt;%\n  summarise(\n    Median = median(rmse, na.rm = T),\n    MAD = mad(rmse, na.rm = T),\n    IQR = IQR(rmse, na.rm = T),\n    Mean = mean(rmse, na.rm = T),\n    SD = sd(rmse, na.rm = T)\n  ) %&gt;%\n  mutate(across(where(is.numeric), ~ round(.x,3))) %&gt;%\n  rename(Package = Estimator) %&gt;%\n  arrange(Median) %&gt;% \n  tt()\n\n \n\n  \n    \n\ntinytable_1005n7a3vvcypthp701e\n\n\n      \n\nPackage\n                Median\n                MAD\n                IQR\n                Mean\n                SD\n              \n\n\nmirt\n                  0.657\n                  0.287\n                  0.394\n                  0.704\n                  0.319\n                \n\neRm \n                  0.658\n                  0.289\n                  0.400\n                  0.708\n                  0.321\n                \n\nTAM \n                  0.668\n                  0.283\n                  0.380\n                  0.709\n                  0.296\n                \n\nPAIR\n                  0.700\n                  0.302\n                  0.419\n                  0.768\n                  0.418\n                \n\n\n\n\n    \n\n\n\n5.1.3 Thresholds summary MAE\n\nCode# table summarizing all thresholds and estimators\nresults_params_4t2 %&gt;%\n  pivot_wider(values_from = c(\"T1\", \"T2\", \"T3\", \"T4\"),\n              names_from = \"Type\",\n              id_cols = c(\"iteration\",\"Item\")\n  ) %&gt;%\n  group_by(iteration,Item) %&gt;%\n  summarise(\n    diff_eRm.T1 = abs(T1_eRm - T1_Input),\n    diff_eRm.T2 = abs(T2_eRm - T2_Input),\n    diff_eRm.T3 = abs(T3_eRm - T3_Input),\n    diff_eRm.T4 = abs(T4_eRm - T4_Input),\n    diff_TAM.T1 = abs(T1_TAM - T1_Input),\n    diff_TAM.T2 = abs(T2_TAM - T2_Input),\n    diff_TAM.T3 = abs(T3_TAM - T3_Input),\n    diff_TAM.T4 = abs(T4_TAM - T4_Input),\n    diff_mirt.T1 = abs(T1_mirt - T1_Input),\n    diff_mirt.T2 = abs(T2_mirt - T2_Input),\n    diff_mirt.T3 = abs(T3_mirt - T3_Input),\n    diff_mirt.T4 = abs(T4_mirt - T4_Input),\n    diff_PAIR.T1 = abs(T1_PAIR - T1_Input),\n    diff_PAIR.T2 = abs(T2_PAIR - T2_Input),\n    diff_PAIR.T3 = abs(T3_PAIR - T3_Input),\n    diff_PAIR.T4 = abs(T4_PAIR - T4_Input)\n  ) %&gt;%\n  ungroup() %&gt;%\n  pivot_longer(cols = starts_with(\"diff\"),\n               names_to = c(\"Estimator\",\"Threshold\"),\n               names_sep = \"\\\\.\",\n               values_to = \"diff\") %&gt;%\n  mutate(Package = gsub(\"diff_\",\"\",Estimator)) %&gt;%\n  group_by(Package, Threshold) %&gt;%\n  summarise(\n    Median = median(diff, na.rm = TRUE),\n    MAD = mad(diff, na.rm = TRUE),\n    IQR = IQR(diff, na.rm = TRUE),\n    Mean = mean(diff, na.rm = TRUE),\n    SD = sd(diff, na.rm = TRUE)\n  ) %&gt;%\n  mutate(across(where(is.numeric), ~ round(.x,3))) %&gt;%\n  arrange(Threshold,Median) %&gt;%\n  tt() %&gt;% \n  style_tt(i = c(1, 5, 9, 13), j = 2, rowspan = 4, alignv = \"t\")\n\n \n\n  \n    \n\ntinytable_777lxuxj5dm3imapdb4u\n\n\n      \n\nPackage\n                Threshold\n                Median\n                MAD\n                IQR\n                Mean\n                SD\n              \n\n\nTAM \n                  T1\n                  0.277\n                  0.240\n                  0.332\n                  0.330\n                  0.259\n                \n\nPAIR\n                  T1\n                  0.286\n                  0.252\n                  0.353\n                  0.357\n                  0.308\n                \n\neRm \n                  T1\n                  0.290\n                  0.258\n                  0.374\n                  0.360\n                  0.303\n                \n\nmirt\n                  T1\n                  0.292\n                  0.261\n                  0.367\n                  0.358\n                  0.299\n                \n\nmirt\n                  T2\n                  0.184\n                  0.162\n                  0.233\n                  0.223\n                  0.177\n                \n\neRm \n                  T2\n                  0.187\n                  0.166\n                  0.231\n                  0.224\n                  0.178\n                \n\nTAM \n                  T2\n                  0.226\n                  0.206\n                  0.290\n                  0.270\n                  0.209\n                \n\nPAIR\n                  T2\n                  0.247\n                  0.214\n                  0.296\n                  0.289\n                  0.229\n                \n\nmirt\n                  T3\n                  0.194\n                  0.171\n                  0.241\n                  0.232\n                  0.179\n                \n\neRm \n                  T3\n                  0.196\n                  0.170\n                  0.245\n                  0.234\n                  0.180\n                \n\nTAM \n                  T3\n                  0.236\n                  0.207\n                  0.290\n                  0.278\n                  0.211\n                \n\nPAIR\n                  T3\n                  0.252\n                  0.220\n                  0.302\n                  0.296\n                  0.239\n                \n\nTAM \n                  T4\n                  0.258\n                  0.234\n                  0.335\n                  0.325\n                  0.264\n                \n\neRm \n                  T4\n                  0.281\n                  0.251\n                  0.349\n                  0.353\n                  0.299\n                \n\nmirt\n                  T4\n                  0.284\n                  0.250\n                  0.352\n                  0.353\n                  0.297\n                \n\nPAIR\n                  T4\n                  0.290\n                  0.262\n                  0.368\n                  0.363\n                  0.360\n                \n\n\n\n\n    \n\n\n\n5.1.4 Figure\n\nCoderesults_params_4t2 %&gt;%\n  pivot_wider(values_from = c(\"T1\", \"T2\", \"T3\", \"T4\"),\n              names_from = \"Type\",\n              id_cols = c(\"iteration\",\"Item\")\n  ) %&gt;%\n  group_by(iteration,Item) %&gt;%\n  summarise(\n    diff_eRm.T1 = T1_eRm - T1_Input,\n    diff_eRm.T2 = T2_eRm - T2_Input,\n    diff_eRm.T3 = T3_eRm - T3_Input,\n    diff_eRm.T4 = T4_eRm - T4_Input,\n    diff_TAM.T1 = T1_TAM - T1_Input,\n    diff_TAM.T2 = T2_TAM - T2_Input,\n    diff_TAM.T3 = T3_TAM - T3_Input,\n    diff_TAM.T4 = T4_TAM - T4_Input,\n    diff_mirt.T1 = T1_mirt - T1_Input,\n    diff_mirt.T2 = T2_mirt - T2_Input,\n    diff_mirt.T3 = T3_mirt - T3_Input,\n    diff_mirt.T4 = T4_mirt - T4_Input,\n    diff_PAIR.T1 = T1_PAIR - T1_Input,\n    diff_PAIR.T2 = T2_PAIR - T2_Input,\n    diff_PAIR.T3 = T3_PAIR - T3_Input,\n    diff_PAIR.T4 = T4_PAIR - T4_Input\n  ) %&gt;%\n  ungroup() %&gt;%\n  pivot_longer(cols = starts_with(\"diff\"),\n               names_to = c(\"Estimator\",\"Threshold\"),\n               names_sep = \"\\\\.\",\n               values_to = \"diff\") %&gt;%\n  mutate(Package = gsub(\"diff_\",\"\",Estimator)) %&gt;%\n  mutate(Threshold = car::recode(Threshold,\"'T1'='Threshold 1';'T2'='Threshold 2';'T3'='Threshold 3';'T4'='Threshold 4';\")) %&gt;%\n  ggplot(aes(x = diff, y = Package, slab_fill = after_stat(level))) +\n  stat_dotsinterval(quantiles = 250, point_interval = \"median_qi\",\n                    layout = \"weave\", slab_color = NA, .width = c(.66,.95)) +\n  labs(caption = str_wrap(\"Point interval: median_qi (.66 and .95). Based on 250 simulated datasets with 9 items and 108 respondents each.\"),\n       x = \"Bias (logit scale)\",\n       y = \"Package\",\n       title = \"Distribution of item threshold estimation bias\",\n       subtitle = \"Rasch Partial Credit Model\") +\n  scale_color_manual(guide = \"none\", values = scales::brewer_pal()(3)[-1], aesthetics = \"slab_fill\") +\n  facet_wrap(~Threshold, nrow = 1) +\n  geom_vline(xintercept = 0, linetype = \"dashed\") +\n  theme_minimal()"
  },
  {
    "objectID": "est_comp.html#references",
    "href": "est_comp.html#references",
    "title": "Comparing Rasch packages/estimators",
    "section": "\n6 References",
    "text": "6 References"
  },
  {
    "objectID": "est_comp.html#even-smaller-sample",
    "href": "est_comp.html#even-smaller-sample",
    "title": "Comparing Rasch packages/estimators",
    "section": "\n6 Even smaller sample",
    "text": "6 Even smaller sample\nn = 54, 2 per threshold estimated.\n\nCoderegisterDoParallel(cores = 8)\niterations = 250\nsamplesize = 54 # n = 2 per threshold (instead of 20/threshold)\n\nsim_results_4t3 &lt;- list()\n\nsim_results_4t3 &lt;- foreach(i = 1:iterations) %dopar% {\n\n  input_params &lt;- sim_data_4t[[i]]$input_params\n  testData &lt;- sim_data_4t[[i]]$data[sample(1:720, samplesize), ]\n\n  # check mean/centrality (item parameters should have been centered before data generation)\n  input_params_correction &lt;- mean(input_params)\n\n  # eRm\n  erm_out &lt;- PCM(testData)\n  erm_params &lt;- thresholds(erm_out)[[3]][[1]][,-1] %&gt;%\n    as.data.frame() %&gt;%\n    set_names(c(\"T1\",\"T2\",\"T3\",\"T4\")) %&gt;%\n    as.matrix()\n\n  erm_params_c &lt;- erm_params - mean(erm_params) + input_params_correction\n\n  # TAM\n  tam_out &lt;- tam(as.matrix(testData), irtmodel = \"PCM\", verbose = FALSE)\n\n  tam_params &lt;- tam_out$item_irt %&gt;%\n    as.data.frame() %&gt;%\n    select(starts_with(\"tau\")) %&gt;%\n    set_names(c(\"T1\",\"T2\",\"T3\",\"T4\")) %&gt;%\n    as.matrix()\n\n  tam_params_c &lt;- tam_params - mean(tam_params) + input_params_correction\n\n  # mirt\n  mirt_out &lt;- mirt(data = testData, model = 1, itemtype = \"Rasch\", verbose = FALSE)\n  mirt_params &lt;- coef(mirt_out, simplify = TRUE, IRTpars = TRUE)$items %&gt;%\n    as.data.frame() %&gt;%\n    select(!a) %&gt;%\n    set_names(c(\"T1\",\"T2\",\"T3\",\"T4\")) %&gt;%\n    as.matrix()\n  mirt_params_c &lt;- mirt_params - mean(mirt_params) + input_params_correction\n\n  # PAIR\n  pair_out &lt;- pair(testData, m = 5)\n  pair_params &lt;- deltapar(pair_out) %&gt;%\n    as.data.frame()\n\n  pair_params &lt;- pair_params[,-1] %&gt;%\n    set_names(c(\"T1\",\"T2\",\"T3\",\"T4\")) %&gt;%\n    as.matrix()\n\n  pair_params_c &lt;- pair_params - mean(pair_params) + input_params_correction\n\n  # combine item parameters and calculate lowest to highest threshold distances\n  input_params %&gt;%\n    as.data.frame() %&gt;%\n    rename(T1 = V1,\n           T2 = V2,\n           T3 = V3,\n           T4 = V4) %&gt;%\n    add_column(Item = c(1:9),\n               Type = \"Input\") %&gt;%\n    bind_rows(\n      erm_params_c %&gt;%\n        as.data.frame() %&gt;%\n        add_column(Item = c(1:9),\n                   Type = \"eRm\")\n    ) %&gt;%\n    bind_rows(\n      tam_params_c %&gt;%\n        as.data.frame() %&gt;%\n        add_column(Item = c(1:9),\n                   Type = \"TAM\")\n    ) %&gt;%\n    bind_rows(\n      mirt_params_c %&gt;%\n        as.data.frame() %&gt;%\n        add_column(Item = c(1:9),\n                   Type = \"mirt\")\n    ) %&gt;%\n    bind_rows(\n      pair_params_c %&gt;%\n        as.data.frame() %&gt;%\n        add_column(Item = c(1:9),\n                   Type = \"PAIR\")\n    ) %&gt;%\n    remove_rownames()\n}\n\n\n\n6.1 Results\n\nCode# collect item threshold parameters to a single dataframe\nresults_params_4t3 &lt;- map_df(1:iterations, ~ sim_results_4t3[[.x]] %&gt;%\n                               mutate(iteration = .x))\n\n\n\n6.1.1 Summary table MAE\n\nCode# calculate absolute differences between input and estimated item thresholds per estimator, average by item\nresults_params_diff_4t3 &lt;- results_params_4t3 %&gt;%\n  pivot_wider(\n    values_from = c(\"T1\", \"T2\", \"T3\", \"T4\"),\n    names_from = \"Type\",\n    id_cols = c(\"iteration\", \"Item\")\n  ) %&gt;%\n  group_by(iteration, Item) %&gt;%\n  summarise(\n    diff_eRm = sum(abs(T1_Input - T1_eRm) + abs(T2_Input - T2_eRm) + abs(T3_Input - T3_eRm) + abs(T4_Input - T4_eRm))/4,\n    diff_TAM = sum(abs(T1_Input - T1_TAM) + abs(T2_Input - T2_TAM) + abs(T3_Input - T3_TAM) + abs(T4_Input - T4_TAM))/4,\n    diff_mirt = sum(abs(T1_Input - T1_mirt) + abs(T2_Input - T2_mirt) + abs(T3_Input - T3_mirt) + abs(T4_Input - T4_mirt))/4,\n    diff_PAIR = sum(abs(T1_Input - T1_PAIR) + abs(T2_Input - T2_PAIR) + abs(T3_Input - T3_PAIR) + abs(T4_Input - T4_PAIR))/4\n  ) %&gt;%\n  ungroup() %&gt;%\n  pivot_longer(\n    cols = c(diff_eRm, diff_TAM,diff_mirt,diff_PAIR),\n    names_to = c(NA, \"Estimator\"),\n    names_sep = \"_\",\n    values_to = \"diff\"\n  )\n\n# produce table with summary stats per estimator/package\nresults_params_diff_4t3 %&gt;%\n  group_by(Estimator) %&gt;%\n  summarise(\n    Median = median(diff, na.rm = TRUE),\n    MAD = mad(diff, na.rm = TRUE),\n    IQR = IQR(diff, na.rm = TRUE),\n    Mean = mean(diff, na.rm = TRUE),\n    SD = sd(diff, na.rm = TRUE),\n  ) %&gt;%\n  mutate(across(where(is.numeric), ~ round(.x,3))) %&gt;%\n  rename(Package = Estimator) %&gt;%\n  arrange(Median) %&gt;%\n  tt()\n\n \n\n  \n    \n\ntinytable_0avvuv15de1s8xf3hktk\n\n\n      \n\nPackage\n                Median\n                MAD\n                IQR\n                Mean\n                SD\n              \n\n\nTAM \n                  0.384\n                  0.167\n                  0.237\n                  0.575\n                  1.471\n                \n\nmirt\n                  0.390\n                  0.170\n                  0.233\n                  0.410\n                  0.177\n                \n\neRm \n                  0.392\n                  0.177\n                  0.237\n                  0.413\n                  0.180\n                \n\nPAIR\n                  0.422\n                  0.207\n                  0.293\n                  0.535\n                  0.470\n                \n\n\n\n\n    \n\n\n\n6.1.2 Summary table RMSE\n\nCode# RMSE\nresults_params_4t3 %&gt;%\n  pivot_wider(\n    values_from = c(\"T1\", \"T2\", \"T3\", \"T4\"),\n    names_from = \"Type\",\n    id_cols = c(\"iteration\", \"Item\")\n  ) %&gt;%\n  group_by(iteration, Item) %&gt;% # sqrt(mean((data$actual - data$predicted)^2))\n  summarise(\n    rmse_eRm = sqrt(mean(((T1_Input - T1_eRm)^2) + ((T2_Input - T2_eRm)^2) + ((T3_Input - T3_eRm)^2) + ((T4_Input - T4_eRm))^2)),\n        rmse_TAM = sqrt(mean(((T1_Input - T1_TAM)^2) + ((T2_Input - T2_TAM)^2) + ((T3_Input - T3_TAM)^2) + ((T4_Input - T4_TAM))^2)),\n        rmse_mirt = sqrt(mean(((T1_Input - T1_mirt)^2) + ((T2_Input - T2_mirt)^2) + ((T3_Input - T3_mirt)^2) + ((T4_Input - T4_mirt))^2)),\n        rmse_PAIR = sqrt(mean(((T1_Input - T1_PAIR)^2) + ((T2_Input - T2_PAIR)^2) + ((T3_Input - T3_PAIR)^2) + ((T4_Input - T4_PAIR))^2))\n  ) %&gt;%\n  ungroup() %&gt;%\n  pivot_longer(\n    cols = c(rmse_eRm, rmse_TAM,rmse_mirt,rmse_PAIR),\n    names_to = c(NA, \"Estimator\"),\n    names_sep = \"_\",\n    values_to = \"rmse\"\n  ) %&gt;% \n  group_by(Estimator) %&gt;%\n  summarise(\n    Median = median(rmse, na.rm = T),\n    MAD = mad(rmse, na.rm = T),\n    IQR = IQR(rmse, na.rm = T),\n    Mean = mean(rmse, na.rm = T),\n    SD = sd(rmse, na.rm = T)\n  ) %&gt;%\n  mutate(across(where(is.numeric), ~ round(.x,3))) %&gt;%\n  rename(Package = Estimator) %&gt;%\n  arrange(Median) %&gt;% \n  tt()\n\n \n\n  \n    \n\ntinytable_zmjrgt9crv6rrmgkpnid\n\n\n      \n\nPackage\n                Median\n                MAD\n                IQR\n                Mean\n                SD\n              \n\n\nTAM \n                  0.902\n                  0.370\n                  0.507\n                  1.342\n                  3.381\n                \n\nmirt\n                  0.925\n                  0.400\n                  0.543\n                  0.987\n                  0.430\n                \n\neRm \n                  0.937\n                  0.405\n                  0.550\n                  0.994\n                  0.439\n                \n\nPAIR\n                  1.009\n                  0.468\n                  0.655\n                  1.250\n                  1.078\n                \n\n\n\n\n    \n\n\n\n6.1.3 Thresholds summary MAE\n\nCode# table summarizing all thresholds and estimators\nresults_params_4t3 %&gt;%\n  pivot_wider(values_from = c(\"T1\", \"T2\", \"T3\", \"T4\"),\n              names_from = \"Type\",\n              id_cols = c(\"iteration\",\"Item\")\n  ) %&gt;%\n  group_by(iteration,Item) %&gt;%\n  summarise(\n    diff_eRm.T1 = abs(T1_eRm - T1_Input),\n    diff_eRm.T2 = abs(T2_eRm - T2_Input),\n    diff_eRm.T3 = abs(T3_eRm - T3_Input),\n    diff_eRm.T4 = abs(T4_eRm - T4_Input),\n    diff_TAM.T1 = abs(T1_TAM - T1_Input),\n    diff_TAM.T2 = abs(T2_TAM - T2_Input),\n    diff_TAM.T3 = abs(T3_TAM - T3_Input),\n    diff_TAM.T4 = abs(T4_TAM - T4_Input),\n    diff_mirt.T1 = abs(T1_mirt - T1_Input),\n    diff_mirt.T2 = abs(T2_mirt - T2_Input),\n    diff_mirt.T3 = abs(T3_mirt - T3_Input),\n    diff_mirt.T4 = abs(T4_mirt - T4_Input),\n    diff_PAIR.T1 = abs(T1_PAIR - T1_Input),\n    diff_PAIR.T2 = abs(T2_PAIR - T2_Input),\n    diff_PAIR.T3 = abs(T3_PAIR - T3_Input),\n    diff_PAIR.T4 = abs(T4_PAIR - T4_Input)\n  ) %&gt;%\n  ungroup() %&gt;%\n  pivot_longer(cols = starts_with(\"diff\"),\n               names_to = c(\"Estimator\",\"Threshold\"),\n               names_sep = \"\\\\.\",\n               values_to = \"diff\") %&gt;%\n  mutate(Package = gsub(\"diff_\",\"\",Estimator)) %&gt;%\n  group_by(Package, Threshold) %&gt;%\n  summarise(\n    Median = median(diff, na.rm = TRUE),\n    MAD = mad(diff, na.rm = TRUE),\n    IQR = IQR(diff, na.rm = TRUE),\n    Mean = mean(diff, na.rm = TRUE),\n    SD = sd(diff, na.rm = TRUE)\n  ) %&gt;%\n  mutate(across(where(is.numeric), ~ round(.x,3))) %&gt;%\n  arrange(Threshold,Median) %&gt;%\n  tt() %&gt;% \n  style_tt(i = c(1, 5, 9, 13), j = 2, rowspan = 4, alignv = \"t\")\n\n \n\n  \n    \n\ntinytable_4111b9318frvetbjqouw\n\n\n      \n\nPackage\n                Threshold\n                Median\n                MAD\n                IQR\n                Mean\n                SD\n              \n\n\nTAM \n                  T1\n                  0.386\n                  0.343\n                  0.486\n                  0.794\n                  2.957\n                \n\nmirt\n                  T1\n                  0.407\n                  0.366\n                  0.512\n                  0.508\n                  0.417\n                \n\neRm \n                  T1\n                  0.412\n                  0.368\n                  0.524\n                  0.514\n                  0.425\n                \n\nPAIR\n                  T1\n                  0.433\n                  0.399\n                  0.575\n                  0.619\n                  0.749\n                \n\nmirt\n                  T2\n                  0.272\n                  0.239\n                  0.332\n                  0.319\n                  0.249\n                \n\neRm \n                  T2\n                  0.274\n                  0.246\n                  0.339\n                  0.322\n                  0.251\n                \n\nTAM \n                  T2\n                  0.312\n                  0.279\n                  0.398\n                  0.479\n                  1.013\n                \n\nPAIR\n                  T2\n                  0.353\n                  0.315\n                  0.448\n                  0.451\n                  0.404\n                \n\nmirt\n                  T3\n                  0.266\n                  0.237\n                  0.336\n                  0.320\n                  0.249\n                \n\neRm \n                  T3\n                  0.270\n                  0.235\n                  0.329\n                  0.322\n                  0.250\n                \n\nTAM \n                  T3\n                  0.302\n                  0.271\n                  0.394\n                  0.473\n                  1.011\n                \n\nPAIR\n                  T3\n                  0.342\n                  0.309\n                  0.431\n                  0.446\n                  0.409\n                \n\nTAM \n                  T4\n                  0.367\n                  0.339\n                  0.474\n                  0.555\n                  1.025\n                \n\neRm \n                  T4\n                  0.412\n                  0.361\n                  0.503\n                  0.496\n                  0.396\n                \n\nmirt\n                  T4\n                  0.416\n                  0.367\n                  0.507\n                  0.494\n                  0.392\n                \n\nPAIR\n                  T4\n                  0.425\n                  0.403\n                  0.590\n                  0.626\n                  0.809\n                \n\n\n\n\n    \n\n\n\n6.1.4 Figure\n\nCoderesults_params_4t3 %&gt;%\n  pivot_wider(values_from = c(\"T1\", \"T2\", \"T3\", \"T4\"),\n              names_from = \"Type\",\n              id_cols = c(\"iteration\",\"Item\")\n  ) %&gt;%\n  group_by(iteration,Item) %&gt;%\n  summarise(\n    diff_eRm.T1 = T1_eRm - T1_Input,\n    diff_eRm.T2 = T2_eRm - T2_Input,\n    diff_eRm.T3 = T3_eRm - T3_Input,\n    diff_eRm.T4 = T4_eRm - T4_Input,\n    diff_TAM.T1 = T1_TAM - T1_Input,\n    diff_TAM.T2 = T2_TAM - T2_Input,\n    diff_TAM.T3 = T3_TAM - T3_Input,\n    diff_TAM.T4 = T4_TAM - T4_Input,\n    diff_mirt.T1 = T1_mirt - T1_Input,\n    diff_mirt.T2 = T2_mirt - T2_Input,\n    diff_mirt.T3 = T3_mirt - T3_Input,\n    diff_mirt.T4 = T4_mirt - T4_Input,\n    diff_PAIR.T1 = T1_PAIR - T1_Input,\n    diff_PAIR.T2 = T2_PAIR - T2_Input,\n    diff_PAIR.T3 = T3_PAIR - T3_Input,\n    diff_PAIR.T4 = T4_PAIR - T4_Input\n  ) %&gt;%\n  ungroup() %&gt;%\n  pivot_longer(cols = starts_with(\"diff\"),\n               names_to = c(\"Estimator\",\"Threshold\"),\n               names_sep = \"\\\\.\",\n               values_to = \"diff\") %&gt;%\n  mutate(Package = gsub(\"diff_\",\"\",Estimator)) %&gt;%\n  mutate(Threshold = car::recode(Threshold,\"'T1'='Threshold 1';'T2'='Threshold 2';'T3'='Threshold 3';'T4'='Threshold 4';\")) %&gt;%\n  ggplot(aes(x = diff, y = Package, slab_fill = after_stat(level))) +\n  stat_dotsinterval(quantiles = 250, point_interval = \"median_qi\",\n                    layout = \"weave\", slab_color = NA, .width = c(.66,.95)) +\n  labs(caption = str_wrap(\"Point interval: median_qi (.66 and .95). Based on 250 simulated datasets with 9 items and 54 respondents each. X-axis omits extreme values.\"),\n       x = \"Bias (logit scale)\",\n       y = \"Package\",\n       title = \"Distribution of item threshold estimation bias\",\n       subtitle = \"Rasch Partial Credit Model\") +\n  scale_color_manual(guide = \"none\", values = scales::brewer_pal()(3)[-1], aesthetics = \"slab_fill\") +\n  facet_wrap(~Threshold, nrow = 1) +\n  geom_vline(xintercept = 0, linetype = \"dashed\") +\n  theme_minimal() +\n  coord_cartesian(xlim = c(-2.5,2.5))\n\n\n\n\n\n\n\n\nCodesessionInfo()\n\nR version 4.4.1 (2024-06-14)\nPlatform: aarch64-apple-darwin20\nRunning under: macOS Sonoma 14.6.1\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\ntime zone: Europe/Stockholm\ntzcode source: internal\n\nattached base packages:\n[1] parallel  stats4    stats     graphics  grDevices utils     datasets \n[8] methods   base     \n\nother attached packages:\n [1] tinytable_0.3.0   doParallel_1.0.17 iterators_1.0.14  foreach_1.5.2    \n [5] ggdist_3.3.2      pairwise_0.6.1-0  mirt_1.42         lattice_0.22-6   \n [9] TAM_4.2-21        CDM_8.2-6         mvtnorm_1.2-5     janitor_2.2.0    \n[13] eRm_1.0-6         lubridate_1.9.3   forcats_1.0.0     stringr_1.5.1    \n[17] dplyr_1.1.4       purrr_1.0.2       readr_2.1.5       tidyr_1.3.1      \n[21] tibble_3.2.1      ggplot2_3.5.1     tidyverse_2.0.0  \n\nloaded via a namespace (and not attached):\n [1] mnormt_2.1.1         pbapply_1.7-2        gridExtra_2.3       \n [4] permute_0.9-7        testthat_3.2.1.1     rlang_1.1.4         \n [7] magrittr_2.0.3       snakecase_0.11.1     compiler_4.4.1      \n[10] mgcv_1.9-1           vctrs_0.6.5          quadprog_1.5-8      \n[13] pkgconfig_2.0.3      fastmap_1.2.0        labeling_0.4.3      \n[16] utf8_1.2.4           rmarkdown_2.27       sessioninfo_1.2.2   \n[19] tzdb_0.4.0           xfun_0.46            jsonlite_1.8.8      \n[22] Deriv_4.1.3          psych_2.4.6.26       cluster_2.1.6       \n[25] R6_2.5.1             stringi_1.8.4        RColorBrewer_1.1-3  \n[28] parallelly_1.38.0    car_3.1-2            brio_1.1.5          \n[31] Rcpp_1.0.13          knitr_1.48           future.apply_1.11.2 \n[34] snow_0.4-4           audio_0.1-11         R.utils_2.12.3      \n[37] polycor_0.8-1        Matrix_1.7-0         splines_4.4.1       \n[40] timechange_0.3.0     tidyselect_1.2.1     rstudioapi_0.16.0   \n[43] abind_1.4-5          yaml_2.3.10          vegan_2.6-6.1       \n[46] codetools_0.2-20     dcurver_0.9.2        curl_5.2.1          \n[49] admisc_0.35          listenv_0.9.1        withr_3.0.1         \n[52] evaluate_0.24.0      future_1.34.0        pillar_1.9.0        \n[55] carData_3.0-5        distributional_0.4.0 generics_0.1.3      \n[58] hms_1.1.3            munsell_0.5.1        scales_1.3.0        \n[61] globals_0.16.3       glue_1.7.0           RPushbullet_0.3.4   \n[64] tools_4.4.1          beepr_2.0            SimDesign_2.17.1    \n[67] grid_4.4.1           colorspace_2.1-1     nlme_3.1-165        \n[70] cli_3.6.3            fansi_1.0.6          gtable_0.3.5        \n[73] R.methodsS3_1.8.2    digest_0.6.36        progressr_0.14.0    \n[76] GPArotation_2024.3-1 htmlwidgets_1.6.4    farver_2.1.2        \n[79] htmltools_0.5.8.1    R.oo_1.26.0          lifecycle_1.0.4     \n[82] MASS_7.3-61"
  },
  {
    "objectID": "item_restscore.html",
    "href": "item_restscore.html",
    "title": "Rasch item-restscore and sample size",
    "section": "",
    "text": "Codelibrary(iarm)\nlibrary(ggdist)\nlibrary(tidyverse)\nlibrary(RISEkbmRasch)\n### some commands exist in multiple packages, here we define preferred ones that are frequently used\nselect &lt;- dplyr::select\ncount &lt;- dplyr::count\nrename &lt;- dplyr::rename"
  },
  {
    "objectID": "item_restscore.html#background",
    "href": "item_restscore.html#background",
    "title": "Rasch item-restscore and sample size",
    "section": "\n1 Background",
    "text": "1 Background\nIt seems to me that the iarm::item_restscore() function is sensitive to sample size, which is not surprising as it provides a p-value. The questions of interest to me are:\n\nhow important is that p-value in itself?\nwhat is the connection between the difference between observed and expected score and the p-value? (magnitude and significance, if you will)\nhow is item-restscore related to the bootstrapped p-values from the iarm::boot_fit() function (which calculates conditional item fit values)?\nhow is simulation based cutoff values for conditional infit/outfit related to the two above?\n\nI will use the Rasch Partial Credit Model (PCM) for some explorations of this topic.\n\n1.1 Example\nFirst, we’ll review results from the dataset pcmdat2 included in the eRm package.\n\nCodeRItileplot(pcmdat2)\n\n\n\n\n\n\n\n\nCodeRItargeting(pcmdat2)\n\n\n\n\n\n\n\n300 respondents, 4 items with 4 response categories each.\n\nCodetestfit &lt;- function(data, boot = 100, sfit = 500) {\n  \n  i1 &lt;- item_restscore(PCM(data))\n  i1 &lt;- as.data.frame(i1)\n  \n  i2 &lt;- data.frame(\"observed\" = as.numeric(i1[[1]][1:ncol(data),1]),\n                   \"expected\" = as.numeric(i1[[1]][1:ncol(data),2]),\n                   \"se\" = as.numeric(i1[[1]][1:ncol(data),3]),\n                   \"p.value\" = as.numeric(i1[[1]][1:ncol(data),4]),\n                   \"p.adj.BH\" = as.numeric(i1[[1]][1:ncol(data),5])\n  ) %&gt;% \n    mutate(diff_abs = abs(expected - observed),\n           diff = expected - observed)\n  ifit &lt;- out_infit(PCM(data))\n  bfit &lt;- boot_fit(PCM(data),boot)\n  \n  sfit &lt;- RIgetfit(data,sfit,8)\n  \n  out &lt;- list(\"item_restscore\" = i2,\n              \"out_infit\" = ifit,\n              \"boot_fit\" = bfit,\n              \"sfit\" = sfit)\n  return(out)\n}\n\ntest1 &lt;- testfit(pcmdat2)\n\n\n\n Number of bootstrap samples:  10, 20, 30, 40, 50, 60, 70, 80, 90, 100, \n \n\nCodetest1$item_restscore\n\n  observed expected     se p.value p.adj.BH diff_abs    diff\n1   0.4729   0.5243 0.0530  0.3329   0.4439   0.0514  0.0514\n2   0.4414   0.5244 0.0593  0.1620   0.3240   0.0830  0.0830\n3   0.6427   0.4849 0.0541  0.0035   0.0140   0.1578 -0.1578\n4   0.5351   0.4956 0.0599  0.5098   0.5098   0.0395 -0.0395\n\nCodetest1$boot_fit\n\n\n   Outfit pvalue padj  sig   Infit pvalue padj  sig   \nI1 1.069  0.36   0.48        1.08  0.136  0.271       \nI2 1.084  0.186  0.298       1.155 0      0     ***   \nI3 0.802  0.019  0.05   .    0.828 0      0     ***   \nI4 1.024  0.814  0.93        1.007 0.955  0.955       \n\nP value adjustment: BH\n\nCodeRIitemfit(pcmdat2,test1$sfit)\n\n\n\n Item \n    InfitMSQ \n    Infit thresholds \n    OutfitMSQ \n    Outfit thresholds \n    Infit diff \n    Outfit diff \n  \n\n\n I1 \n    1.08 \n    [0.861, 1.156] \n    1.069 \n    [0.839, 1.212] \n    no misfit \n    no misfit \n  \n\n I2 \n    1.155 \n    [0.839, 1.202] \n    1.084 \n    [0.813, 1.32] \n    no misfit \n    no misfit \n  \n\n I3 \n    0.828 \n    [0.861, 1.157] \n    0.802 \n    [0.812, 1.226] \n    0.033 \n    0.01 \n  \n\n I4 \n    1.007 \n    [0.836, 1.183] \n    1.024 \n    [0.814, 1.23] \n    no misfit \n    no misfit \n  \n\n\nNote: \n\n MSQ values based on conditional calculations (n = 300 complete cases).                                Simulation based thresholds from 500 simulated datasets.\n\n\n\n\nWe have a significant item-restscore deviation for item 3 (higher than expected), and the low infit/outfit is significant when using bootstrapped p-values (100 iterations). The simulation based cutoff values for infit/outfit agree (500 iterations).\nAlso, note that item 3 has the worst targeting (see ?@fig-targeting1)\n\n1.2 Reduced sample\nOur sample size is 300. What happens if we reduce that to 100?\n\nCodeset.seed(1324)\nd100 &lt;- pcmdat2[sample(1:300, 100), ]\n\ntest2 &lt;- testfit(d100)\n\n\n\n Number of bootstrap samples:  10, 20, 30, 40, 50, 60, 70, 80, 90, 100, \n \n\nCodetest2$item_restscore\n\n  observed expected     se p.value p.adj.BH diff_abs    diff\n1   0.4805   0.4989 0.0915  0.8407   0.9772   0.0184  0.0184\n2   0.4541   0.5061 0.1016  0.6084   0.9772   0.0520  0.0520\n3   0.6168   0.4551 0.1117  0.1477   0.5909   0.1617 -0.1617\n4   0.4763   0.4730 0.1150  0.9772   0.9772   0.0033 -0.0033\n\nCodetest2$boot_fit\n\n\n   Outfit pvalue padj  sig  Infit pvalue padj  sig \nI1 1.039  0.65   0.698      1.024 0.698  0.698     \nI2 1.042  0.658  0.698      1.071 0.675  0.698     \nI3 0.911  0.404  0.698      0.869 0.12   0.698     \nI4 1.039  0.66   0.698      1.045 0.554  0.698     \n\nP value adjustment: BH\n\nCodeRIitemfit(d100,test2$sfit)\n\n\n\n Item \n    InfitMSQ \n    Infit thresholds \n    OutfitMSQ \n    Outfit thresholds \n    Infit diff \n    Outfit diff \n  \n\n\n I1 \n    1.024 \n    [0.744, 1.288] \n    1.039 \n    [0.726, 1.609] \n    no misfit \n    no misfit \n  \n\n I2 \n    1.071 \n    [0.742, 1.305] \n    1.042 \n    [0.728, 1.647] \n    no misfit \n    no misfit \n  \n\n I3 \n    0.869 \n    [0.763, 1.269] \n    0.911 \n    [0.67, 1.456] \n    no misfit \n    no misfit \n  \n\n I4 \n    1.045 \n    [0.76, 1.31] \n    1.039 \n    [0.692, 1.439] \n    no misfit \n    no misfit \n  \n\n\nNote: \n\n MSQ values based on conditional calculations (n = 100 complete cases).                                Simulation based thresholds from 486 simulated datasets.\n\n\n\n\nEverything now looks ok. This could of course be due to random sampling. Let’s use the sample size of 100 and do 1000 iterations of random samples to see how many show misfit for item 3. We should really use a simulated dataset with induced (known) misfit, but let’s do that later."
  },
  {
    "objectID": "item_restscore.html#non-parametric-bootstrapping",
    "href": "item_restscore.html#non-parametric-bootstrapping",
    "title": "Rasch item-restscore and sample size",
    "section": "\n2 Non-parametric bootstrapping",
    "text": "2 Non-parametric bootstrapping\nFirst, we build a function to run multiple samples.\n\nCodetestfit2 &lt;- function(dat, boot = 100, sfit = 500, iterations = 1:10) {\n  \n  fit &lt;- list()\n  fit &lt;- foreach(iterations) %do% {\n    \n    data &lt;- dat[sample(1:nrow(dat), 100), ]\n    \n    i1 &lt;- item_restscore(PCM(data))\n    i1 &lt;- as.data.frame(i1)\n    \n    i2 &lt;- data.frame(\"observed\" = as.numeric(i1[[1]][1:ncol(data),1]),\n                     \"expected\" = as.numeric(i1[[1]][1:ncol(data),2]),\n                     \"se\" = as.numeric(i1[[1]][1:ncol(data),3]),\n                     \"p.value\" = as.numeric(i1[[1]][1:ncol(data),4]),\n                     \"p.adj.BH\" = as.numeric(i1[[1]][1:ncol(data),5])\n    ) %&gt;% \n      mutate(diff_abs = abs(expected - observed),\n             diff = expected - observed,\n             ir_padj = ifelse(p.adj.BH &lt; .05, \"sign. misfit\",\"no misfit\")) %&gt;% \n      select(ir_padj, diff, diff_abs) %&gt;% \n      mutate(item = 1:ncol(data))\n    \n    #ifit &lt;- out_infit(PCM(data))\n    bfit &lt;- boot_fit(PCM(data),boot)\n    bfit &lt;- bfit[[1]] %&gt;% \n      as.data.frame() %&gt;% \n      mutate(bfit_sig = case_when(out.pkorr &lt; .05 ~ \"sign. outfit\",\n                             in.pkorr &lt; .05 ~ \"sign. infit\",\n                             out.pkorr &lt; .05 & in.pkorr &lt; .05 ~ \"sign. inoutfit\",\n                             .default = \"no misfit\")) %&gt;% \n      mutate(item = 1:ncol(data)) %&gt;% \n      select(item,bfit_sig,Outfit,Infit)\n    \n    simfit &lt;- RIgetfit(data,sfit,8)\n    rfit &lt;- RIitemfit(data,simfit,output = \"dataframe\") %&gt;% \n      select(infit_diff,outfit_diff) %&gt;% \n      mutate(item = 1:ncol(data))\n    \n    list(\"item_restscore\" = i2,\n         #\"out_infit\" = ifit,\n         \"boot_fit\" = bfit,\n         \"rfit\" = rfit)\n  }\n  return(fit)\n}\n\n\nThen we run it for 1000 iterations.\n\nCode#test3 &lt;- testfit2(pcmdat2, iterations = 1:1000)\n#saveRDS(test3,\"irscore.Rdata\")\ntest3 &lt;- readRDS(\"irscore.Rdata\")\n\n\n\n2.1 Results\nJoin results to a dataframe.\n\nCodeiterations = 1:1000\nresult_irscore &lt;- map_dfr(iterations, ~ bind_rows(test3[[.x]]$item_restscore) %&gt;% \n          add_column(iteration = .x))\n\nresult_bfit &lt;- map_dfr(iterations, ~ bind_rows(test3[[.x]]$boot_fit) %&gt;% \n          add_column(iteration = .x))\n\nresult_sfit &lt;- map_dfr(iterations, ~ bind_rows(test3[[.x]]$rfit) %&gt;% \n          add_column(iteration = .x))\n\nresults &lt;- left_join(result_irscore,result_bfit, by = c(\"iteration\",\"item\"))\nresults &lt;- left_join(results, result_sfit, by = c(\"iteration\",\"item\"))\nhead(results, 24)\n\n     ir_padj    diff diff_abs item iteration     bfit_sig    Outfit     Infit\n1  no misfit  0.0727   0.0727    1         1    no misfit 1.0659711 1.0993426\n2  no misfit  0.0186   0.0186    2         1    no misfit 0.9658042 1.0194994\n3  no misfit -0.0587   0.0587    3         1    no misfit 1.0207190 0.9454184\n4  no misfit -0.0941   0.0941    4         1    no misfit 1.0042747 0.9572694\n5  no misfit -0.0027   0.0027    1         2    no misfit 0.9899937 0.9862542\n6  no misfit  0.0375   0.0375    2         2    no misfit 1.0107473 1.0574265\n7  no misfit -0.1745   0.1745    3         2  sign. infit 0.7756829 0.7640317\n8  no misfit  0.0772   0.0772    4         2 sign. outfit 1.2900957 1.2411078\n9  no misfit  0.0340   0.0340    1         3    no misfit 1.0615916 1.0620552\n10 no misfit  0.1110   0.1110    2         3    no misfit 1.1499652 1.1798024\n11 no misfit -0.2230   0.2230    3         3    no misfit 0.7856128 0.8097686\n12 no misfit -0.0700   0.0700    4         3    no misfit 1.0055846 0.9904388\n13 no misfit  0.1459   0.1459    1         4    no misfit 1.2733994 1.2708954\n14 no misfit  0.0486   0.0486    2         4    no misfit 1.0097530 1.0603228\n15 no misfit -0.1482   0.1482    3         4    no misfit 0.8384259 0.8156593\n16 no misfit -0.0927   0.0927    4         4    no misfit 0.8229410 0.8741012\n17 no misfit  0.0175   0.0175    1         5    no misfit 1.0121368 1.0234842\n18 no misfit  0.0861   0.0861    2         5    no misfit 1.1212221 1.1665047\n19 no misfit -0.1118   0.1118    3         5    no misfit 0.8405235 0.8587897\n20 no misfit -0.0295   0.0295    4         5    no misfit 1.0095068 0.9915725\n21 no misfit  0.0320   0.0320    1         6    no misfit 1.0593372 1.0527971\n22 no misfit  0.1783   0.1783    2         6  sign. infit 1.2267309 1.3885530\n23 no misfit -0.1901   0.1901    3         6    no misfit 0.8225946 0.8096652\n24 no misfit -0.1424   0.1424    4         6    no misfit 0.8847511 0.8817594\n   infit_diff outfit_diff\n1   no misfit   no misfit\n2   no misfit   no misfit\n3   no misfit   no misfit\n4   no misfit   no misfit\n5   no misfit   no misfit\n6   no misfit   no misfit\n7   no misfit   no misfit\n8   no misfit   no misfit\n9   no misfit   no misfit\n10  no misfit   no misfit\n11  no misfit   no misfit\n12  no misfit   no misfit\n13      0.013   no misfit\n14  no misfit   no misfit\n15  no misfit   no misfit\n16  no misfit   no misfit\n17  no misfit   no misfit\n18  no misfit   no misfit\n19  no misfit   no misfit\n20  no misfit   no misfit\n21  no misfit   no misfit\n22      0.083   no misfit\n23  no misfit   no misfit\n24  no misfit   no misfit\n\n\nOk, we have some results to summarise. First, we filter on item-restscore misfit and look at the first 25 rows.\n\nCoderesults %&gt;% \n  filter(ir_padj == \"sign. misfit\") %&gt;% \n  head(25)\n\n        ir_padj    diff diff_abs item iteration     bfit_sig    Outfit\n1  sign. misfit -0.1797   0.1797    3        10 sign. outfit 0.6402285\n2  sign. misfit -0.2376   0.2376    3        18    no misfit 0.7747958\n3  sign. misfit -0.2552   0.2552    3        20 sign. outfit 0.6827816\n4  sign. misfit -0.2531   0.2531    3        31 sign. outfit 0.6621439\n5  sign. misfit -0.2729   0.2729    3        38 sign. outfit 0.6762267\n6  sign. misfit -0.2129   0.2129    4        41 sign. outfit 0.6905030\n7  sign. misfit -0.2334   0.2334    3        49 sign. outfit 0.6526281\n8  sign. misfit -0.1905   0.1905    3        56 sign. outfit 0.6704844\n9  sign. misfit -0.2583   0.2583    3        59 sign. outfit 0.7076849\n10 sign. misfit -0.2280   0.2280    3        64 sign. outfit 0.6629790\n11 sign. misfit -0.2323   0.2323    3        70  sign. infit 0.7532934\n12 sign. misfit -0.2511   0.2511    3        71 sign. outfit 0.7174040\n13 sign. misfit -0.2065   0.2065    3        76 sign. outfit 0.6904129\n14 sign. misfit -0.2055   0.2055    3        84  sign. infit 0.7523724\n15 sign. misfit -0.2479   0.2479    3        87    no misfit 0.6721512\n16 sign. misfit -0.2206   0.2206    3        89 sign. outfit 0.6917995\n17 sign. misfit -0.2220   0.2220    3        90    no misfit 0.6786376\n18 sign. misfit -0.2364   0.2364    3        97 sign. outfit 0.5883898\n19 sign. misfit -0.2585   0.2585    3       101 sign. outfit 0.6865158\n20 sign. misfit -0.2799   0.2799    3       103 sign. outfit 0.6043981\n21 sign. misfit -0.2521   0.2521    3       108 sign. outfit 0.7258624\n22 sign. misfit -0.2284   0.2284    3       109 sign. outfit 0.6607257\n23 sign. misfit -0.2850   0.2850    3       113 sign. outfit 0.6595454\n24 sign. misfit -0.2197   0.2197    3       123 sign. outfit 0.6570153\n25 sign. misfit -0.2744   0.2744    3       124 sign. outfit 0.6481838\n       Infit infit_diff outfit_diff\n1  0.7305199  no misfit   no misfit\n2  0.7693903      0.004   no misfit\n3  0.7218550      0.043       0.032\n4  0.7247758      0.028       0.022\n5  0.7155371      0.015   no misfit\n6  0.7658634  no misfit   no misfit\n7  0.7104056      0.045   no misfit\n8  0.7227110      0.022   no misfit\n9  0.7764644       0.01   no misfit\n10 0.7339133      0.038       0.013\n11 0.7558214      0.013   no misfit\n12 0.7302366      0.026   no misfit\n13 0.7466734      0.007   no misfit\n14 0.7191440       0.03   no misfit\n15 0.7524506      0.001   no misfit\n16 0.7353687      0.057   no misfit\n17 0.7417101  no misfit   no misfit\n18 0.6870658       0.04       0.033\n19 0.7523964      0.005       0.014\n20 0.6582644      0.071       0.074\n21 0.7784688  no misfit   no misfit\n22 0.7416230      0.016       0.028\n23 0.6928638      0.067   no misfit\n24 0.7170398      0.011   no misfit\n25 0.7348365      0.038   no misfit\n\nCoderesults %&gt;% \n  filter(ir_padj == \"sign. misfit\") %&gt;% \n  count(item)\n\n  item   n\n1    3 200\n2    4   1\n\nCodeir &lt;- results %&gt;% \n  filter(ir_padj == \"sign. misfit\") %&gt;% \n  nrow()\n\nbf &lt;- results %&gt;% \n  filter(ir_padj == \"sign. misfit\") %&gt;% \n  filter(bfit_sig == \"no misfit\") %&gt;% \n  nrow()\n\nbfout &lt;- results %&gt;% \n  filter(ir_padj == \"sign. misfit\") %&gt;% \n  filter(bfit_sig == \"sign. outfit\") %&gt;% \n  nrow()\n\nsimfitin &lt;- results %&gt;% \n  filter(ir_padj == \"sign. misfit\") %&gt;% \n  filter(!infit_diff == \"no misfit\") %&gt;% \n  nrow()\nsimfitout &lt;- results %&gt;% \n  filter(ir_padj == \"sign. misfit\") %&gt;% \n  filter(!outfit_diff == \"no misfit\") %&gt;% \n  nrow()\nsimfitboth &lt;- results %&gt;% \n  filter(ir_padj == \"sign. misfit\") %&gt;% \n  filter(!outfit_diff == \"no misfit\" | !infit_diff == \"no misfit\") %&gt;% \n  nrow()\n\n\nItem 3 misfit almost exclusively, except for one instance of item 4. Item-restscore shows misfit in 201 (out of 1000) datasets, 20.1%.\nBootstrapped (using 100 bootstraps per dataset) item infit/outfit p-values also indicate significant misfit amongst the datasets identified by item-restscore, and agrees with item-restscore all but 24 cases.\nSimulation based cutoffs using 500 iterations (per dataset) indicate less misfit in this sample of item-restscore misfits. 149 out of 201, of which 142 were flagged for infit, and 70 for outfit.\n\n2.1.1 Item-restscore\nLet’s review the observed/expected differences.\n\nCoderesults %&gt;% \n  filter(ir_padj == \"sign. misfit\") %&gt;% \n  summarise(Median = median(diff_abs),\n            MAD = mad(diff_abs),\n            Mean = mean(diff_abs),\n            SD = sd(diff_abs),\n            Max = max(diff_abs),\n            Min = min(diff_abs)) %&gt;% \n  round(3) %&gt;% \n  knitr::kable()\n\n\n\nMedian\nMAD\nMean\nSD\nMax\nMin\n\n\n0.232\n0.031\n0.235\n0.031\n0.348\n0.167\n\n\n\nCoderesults %&gt;% \n  filter(ir_padj == \"sign. misfit\") %&gt;% \n  ggplot() +\n  stat_dotsinterval(aes(x = diff_abs)) +\n  scale_x_continuous('Distribution of absolut differences', limits = c(0,0.4)) +\n  theme_minimal() +\n  labs(title = \"Observed vs expected item-restscore values\")\n\n\n\n\n\n\n\nThe results above were filtered on only item-restscore, so there may be other results for the other methods.\n\n2.1.2 Bootfit\n\nCodebfo &lt;- results %&gt;% \n  filter(bfit_sig == \"sign. outfit\") %&gt;% \n  nrow()\n\nresults %&gt;% \n  filter(bfit_sig == \"sign. outfit\") %&gt;% \n  count(item) %&gt;% \n  knitr::kable(caption = \"Items with statistically significant outfit\")\n\n\nItems with statistically significant outfit\n\nitem\nn\n\n\n\n1\n24\n\n\n2\n9\n\n\n3\n269\n\n\n4\n35\n\n\n\n\nCodebfi &lt;- results %&gt;% \n  filter(bfit_sig == \"sign. infit\") %&gt;% \n  nrow()\n\nresults %&gt;% \n  filter(bfit_sig == \"sign. infit\") %&gt;% \n  count(item) %&gt;% \n  knitr::kable(caption = \"Items with statistically significant infit\")\n\n\nItems with statistically significant infit\n\nitem\nn\n\n\n\n1\n21\n\n\n2\n100\n\n\n3\n56\n\n\n4\n1\n\n\n\n\nCodebfboth &lt;- results %&gt;% \n  filter(bfit_sig == \"sign. infit\" | bfit_sig == \"sign. outfit\") %&gt;% \n  nrow()\n\nresults %&gt;% \n  filter(bfit_sig == \"sign. infit\" | bfit_sig == \"sign. outfit\") %&gt;% \n  count(item) %&gt;% \n  knitr::kable(caption = \"Items with statistically significant infit and/or outfit\")\n\n\nItems with statistically significant infit and/or outfit\n\nitem\nn\n\n\n\n1\n45\n\n\n2\n109\n\n\n3\n325\n\n\n4\n36\n\n\n\n\n\n337 datasets with significant outfit and 178 with significant infit.\n\n2.1.3 Simulation based cutoff\n\nCodesfo &lt;- results %&gt;% \n  filter(!outfit_diff == \"no misfit\") %&gt;% \n  nrow()\n\nresults %&gt;% \n  filter(!outfit_diff == \"no misfit\") %&gt;% \n  count(item) %&gt;% \n  knitr::kable(caption = \"Items with deviant outfit\")\n\n\nItems with deviant outfit\n\nitem\nn\n\n\n\n3\n75\n\n\n4\n3\n\n\n\n\nCodesfi &lt;- results %&gt;% \n  filter(!infit_diff == \"no misfit\") %&gt;% \n  nrow()\n\nresults %&gt;% \n  filter(!infit_diff == \"no misfit\") %&gt;% \n  count(item) %&gt;% \n  knitr::kable(caption = \"Items with deviant infit\")\n\n\nItems with deviant infit\n\nitem\nn\n\n\n\n1\n15\n\n\n2\n71\n\n\n3\n169\n\n\n4\n7\n\n\n\n\nCodesfboth &lt;- results %&gt;% \n  filter(!infit_diff == \"no misfit\" | !outfit_diff == \"no misfit\") %&gt;% \n  nrow()\n\nresults %&gt;% \n  filter(!infit_diff == \"no misfit\" | !outfit_diff == \"no misfit\") %&gt;% \n  count(item) %&gt;% \n  knitr::kable(caption = \"Items with deviant infit and/or outfit\")\n\n\nItems with deviant infit and/or outfit\n\nitem\nn\n\n\n\n1\n15\n\n\n2\n71\n\n\n3\n178\n\n\n4\n9\n\n\n\n\n\n78 datasets with significant outfit and 262 with significant infit, resulting in 273 with either type of misfit.\nWe can review the distribution of infit/outfit for misfitting items.\n\n2.1.4 Infit\n\nCoderesults %&gt;% \n  filter(ir_padj == \"sign. misfit\") %&gt;% \n  summarise(Median = median(Infit),\n            MAD = mad(Infit),\n            Mean = mean(Infit),\n            SD = sd(Infit),\n            Max = max(Infit),\n            Min = min(Infit)) %&gt;% \n  round(3) %&gt;% \n  knitr::kable()\n\n\n\nMedian\nMAD\nMean\nSD\nMax\nMin\n\n\n0.733\n0.031\n0.729\n0.034\n0.807\n0.606\n\n\n\nCoderesults %&gt;% \n  filter(ir_padj == \"sign. misfit\") %&gt;% \n  ggplot() +\n  stat_dotsinterval(aes(x = Infit)) +\n  scale_x_continuous('Distribution conditional infit MSQ', limits = c(0.5,1)) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n2.1.5 Outfit\n\nCoderesults %&gt;% \n  filter(ir_padj == \"sign. misfit\") %&gt;% \n  summarise(Median = median(Outfit),\n            MAD = mad(Outfit),\n            Mean = mean(Outfit),\n            SD = sd(Outfit),\n            Max = max(Outfit),\n            Min = min(Outfit)) %&gt;% \n  round(3) %&gt;% \n  knitr::kable()\n\n\n\nMedian\nMAD\nMean\nSD\nMax\nMin\n\n\n0.667\n0.053\n0.667\n0.053\n0.815\n0.53\n\n\n\nCoderesults %&gt;% \n  filter(ir_padj == \"sign. misfit\") %&gt;% \n  ggplot() +\n  stat_dotsinterval(aes(x = Outfit)) +\n  scale_x_continuous('Distribution conditional outfit MSQ', limits = c(0.5,1)) +\n  theme_minimal()"
  },
  {
    "objectID": "item_restscore.html#test-2",
    "href": "item_restscore.html#test-2",
    "title": "Rasch item-restscore and sample size",
    "section": "\n3 Test 2",
    "text": "3 Test 2\nWhat happens if we increase the number of iterations for simulation based cutoffs? We’ve been running at 500 to shorten runtime, but could go to 1000 iteration while cutting the number of datasets down from 1000 to 500. We’ll also try using 50 bootstrap samples for bootfit() and see how that affects its p-values.\n\nCode#test4 &lt;- testfit2(pcmdat2, iterations = 1:500, sfit = 1000, boot = 50)\n#saveRDS(test4,\"irscore500.Rdata\")\ntest4 &lt;- readRDS(\"irscore500.Rdata\")\n\n\n\n3.1 Results\nJoin results to a dataframe.\n\nCodeiterations = 1:500\nresult_irscore &lt;- map_dfr(iterations, ~ bind_rows(test4[[.x]]$item_restscore) %&gt;% \n          add_column(iteration = .x))\n\nresult_bfit &lt;- map_dfr(iterations, ~ bind_rows(test4[[.x]]$boot_fit) %&gt;% \n          add_column(iteration = .x))\n\nresult_sfit &lt;- map_dfr(iterations, ~ bind_rows(test4[[.x]]$rfit) %&gt;% \n          add_column(iteration = .x))\n\nresults &lt;- left_join(result_irscore,result_bfit, by = c(\"iteration\",\"item\"))\nresults &lt;- left_join(results, result_sfit, by = c(\"iteration\",\"item\"))\nhead(results, 24)\n\n     ir_padj    diff diff_abs item iteration    bfit_sig    Outfit     Infit\n1  no misfit  0.0278   0.0278    1         1   no misfit 1.0454139 1.0724946\n2  no misfit  0.0860   0.0860    2         1   no misfit 1.0653470 1.2179965\n3  no misfit -0.1556   0.1556    3         1   no misfit 0.8271545 0.8247410\n4  no misfit -0.0592   0.0592    4         1   no misfit 1.0316122 1.0171284\n5  no misfit  0.0264   0.0264    1         2   no misfit 1.0371496 1.0423821\n6  no misfit  0.0763   0.0763    2         2   no misfit 1.0761663 1.1804809\n7  no misfit -0.1992   0.1992    3         2 sign. infit 0.7431027 0.7771497\n8  no misfit -0.0060   0.0060    4         2   no misfit 1.0988844 1.1447383\n9  no misfit -0.0187   0.0187    1         3   no misfit 0.9555410 0.9643910\n10 no misfit  0.0529   0.0529    2         3   no misfit 1.0483272 1.0789874\n11 no misfit -0.2161   0.2161    3         3 sign. infit 0.8060479 0.8360092\n12 no misfit  0.0570   0.0570    4         3   no misfit 1.2451834 1.1846969\n13 no misfit -0.0162   0.0162    1         4   no misfit 0.9707772 0.9731070\n14 no misfit  0.1582   0.1582    2         4   no misfit 1.2051081 1.2579444\n15 no misfit -0.1891   0.1891    3         4   no misfit 0.8434983 0.8546639\n16 no misfit -0.0680   0.0680    4         4   no misfit 1.0116630 0.9913493\n17 no misfit  0.0658   0.0658    1         5   no misfit 1.0719629 1.0935663\n18 no misfit  0.0971   0.0971    2         5   no misfit 1.0809464 1.1970012\n19 no misfit -0.1619   0.1619    3         5   no misfit 0.7307903 0.8012734\n20 no misfit -0.0592   0.0592    4         5   no misfit 1.0086764 0.9944696\n21 no misfit -0.0626   0.0626    1         6   no misfit 0.9247730 0.9184895\n22 no misfit  0.1088   0.1088    2         6   no misfit 1.0813182 1.1484476\n23 no misfit -0.1263   0.1263    3         6   no misfit 0.9227389 0.9410586\n24 no misfit -0.0176   0.0176    4         6   no misfit 1.0888125 1.0363839\n   infit_diff outfit_diff\n1   no misfit   no misfit\n2   no misfit   no misfit\n3   no misfit   no misfit\n4   no misfit   no misfit\n5   no misfit   no misfit\n6   no misfit   no misfit\n7   no misfit   no misfit\n8   no misfit   no misfit\n9   no misfit   no misfit\n10  no misfit   no misfit\n11  no misfit   no misfit\n12  no misfit   no misfit\n13  no misfit   no misfit\n14  no misfit   no misfit\n15  no misfit   no misfit\n16  no misfit   no misfit\n17  no misfit   no misfit\n18  no misfit   no misfit\n19  no misfit   no misfit\n20  no misfit   no misfit\n21  no misfit   no misfit\n22  no misfit   no misfit\n23  no misfit   no misfit\n24  no misfit   no misfit\n\n\nOk, we have some results to summarise. First, we filter on item-restscore misfit and look at the first 25 rows.\n\nCoderesults %&gt;% \n  filter(ir_padj == \"sign. misfit\") %&gt;% \n  head(25)\n\n        ir_padj    diff diff_abs item iteration     bfit_sig    Outfit\n1  sign. misfit -0.2633   0.2633    3         9 sign. outfit 0.6975253\n2  sign. misfit -0.2529   0.2529    3        19 sign. outfit 0.7050639\n3  sign. misfit -0.2867   0.2867    3        27 sign. outfit 0.5386553\n4  sign. misfit -0.1996   0.1996    3        38 sign. outfit 0.5642884\n5  sign. misfit -0.2330   0.2330    3        40 sign. outfit 0.6318898\n6  sign. misfit -0.2387   0.2387    3        44 sign. outfit 0.6268911\n7  sign. misfit -0.1951   0.1951    3        46 sign. outfit 0.6470683\n8  sign. misfit -0.2359   0.2359    3        49    no misfit 0.6304596\n9  sign. misfit -0.3082   0.3082    3        52    no misfit 0.7689529\n10 sign. misfit -0.1964   0.1964    3        58 sign. outfit 0.6660527\n11 sign. misfit -0.2999   0.2999    3        59 sign. outfit 0.6162333\n12 sign. misfit -0.2393   0.2393    3        62 sign. outfit 0.6830782\n13 sign. misfit -0.2269   0.2269    3        64 sign. outfit 0.7086075\n14 sign. misfit -0.2317   0.2317    3        69 sign. outfit 0.6322802\n15 sign. misfit -0.1926   0.1926    3        78 sign. outfit 0.6864711\n16 sign. misfit -0.2397   0.2397    3        87 sign. outfit 0.6133337\n17 sign. misfit -0.2174   0.2174    3        99 sign. outfit 0.7383997\n18 sign. misfit -0.2289   0.2289    3       105 sign. outfit 0.6724876\n19 sign. misfit -0.2477   0.2477    3       108 sign. outfit 0.6693367\n20 sign. misfit -0.1854   0.1854    3       115  sign. infit 0.7460685\n21 sign. misfit -0.2615   0.2615    3       126 sign. outfit 0.6203168\n22 sign. misfit -0.2008   0.2008    3       136 sign. outfit 0.6706957\n23 sign. misfit -0.2123   0.2123    3       138 sign. outfit 0.6372636\n24 sign. misfit -0.2315   0.2315    3       141 sign. outfit 0.6989589\n25 sign. misfit -0.2407   0.2407    3       156 sign. outfit 0.6788173\n       Infit infit_diff outfit_diff\n1  0.7617014      0.019   no misfit\n2  0.7860327  no misfit   no misfit\n3  0.6320351      0.082       0.104\n4  0.6866108      0.046       0.002\n5  0.7016374      0.023   no misfit\n6  0.7338684      0.007   no misfit\n7  0.7340116  no misfit   no misfit\n8  0.7332496      0.011   no misfit\n9  0.8015139  no misfit   no misfit\n10 0.7259577      0.035   no misfit\n11 0.6753428      0.083       0.018\n12 0.7662194  no misfit   no misfit\n13 0.7685445  no misfit   no misfit\n14 0.6826949      0.059       0.031\n15 0.7642318  no misfit   no misfit\n16 0.6657616      0.071       0.015\n17 0.7664598  no misfit   no misfit\n18 0.7114820      0.044   no misfit\n19 0.7466069      0.014       0.019\n20 0.7465324  no misfit   no misfit\n21 0.6809506      0.075       0.036\n22 0.7295246      0.023       0.022\n23 0.7533044  no misfit   no misfit\n24 0.7590361  no misfit   no misfit\n25 0.7417980      0.016   no misfit\n\nCoderesults %&gt;% \n  filter(ir_padj == \"sign. misfit\") %&gt;% \n  count(item) %&gt;% \n  mutate(Percent = n*100/500) %&gt;% \n  knitr::kable()\n\n\n\nitem\nn\nPercent\n\n\n3\n76\n15.2\n\n\n\nCodeir &lt;- results %&gt;% \n  filter(ir_padj == \"sign. misfit\") %&gt;% \n  nrow()\n\nbf &lt;- results %&gt;% \n  filter(ir_padj == \"sign. misfit\") %&gt;% \n  filter(bfit_sig == \"no misfit\") %&gt;% \n  nrow()\n\nbfout &lt;- results %&gt;% \n  filter(ir_padj == \"sign. misfit\") %&gt;% \n  filter(bfit_sig == \"sign. outfit\") %&gt;% \n  nrow()\n\nsimfitin &lt;- results %&gt;% \n  filter(ir_padj == \"sign. misfit\") %&gt;% \n  filter(!infit_diff == \"no misfit\") %&gt;% \n  nrow()\nsimfitout &lt;- results %&gt;% \n  filter(ir_padj == \"sign. misfit\") %&gt;% \n  filter(!outfit_diff == \"no misfit\") %&gt;% \n  nrow()\nsimfitboth &lt;- results %&gt;% \n  filter(ir_padj == \"sign. misfit\") %&gt;% \n  filter(!outfit_diff == \"no misfit\" | !infit_diff == \"no misfit\") %&gt;% \n  nrow()\n\n\nItem 3 misfit almost exclusively, except for one instance of item 4. Item-restscore shows misfit in 76 (out of 1000) datasets, 15.2%.\nBootstrapped (using 100 bootstraps per dataset) item infit/outfit p-values also indicate significant misfit amongst the datasets identified by item-restscore, and agrees with item-restscore all but 24 cases.\nSimulation based cutoffs using 500 iterations (per dataset) indicate less misfit in this sample of item-restscore misfits. 55 out of 76, of which 54 were flagged for infit, and 25 for outfit.\n\n3.1.1 Item-restscore\nLet’s review the observed/expected differences.\n\nCoderesults %&gt;% \n  filter(ir_padj == \"sign. misfit\") %&gt;% \n  count(item) %&gt;% \n  mutate(Percent = n*100/500) %&gt;% \n  knitr::kable()\n\n\n\nitem\nn\nPercent\n\n\n3\n76\n15.2\n\n\n\nCoderesults %&gt;% \n  filter(ir_padj == \"sign. misfit\") %&gt;% \n  summarise(Median = median(diff_abs),\n            MAD = mad(diff_abs),\n            Mean = mean(diff_abs),\n            SD = sd(diff_abs),\n            Max = max(diff_abs),\n            Min = min(diff_abs)) %&gt;% \n  round(3) %&gt;% \n  knitr::kable()\n\n\n\nMedian\nMAD\nMean\nSD\nMax\nMin\n\n\n0.238\n0.035\n0.236\n0.033\n0.34\n0.176\n\n\n\nCoderesults %&gt;% \n  filter(ir_padj == \"sign. misfit\") %&gt;% \n  ggplot() +\n  stat_dotsinterval(aes(x = diff_abs)) +\n  scale_x_continuous('Distribution of absolut differences', limits = c(0,0.4)) +\n  theme_minimal() +\n  labs(title = \"Observed vs expected item-restscore values\")\n\n\n\n\n\n\n\nThe results above were filtered on only item-restscore, so there may be other results for the other methods.\n\n3.1.2 Bootfit\n\nCodebfo &lt;- results %&gt;% \n  filter(bfit_sig == \"sign. outfit\") %&gt;% \n  nrow()\n\nresults %&gt;% \n  filter(bfit_sig == \"sign. outfit\") %&gt;% \n  count(item) %&gt;% \n  mutate(Percent = n*100/500) %&gt;% \n  knitr::kable(caption = \"Items with statistically significant outfit\")\n\n\nItems with statistically significant outfit\n\nitem\nn\nPercent\n\n\n\n1\n12\n2.4\n\n\n2\n11\n2.2\n\n\n3\n157\n31.4\n\n\n4\n26\n5.2\n\n\n\n\nCodebfi &lt;- results %&gt;% \n  filter(bfit_sig == \"sign. infit\") %&gt;% \n  nrow()\n\nresults %&gt;% \n  filter(bfit_sig == \"sign. infit\") %&gt;% \n  count(item) %&gt;% \n  mutate(Percent = n*100/500) %&gt;% \n  knitr::kable(caption = \"Items with statistically significant infit\")\n\n\nItems with statistically significant infit\n\nitem\nn\nPercent\n\n\n\n1\n16\n3.2\n\n\n2\n78\n15.6\n\n\n3\n40\n8.0\n\n\n4\n7\n1.4\n\n\n\n\nCodebfboth &lt;- results %&gt;% \n  filter(bfit_sig == \"sign. infit\" | bfit_sig == \"sign. outfit\") %&gt;% \n  nrow()\n\nresults %&gt;% \n  filter(bfit_sig == \"sign. infit\" | bfit_sig == \"sign. outfit\") %&gt;% \n  count(item) %&gt;%\n  mutate(Percent = n*100/500) %&gt;% \n  knitr::kable(caption = \"Items with statistically significant infit and/or outfit\")\n\n\nItems with statistically significant infit and/or outfit\n\nitem\nn\nPercent\n\n\n\n1\n28\n5.6\n\n\n2\n89\n17.8\n\n\n3\n197\n39.4\n\n\n4\n33\n6.6\n\n\n\n\n\n206 datasets with significant outfit and 141 with significant infit.\n\n3.1.3 Simulation based cutoff\n\nCodesfo &lt;- results %&gt;% \n  filter(!outfit_diff == \"no misfit\") %&gt;% \n  nrow()\n\nresults %&gt;% \n  filter(!outfit_diff == \"no misfit\") %&gt;% \n  count(item) %&gt;% \n  mutate(Percent = n*100/500) %&gt;% \n  knitr::kable(caption = \"Items with deviant outfit\")\n\n\nItems with deviant outfit\n\nitem\nn\nPercent\n\n\n\n3\n26\n5.2\n\n\n4\n1\n0.2\n\n\n\n\nCodesfi &lt;- results %&gt;% \n  filter(!infit_diff == \"no misfit\") %&gt;% \n  nrow()\n\nresults %&gt;% \n  filter(!infit_diff == \"no misfit\") %&gt;% \n  count(item) %&gt;% \n  mutate(Percent = n*100/500) %&gt;% \n  knitr::kable(caption = \"Items with deviant infit\")\n\n\nItems with deviant infit\n\nitem\nn\nPercent\n\n\n\n1\n4\n0.8\n\n\n2\n33\n6.6\n\n\n3\n63\n12.6\n\n\n4\n1\n0.2\n\n\n\n\nCodesfboth &lt;- results %&gt;% \n  filter(!infit_diff == \"no misfit\" | !outfit_diff == \"no misfit\") %&gt;% \n  nrow()\n\nresults %&gt;% \n  filter(!infit_diff == \"no misfit\" | !outfit_diff == \"no misfit\") %&gt;% \n  count(item) %&gt;% \n  mutate(Percent = n*100/500) %&gt;% \n  knitr::kable(caption = \"Items with deviant infit and/or outfit\")\n\n\nItems with deviant infit and/or outfit\n\nitem\nn\nPercent\n\n\n\n1\n4\n0.8\n\n\n2\n33\n6.6\n\n\n3\n64\n12.8\n\n\n4\n2\n0.4\n\n\n\n\n\n27 datasets with significant outfit and 101 with significant infit, resulting in 103 with either type of misfit.\nWe can review the distribution of infit/outfit for misfitting items.\n\n3.1.4 Infit\n\nCoderesults %&gt;% \n  filter(ir_padj == \"sign. misfit\") %&gt;% \n  summarise(Median = median(Infit),\n            MAD = mad(Infit),\n            Mean = mean(Infit),\n            SD = sd(Infit),\n            Max = max(Infit),\n            Min = min(Infit)) %&gt;% \n  round(3) %&gt;% \n  knitr::kable()\n\n\n\nMedian\nMAD\nMean\nSD\nMax\nMin\n\n\n0.734\n0.035\n0.729\n0.035\n0.808\n0.632\n\n\n\nCoderesults %&gt;% \n  filter(ir_padj == \"sign. misfit\") %&gt;% \n  ggplot() +\n  stat_dotsinterval(aes(x = Infit)) +\n  scale_x_continuous('Distribution conditional infit MSQ', limits = c(0.5,1)) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n3.1.5 Outfit\n\nCoderesults %&gt;% \n  filter(ir_padj == \"sign. misfit\") %&gt;% \n  summarise(Median = median(Outfit),\n            MAD = mad(Outfit),\n            Mean = mean(Outfit),\n            SD = sd(Outfit),\n            Max = max(Outfit),\n            Min = min(Outfit)) %&gt;% \n  round(3) %&gt;% \n  knitr::kable()\n\n\n\nMedian\nMAD\nMean\nSD\nMax\nMin\n\n\n0.665\n0.048\n0.665\n0.05\n0.798\n0.539\n\n\n\nCoderesults %&gt;% \n  filter(ir_padj == \"sign. misfit\") %&gt;% \n  ggplot() +\n  stat_dotsinterval(aes(x = Outfit)) +\n  scale_x_continuous('Distribution conditional outfit MSQ', limits = c(0.5,1)) +\n  theme_minimal()"
  },
  {
    "objectID": "item_restscore.html#item-restscore-sample-size-variation",
    "href": "item_restscore.html#item-restscore-sample-size-variation",
    "title": "Rasch item-restscore and sample size",
    "section": "\n4 Item-restscore sample size variation",
    "text": "4 Item-restscore sample size variation\nLet us look only at item-restscore and see how sample size affects the percentage of significant deviations indicated for item 3. We’ll do 50-250 in steps of 25.\n\nCodetestfit3 &lt;- function(dat, iterations, samplesize, cpu = 8) {\n  \n  require(doParallel)\n  registerDoParallel(cores = cpu)\n  \n  fit &lt;- data.frame()\n  fit &lt;- foreach(i = 1:iterations, .combine = rbind) %dopar% {\n    data &lt;- dat[sample(1:nrow(dat), samplesize), ]\n    \n    i1 &lt;- item_restscore(PCM(data))\n    i1 &lt;- as.data.frame(i1)\n    \n    data.frame(\"observed\" = as.numeric(i1[[1]][1:ncol(data),1]),\n                     \"expected\" = as.numeric(i1[[1]][1:ncol(data),2]),\n                     \"se\" = as.numeric(i1[[1]][1:ncol(data),3]),\n                     \"p.value\" = as.numeric(i1[[1]][1:ncol(data),4]),\n                     \"p.adj.BH\" = as.numeric(i1[[1]][1:ncol(data),5])\n    ) %&gt;% \n      mutate(diff_abs = abs(expected - observed),\n             diff = expected - observed,\n             ir_padj = ifelse(p.adj.BH &lt; .05, \"sign. misfit\",\"no misfit\")) %&gt;% \n      select(ir_padj, diff, diff_abs) %&gt;% \n      mutate(item = 1:ncol(data)) %&gt;% \n      add_column(iteration = i,\n                 samplesize = samplesize)\n  }\n  return(fit)\n}\n\n\n\nCode#irscoreonly &lt;- list()\n#irscoreonly &lt;- map(samplesizes, ~ testfit3(pcmdat2, iterations = 1000, samplesize = .x))\n#saveRDS(irscoreonly,\"item_restscore1000.Rdata\")\nirscoreonly &lt;- readRDS(\"item_restscore1000.Rdata\")\n\n\n\nCoderesults_ir &lt;- bind_rows(irscoreonly) %&gt;% \n  filter(item == 3) %&gt;% \n  filter(ir_padj == \"sign. misfit\")\n\n\n\n4.1 Results\n\nCoderesults_ir %&gt;% \n  group_by(samplesize) %&gt;% \n  summarise(n = n(),\n            Percent = n*100/1000) %&gt;% \n  ggplot(aes(x = samplesize, y = Percent)) +\n  geom_point() +\n  geom_line() +\n  coord_cartesian(ylim = c(0,100)) +\n  scale_y_continuous(breaks = seq(0,100,20)) +\n  theme_minimal() +\n  labs(y = \"Percentage of datasets with indicated misfit for item 3\",\n       x = \"Sample size\",\n       caption = \"Note: 1000 non-parametric bootstrap samples for each sample size\",\n       title = \"Item-restscore\",\n       subtitle = \"Sample size affects identification of misfitting item?\")\n\n\n\n\n\n\n\n\nCodep01 = round(quantile(results_ir$diff_abs, .01),3)\np05 = round(quantile(results_ir$diff_abs, .05),3)\n\nresults_ir %&gt;% \n  ggplot() +\n  stat_dotsinterval(aes(x = diff_abs)) +\n  scale_x_continuous('Distribution of absolute differences', limits = c(0,0.4), breaks = seq(0,0.4,0.05)) +\n  theme_rise() +\n  labs(title = \"Observed vs expected item-restscore values\",\n       subtitle = \"Only statistically significant values included in plot\",\n       y = \"\",\n       caption = paste0(\"Note: 1st percentile of the absolute difference between observed and expected item-restscore is \",p01,\". 5th percentile is \",p05,\".\"))\n\n\n\n\n\n\n\nNow, let’s dig up a simulation function that allows us to specify a misfitting item."
  },
  {
    "objectID": "dif_magnitude.html",
    "href": "dif_magnitude.html",
    "title": "Rasch DIF magnitude & item split",
    "section": "",
    "text": "Out of interest of better understanding som aspects of differential item functioning (DIF, a form of invariance test):\n\nhow much does DIF affect estimated thetas (factor scores)?\nhow to do item-split in R (creating separate items for subgroups from one item with problematic DIF)\nhow an item-split compares to removing the item (and keeping the DIF item) in terms of absolute differences in estimated thetas\n\nWe’ll use simulated data in order to have knowledge of the thetas used to generate response data (“input thetas” in the text below), and make objective comparisons using the different estimated thetas.\nIdeally, this would be a simulation study where we create lots of datasets with a systematic variation in some parameters to investigate effects. But maybe this is a first step towards that.\n\nCodelibrary(tidyverse)\nlibrary(eRm)\nlibrary(iarm)\nlibrary(mirt)\nlibrary(catR)\nlibrary(easyRasch) # devtools::install_github(\"pgmj/easyRasch\", dependencies = TRUE)\nlibrary(summarytools)\nlibrary(ghibli)\nlibrary(broom)\n\n### some commands exist in multiple packages, here we define preferred ones that are frequently used\nselect &lt;- dplyr::select\ncount &lt;- dplyr::count\nrecode &lt;- car::recode\nrename &lt;- dplyr::rename\n\ntheme_rise &lt;- function(fontfamily = \"Lato\", axissize = 13, titlesize = 15,\n                       margins = 12, axisface = \"plain\", panelDist = 0.6, ...) {\n  theme_minimal() +\n  theme(\n    text = element_text(family = fontfamily),\n    axis.title.x = element_text(\n      margin = margin(t = margins),\n      size = axissize\n    ),\n    axis.title.y = element_text(\n      margin = margin(r = margins),\n      size = axissize\n    ),\n    plot.title = element_text(\n      face = \"bold\",\n      size = titlesize\n    ),\n    axis.title = element_text(\n      face = axisface\n    ),\n    plot.caption = element_text(\n      face = \"italic\"\n    ),\n    legend.text = element_text(family = fontfamily),\n    legend.background = element_rect(color = \"lightgrey\"),\n    strip.background = element_rect(color = \"lightgrey\"),\n    panel.spacing = unit(panelDist, \"cm\", data = NULL),\n    panel.border = element_rect(color = \"grey\", fill = NA),\n    ...\n  )\n}\n\ntheme_set(theme_rise())\n\n\n\nFirst define input parameters for items.\n\nCode# make a tibble/dataframe also, for possible later use\ninputParams1 &lt;- tibble(\n  q1  = c(1.2, 1.8, 2.4),\n  q2  = c(-1.3, -0.5, 0.5),\n  q3a = c(-0.3, 0.3, 1.2), # this is the DIF item\n  q3b = c(-0.3+1, 0.3+1, 1.2+1), # this is the DIF item\n  q4  = c(0.1, 0.6, 1.6),\n  q5  = c(-0.3, 0.7, 1.5),\n  q6  = c(-1.6, -1, -0.3),\n  q7  = c(1, 1.8, 2.5),\n  q8  = c(-1.3, -0.7, 0.4),\n  q9  = c(-0.8, 1.4, 1.9),\n  q10 = c(0.25, 1.25, 2.15)\n) %&gt;%\n  t() %&gt;%\n  as.matrix()\n\n# center to 0\ninputParams1c &lt;- inputParams1 - mean(inputParams1)\n\n# item list for simulation for group 1\ntlist1 &lt;- list(\n  q1 =  list(inputParams1c[1,]),\n  q2 =  list(inputParams1c[2,]),\n  q3 =  list(inputParams1c[3,]), # this is the DIF item\n  q4 =  list(inputParams1c[5,]),\n  q5 =  list(inputParams1c[6,]),\n  q6 =  list(inputParams1c[7,]),\n  q7 =  list(inputParams1c[8,]),\n  q8 =  list(inputParams1c[9,]),\n  q9 =  list(inputParams1c[10,]),\n  q10 = list(inputParams1c[11,])\n)\n\n# item list for simulation for group 2\ntlist2 &lt;- list(\n  q1 =  list(inputParams1c[1,]),\n  q2 =  list(inputParams1c[2,]),\n  q3 =  list(inputParams1c[4,]), # this is the DIF item\n  q4 =  list(inputParams1c[5,]),\n  q5 =  list(inputParams1c[6,]),\n  q6 =  list(inputParams1c[7,]),\n  q7 =  list(inputParams1c[8,]),\n  q8 =  list(inputParams1c[9,]),\n  q9 =  list(inputParams1c[10,]),\n  q10 = list(inputParams1c[11,])\n)\n\n\nThen generate random thetas that we save to file to be able to reproduce the analysis.\n\nCode# simulate thetas\nthetas1 &lt;- rnorm(300, mean = 0, sd = 1.5)\nthetas2 &lt;- rnorm(300, mean = 0, sd = 1.5)\n\ninput_thetas &lt;- c(thetas1,thetas2)\n\n# simulate response data based on the above defined item thresholds\ntd1 &lt;- SimPartialScore(\n  deltaslist = tlist1,\n  thetavec = thetas1\n) %&gt;%\n  as.data.frame()\n\ntd2 &lt;- SimPartialScore(\n  deltaslist = tlist2,\n  thetavec = thetas2\n) %&gt;%\n  as.data.frame()\n\nd &lt;- rbind(td1,td2) %&gt;% \n  add_column(group = rep(1:2, each = 300))\n\ndif.group &lt;- factor(d$group)\nd$group &lt;- NULL\n\nall_data &lt;- list(simResponses = d,\n                 dif_group = dif.group,\n                 input_thetas = input_thetas)\n# save simulated data for reproducibility\n#saveRDS(all_data,\"dif_magnitude_1_0.Rdata\")\n\n\n\nCode# read simulated data for reprodubility\nall_data &lt;- readRDS(\"dif_magnitude_1_0.Rdata\")\n\nd &lt;- all_data$simResponses\ndif.group &lt;- all_data$dif_group\ninput_thetas &lt;- all_data$input_thetas\n\n\nWe now have 10 items with 4 categories each. There are 600 respondents in all, with 300 showing differential item functioning for one item (item q3). DIF is induced at +1 logit uniform difference in location (all thresholds for item q3 are unformly +1 logits)."
  },
  {
    "objectID": "dif_magnitude.html#background",
    "href": "dif_magnitude.html#background",
    "title": "Rasch DIF magnitude & item split",
    "section": "",
    "text": "Out of interest of better understanding som aspects of differential item functioning (DIF, a form of invariance test):\n\nhow much does DIF affect estimated thetas (factor scores)?\nhow to do item-split in R (creating separate items for subgroups from one item with problematic DIF)\nhow an item-split compares to removing the item (and keeping the DIF item) in terms of absolute differences in estimated thetas\n\nWe’ll use simulated data in order to have knowledge of the thetas used to generate response data (“input thetas” in the text below), and make objective comparisons using the different estimated thetas.\nIdeally, this would be a simulation study where we create lots of datasets with a systematic variation in some parameters to investigate effects. But maybe this is a first step towards that.\n\nCodelibrary(tidyverse)\nlibrary(eRm)\nlibrary(iarm)\nlibrary(mirt)\nlibrary(catR)\nlibrary(easyRasch) # devtools::install_github(\"pgmj/easyRasch\", dependencies = TRUE)\nlibrary(summarytools)\nlibrary(ghibli)\nlibrary(broom)\n\n### some commands exist in multiple packages, here we define preferred ones that are frequently used\nselect &lt;- dplyr::select\ncount &lt;- dplyr::count\nrecode &lt;- car::recode\nrename &lt;- dplyr::rename\n\ntheme_rise &lt;- function(fontfamily = \"Lato\", axissize = 13, titlesize = 15,\n                       margins = 12, axisface = \"plain\", panelDist = 0.6, ...) {\n  theme_minimal() +\n  theme(\n    text = element_text(family = fontfamily),\n    axis.title.x = element_text(\n      margin = margin(t = margins),\n      size = axissize\n    ),\n    axis.title.y = element_text(\n      margin = margin(r = margins),\n      size = axissize\n    ),\n    plot.title = element_text(\n      face = \"bold\",\n      size = titlesize\n    ),\n    axis.title = element_text(\n      face = axisface\n    ),\n    plot.caption = element_text(\n      face = \"italic\"\n    ),\n    legend.text = element_text(family = fontfamily),\n    legend.background = element_rect(color = \"lightgrey\"),\n    strip.background = element_rect(color = \"lightgrey\"),\n    panel.spacing = unit(panelDist, \"cm\", data = NULL),\n    panel.border = element_rect(color = \"grey\", fill = NA),\n    ...\n  )\n}\n\ntheme_set(theme_rise())\n\n\n\nFirst define input parameters for items.\n\nCode# make a tibble/dataframe also, for possible later use\ninputParams1 &lt;- tibble(\n  q1  = c(1.2, 1.8, 2.4),\n  q2  = c(-1.3, -0.5, 0.5),\n  q3a = c(-0.3, 0.3, 1.2), # this is the DIF item\n  q3b = c(-0.3+1, 0.3+1, 1.2+1), # this is the DIF item\n  q4  = c(0.1, 0.6, 1.6),\n  q5  = c(-0.3, 0.7, 1.5),\n  q6  = c(-1.6, -1, -0.3),\n  q7  = c(1, 1.8, 2.5),\n  q8  = c(-1.3, -0.7, 0.4),\n  q9  = c(-0.8, 1.4, 1.9),\n  q10 = c(0.25, 1.25, 2.15)\n) %&gt;%\n  t() %&gt;%\n  as.matrix()\n\n# center to 0\ninputParams1c &lt;- inputParams1 - mean(inputParams1)\n\n# item list for simulation for group 1\ntlist1 &lt;- list(\n  q1 =  list(inputParams1c[1,]),\n  q2 =  list(inputParams1c[2,]),\n  q3 =  list(inputParams1c[3,]), # this is the DIF item\n  q4 =  list(inputParams1c[5,]),\n  q5 =  list(inputParams1c[6,]),\n  q6 =  list(inputParams1c[7,]),\n  q7 =  list(inputParams1c[8,]),\n  q8 =  list(inputParams1c[9,]),\n  q9 =  list(inputParams1c[10,]),\n  q10 = list(inputParams1c[11,])\n)\n\n# item list for simulation for group 2\ntlist2 &lt;- list(\n  q1 =  list(inputParams1c[1,]),\n  q2 =  list(inputParams1c[2,]),\n  q3 =  list(inputParams1c[4,]), # this is the DIF item\n  q4 =  list(inputParams1c[5,]),\n  q5 =  list(inputParams1c[6,]),\n  q6 =  list(inputParams1c[7,]),\n  q7 =  list(inputParams1c[8,]),\n  q8 =  list(inputParams1c[9,]),\n  q9 =  list(inputParams1c[10,]),\n  q10 = list(inputParams1c[11,])\n)\n\n\nThen generate random thetas that we save to file to be able to reproduce the analysis.\n\nCode# simulate thetas\nthetas1 &lt;- rnorm(300, mean = 0, sd = 1.5)\nthetas2 &lt;- rnorm(300, mean = 0, sd = 1.5)\n\ninput_thetas &lt;- c(thetas1,thetas2)\n\n# simulate response data based on the above defined item thresholds\ntd1 &lt;- SimPartialScore(\n  deltaslist = tlist1,\n  thetavec = thetas1\n) %&gt;%\n  as.data.frame()\n\ntd2 &lt;- SimPartialScore(\n  deltaslist = tlist2,\n  thetavec = thetas2\n) %&gt;%\n  as.data.frame()\n\nd &lt;- rbind(td1,td2) %&gt;% \n  add_column(group = rep(1:2, each = 300))\n\ndif.group &lt;- factor(d$group)\nd$group &lt;- NULL\n\nall_data &lt;- list(simResponses = d,\n                 dif_group = dif.group,\n                 input_thetas = input_thetas)\n# save simulated data for reproducibility\n#saveRDS(all_data,\"dif_magnitude_1_0.Rdata\")\n\n\n\nCode# read simulated data for reprodubility\nall_data &lt;- readRDS(\"dif_magnitude_1_0.Rdata\")\n\nd &lt;- all_data$simResponses\ndif.group &lt;- all_data$dif_group\ninput_thetas &lt;- all_data$input_thetas\n\n\nWe now have 10 items with 4 categories each. There are 600 respondents in all, with 300 showing differential item functioning for one item (item q3). DIF is induced at +1 logit uniform difference in location (all thresholds for item q3 are unformly +1 logits)."
  },
  {
    "objectID": "dif_magnitude.html#dif-assessment",
    "href": "dif_magnitude.html#dif-assessment",
    "title": "Rasch DIF magnitude & item split",
    "section": "\n2 DIF assessment",
    "text": "2 DIF assessment\nLet’s test for DIF with some different methods.\n\n2.1 LR-test\n\nCodeRIdifTableLR(d, dif.group)\n\n\n\n\n\nItem locations\nStandard errors\n\n\n Item \n    1 \n    2 \n    MaxDiff \n    All \n    SE_1 \n    SE_2 \n    SE_All \n  \n\n\n\n q1 \n    1.561 \n    1.309 \n    0.252 \n    1.425 \n    0.215 \n    0.221 \n    0.153 \n  \n\n q2 \n    -0.691 \n    -0.888 \n    0.197 \n    -0.792 \n    0.205 \n    0.198 \n    0.142 \n  \n\n q3 \n    0.14 \n    0.899 \n    0.759 \n    0.492 \n    0.186 \n    0.196 \n    0.132 \n  \n\n q4 \n    0.641 \n    0.628 \n    0.013 \n    0.634 \n    0.184 \n    0.192 \n    0.131 \n  \n\n q5 \n    0.345 \n    0.133 \n    0.212 \n    0.233 \n    0.183 \n    0.181 \n    0.128 \n  \n\n q6 \n    -1.285 \n    -1.345 \n    0.06 \n    -1.307 \n    0.244 \n    0.222 \n    0.164 \n  \n\n q7 \n    1.411 \n    1.46 \n    0.049 \n    1.419 \n    0.206 \n    0.223 \n    0.151 \n  \n\n q8 \n    -0.87 \n    -0.891 \n    0.021 \n    -0.873 \n    0.219 \n    0.197 \n    0.146 \n  \n\n q9 \n    0.546 \n    0.445 \n    0.101 \n    0.485 \n    0.183 \n    0.185 \n    0.129 \n  \n\n q10 \n    0.919 \n    0.927 \n    0.008 \n    0.916 \n    0.187 \n    0.195 \n    0.134 \n  \n\n\nNote: \n\n Values highlighted in red are above the chosen cutoff 0.5 logits. Background color brown and blue indicate the lowest and highest values among the DIF groups.\n\n\n\nCodeRIdifThreshFigLR(d, dif.group)\n\n\n\n\n\n\n\n\n2.2 Partial gamma\n\nCodeRIpartgamDIF(d, dif.group)\n\n\n\n Item \n    Partial gamma \n    SE \n    Lower CI \n    Upper CI \n    Adjusted p-value (BH) \n  \n\n q3 \n    -0.454 \n    0.071 \n    -0.594 \n    -0.314 \n    0 \n  \n\n\n\n\n2.3 Psychotree\n\nCodeRIdifTable(d, dif.group)\n\n\n\n\n\n\n\n\n\n\nItem\n\n\n2\n\n\n3\n\n\nMean location\n\n\nStDev\n\n\nMaxDiff\n\n\n\n\n\nq1\n\n\n1.289\n\n\n1.041\n\n\n1.165\n\n\n0.176\n\n\n0.248\n\n\n\n\nq2\n\n\n-0.962\n\n\n-1.155\n\n\n-1.059\n\n\n0.136\n\n\n0.193\n\n\n\n\nq3\n\n\n-0.132\n\n\n0.631\n\n\n0.250\n\n\n0.540\n\n\n0.763\n\n\n\n\nq4\n\n\n0.369\n\n\n0.360\n\n\n0.365\n\n\n0.006\n\n\n0.009\n\n\n\n\nq5\n\n\n0.073\n\n\n-0.135\n\n\n-0.031\n\n\n0.147\n\n\n0.208\n\n\n\n\nq6\n\n\n-1.557\n\n\n-1.613\n\n\n-1.585\n\n\n0.039\n\n\n0.056\n\n\n\n\nq7\n\n\n1.140\n\n\n1.192\n\n\n1.166\n\n\n0.037\n\n\n0.053\n\n\n\n\nq8\n\n\n-1.142\n\n\n-1.159\n\n\n-1.150\n\n\n0.012\n\n\n0.017\n\n\n\n\nq9\n\n\n0.274\n\n\n0.177\n\n\n0.226\n\n\n0.068\n\n\n0.097\n\n\n\n\nq10\n\n\n0.647\n\n\n0.659\n\n\n0.653\n\n\n0.008\n\n\n0.012\n\n\n\n\n\n\nDIF clearly shown, however it is closer to 0.8 logits than the 1.0 used in the input values. This is still generally considered to be a large DIF size, so it should serve our purpose.\n\n2.4 Item split\nNow, we’ll do an item split and compare thetas for both groups with and without split, and also with the DIF item removed.\n\nCodethetas_together &lt;- RIestThetas(d)\n\nthetas_q3_removed &lt;- RIestThetas(d %&gt;% select(!q3))\n\nd2 &lt;- d %&gt;% \n  add_column(group = dif.group) %&gt;% \n  mutate(q3a = if_else(group == 1, q3, NA), \n         q3b = if_else(group == 2, q3, NA)\n         ) %&gt;% \n  select(!group) %&gt;%\n  select(!q3)\n\n\nLet’s look at the data and a targeting plot.\n\nCodeRItileplot(d2)\n\n\n\n\n\n\nCodeRImissing(d2)\n\n\n\n\n\n\nCodeRItargeting(d2)\n\n\n\n\n\n\n\nItem threshold locations for the q3 split items range from -1 logits to +1.5 logits along the theta/latent continuum. This is roughly the range where we expect some impact from DIF.\nComparing targeting to non-split data.\n\nCodeRItargeting(d)\n\n\n\n\n\n\nCodeitemlabels &lt;- data.frame(itemnr = names(d), item = \"\")\nRIitemHierarchy(d)\n\n\n\n\n\n\n\n\n2.5 Estimating thetas\nFirst, we’ll use the convenient function RIestThetas() that estimates item parameters (using eRm) and thetas (using iarm), to see how that works when we have a split item, each with missing data for 50% of respondents.\nCodethetas_separate &lt;- RIestThetas(d2)\n\nhist(thetas_separate$WLE, breaks = 30, col = \"lightblue\")\nhist(thetas_together$WLE, breaks = 30, col = \"lightpink\")\nhist(thetas_q3_removed$WLE, breaks = 30, , col = \"sienna4\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe upper range is rather different for the item split subgroup when using this method, with max score of about 2.2, compared to 4.1 for the item set with the original q3 item and entirely without q3.\nLet’s review the estimated threshold locations.\n\n2.6 Comparing item parameters\n\n\nOriginal data\nItem split\nDIF item removed\nPlot estimation bias\nSummary plot estimation bias\n\n\n\n\nCodeRIitemparams(d)\n\n\n\n   \n    Threshold 1 \n    Threshold 2 \n    Threshold 3 \n    Item location \n  \n\n\n q1 \n    0.74 \n    1.14 \n    1.61 \n    1.16 \n  \n\n q2 \n    -1.94 \n    -1.12 \n    -0.10 \n    -1.05 \n  \n\n q3 \n    -0.28 \n    0.16 \n    0.80 \n    0.23 \n  \n\n q4 \n    -0.28 \n    0.13 \n    1.27 \n    0.37 \n  \n\n q5 \n    -0.89 \n    0.04 \n    0.75 \n    -0.03 \n  \n\n q6 \n    -2.25 \n    -1.67 \n    -0.78 \n    -1.57 \n  \n\n q7 \n    0.65 \n    1.01 \n    1.80 \n    1.16 \n  \n\n q8 \n    -1.82 \n    -1.39 \n    -0.20 \n    -1.14 \n  \n\n q9 \n    -1.33 \n    0.58 \n    1.42 \n    0.22 \n  \n\n q10 \n    -0.24 \n    0.62 \n    1.58 \n    0.65 \n  \n\n\nNote: \n\n Item location is the average of the thresholds for each item.\n\n\n\n\n\n\n\nCodeRIitemparams(d2)\n\n\n\n   \n    Threshold 1 \n    Threshold 2 \n    Threshold 3 \n    Item location \n  \n\n\n q1 \n    0.71 \n    1.12 \n    1.60 \n    1.14 \n  \n\n q2 \n    -1.97 \n    -1.15 \n    -0.13 \n    -1.08 \n  \n\n q4 \n    -0.32 \n    0.10 \n    1.25 \n    0.35 \n  \n\n q5 \n    -0.92 \n    0.02 \n    0.73 \n    -0.06 \n  \n\n q6 \n    -2.29 \n    -1.70 \n    -0.81 \n    -1.6 \n  \n\n q7 \n    0.62 \n    0.99 \n    1.80 \n    1.14 \n  \n\n q8 \n    -1.85 \n    -1.42 \n    -0.22 \n    -1.17 \n  \n\n q9 \n    -1.37 \n    0.55 \n    1.41 \n    0.2 \n  \n\n q10 \n    -0.27 \n    0.59 \n    1.57 \n    0.63 \n  \n\n q3a \n    -0.88 \n    -0.04 \n    0.35 \n    -0.19 \n  \n\n q3b \n    0.10 \n    0.36 \n    1.47 \n    0.64 \n  \n\n\nNote: \n\n Item location is the average of the thresholds for each item.\n\n\n\n\n\n\n\nCodeRIitemparams(d %&gt;% select(!q3))\n\n\n\n   \n    Threshold 1 \n    Threshold 2 \n    Threshold 3 \n    Item location \n  \n\n\n q1 \n    0.76 \n    1.19 \n    1.64 \n    1.19 \n  \n\n q2 \n    -1.91 \n    -1.11 \n    -0.08 \n    -1.03 \n  \n\n q4 \n    -0.27 \n    0.15 \n    1.31 \n    0.4 \n  \n\n q5 \n    -0.87 \n    0.06 \n    0.79 \n    -0.01 \n  \n\n q6 \n    -2.22 \n    -1.66 \n    -0.76 \n    -1.55 \n  \n\n q7 \n    0.67 \n    1.05 \n    1.83 \n    1.19 \n  \n\n q8 \n    -1.79 \n    -1.38 \n    -0.17 \n    -1.12 \n  \n\n q9 \n    -1.32 \n    0.60 \n    1.46 \n    0.25 \n  \n\n q10 \n    -0.22 \n    0.65 \n    1.62 \n    0.68 \n  \n\n\nNote: \n\n Item location is the average of the thresholds for each item.\n\n\n\n\n\n\n\nCodedp &lt;- RIitemparams(d, output = \"dataframe\") %&gt;% \n  select(!Location) %&gt;% \n  set_names(paste0(\"orig_\",1:3)) %&gt;% \n  rownames_to_column(\"item\") %&gt;% \n  filter(!str_detect(item,\"q3\"))\ndp2 &lt;- RIitemparams(d2, output = \"dataframe\") %&gt;% \n  select(!Location) %&gt;% \n  set_names(paste0(\"split_\",1:3)) %&gt;% \n  rownames_to_column(\"item\") %&gt;% \n  filter(!str_detect(item,\"q3\"))\ndp3 &lt;- RIitemparams(d %&gt;% select(!q3), output = \"dataframe\") %&gt;% \n  select(!Location) %&gt;% \n  set_names(paste0(\"q3rem_\",1:3))  %&gt;% \n  rownames_to_column(\"item\") %&gt;% \n  filter(!str_detect(item,\"q3\"))\n\nd_params &lt;- cbind(dp,dp2[,-1],dp3[,-1]) # bind columns, keeping one \"item\" column\n\ndpin &lt;- inputParams1c %&gt;% \n  as.data.frame() %&gt;% \n  set_names(paste0(\"input_\",1:3)) %&gt;% \n  rownames_to_column(\"item\") %&gt;% \n  filter(!str_detect(item,\"q3\"))\n\ndpin_long &lt;- dpin %&gt;% \n  pivot_longer(!item,\n               names_sep = \"_\",\n               names_to = c(\"source\",\"threshold\"),\n               values_to = \"input\")\n\n\n\nCoded_params %&gt;% \n  pivot_longer(!item, \n               names_sep = \"_\",\n               names_to = c(\"source\",\"threshold\")) %&gt;% \n  left_join(dpin_long[,-2], by = c(\"item\",\"threshold\")) %&gt;% \n  group_by(source,item,threshold) %&gt;% \n  summarise(abs_diff = abs(input - value)) %&gt;% \n  \n  ggplot(aes(x = threshold, y = abs_diff, color = source)) +\n  geom_point(size = 2, alpha = 0.85) +\n  geom_line(aes(group = source)) +\n  facet_wrap(~ item) +\n  labs(x = \"Absolute bias (logits)\")\n\n\n\n\n\n\n\n\n\n\nCodeitemlocs &lt;- RIitemparams(d %&gt;% select(!q3), output = \"dataframe\") %&gt;% \n  pull(Location)\n\nitem_order &lt;- sort(as.numeric(itemlocs))\n\nd_params %&gt;% \n  pivot_longer(!item, \n               names_sep = \"_\",\n               names_to = c(\"source\",\"threshold\")) %&gt;% \n  left_join(dpin_long[,-2], by = c(\"item\",\"threshold\")) %&gt;% \n  group_by(source,item,threshold) %&gt;% \n  summarise(abs_diff = abs(input - value)) %&gt;% \n  ungroup() %&gt;% \n  group_by(source,item) %&gt;% \n  summarise(sum_diff = sum(abs_diff)) %&gt;% \n  ungroup() %&gt;% \n  pivot_wider(values_from = \"sum_diff\",\n              names_from = \"source\",\n              id_cols = \"item\") %&gt;% \n  pivot_longer(!item) %&gt;% \n  mutate(item = factor(item, levels = names(itemlocs), \n                        labels = paste0(names(itemlocs),\"_\",as.numeric(itemlocs)))) %&gt;% \n  ggplot(aes(x = item, \n             y = value, \n             color = name)\n         ) +\n  geom_point(size = 3) +\n  labs(title = \"Total estimation bias for each item (all thresholds) and item set\",\n       y = \"Absolute bias (logits)\",\n       x = \"Item (and average location)\") +\n  scale_y_continuous(limits = c(0,NA))\n\n\n\n\n\n\n\n\n\n\n\n2.6.1 mirt comparison\nPerhaps the mirt package could be more accurate in item threshold estimation when we have item split?\n\nCodemirt_out &lt;- mirt(data = d, model = 1, itemtype = \"Rasch\", verbose = FALSE)\nmirt_params &lt;- coef(mirt_out, simplify = TRUE, IRTpars = TRUE)$items %&gt;%\n  as.data.frame() %&gt;%\n  select(!a) %&gt;%\n  set_names(paste0(\"orig_\",1:3)) %&gt;%   \n  as.matrix()\ndpm &lt;- mirt_params - mean(mirt_params)\n\ndpm &lt;- dpm %&gt;% \n  as.data.frame() %&gt;% \n  rownames_to_column(\"item\") %&gt;% \n  filter(!str_detect(item,\"q3\"))\n\nmirt_out &lt;- mirt(data = d2, model = 1, itemtype = \"Rasch\", verbose = FALSE)\nmirt_params &lt;- coef(mirt_out, simplify = TRUE, IRTpars = TRUE)$items %&gt;%\n  as.data.frame() %&gt;%\n  select(!a) %&gt;%\n  set_names(paste0(\"split_\",1:3)) %&gt;% \n  as.matrix()\ndpm2 &lt;- mirt_params - mean(mirt_params)\n\ndpm2 &lt;- dpm2 %&gt;% \n  as.data.frame() %&gt;% \n  rownames_to_column(\"item\") %&gt;% \n  filter(!str_detect(item,\"q3\"))\n\nmirt_out &lt;- mirt(data = d %&gt;% select(!q3), model = 1, itemtype = \"Rasch\", verbose = FALSE)\nmirt_params &lt;- coef(mirt_out, simplify = TRUE, IRTpars = TRUE)$items %&gt;%\n  as.data.frame() %&gt;%\n  select(!a) %&gt;%\n  set_names(paste0(\"q3rem_\",1:3)) %&gt;% \n  as.matrix()\ndpm3 &lt;- mirt_params - mean(mirt_params)\n\ndpm3 &lt;- dpm3 %&gt;% \n  as.data.frame() %&gt;% \n  rownames_to_column(\"item\") %&gt;% \n  filter(!str_detect(item,\"q3\"))\n\n\nd_params2 &lt;- cbind(dpm,dpm2[,-1],dpm3[,-1]) # bind columns, keeping one \"item\" column\n\n\n\n\nPlot estimation bias\nSummary plot estimation bias\n\n\n\n\nCoded_params2 %&gt;% \n  pivot_longer(!item, \n               names_sep = \"_\",\n               names_to = c(\"source\",\"threshold\")) %&gt;% \n  left_join(dpin_long[,-2], by = c(\"item\",\"threshold\")) %&gt;% \n  group_by(source,item,threshold) %&gt;% \n  summarise(abs_diff = abs(input - value)) %&gt;% \n  \n  ggplot(aes(x = threshold, y = abs_diff, color = source)) +\n  geom_point(alpha = 0.85) +\n  facet_wrap(~ item)\n\n\n\n\n\n\n\nSeems indentical to eRm estimates.\n\n\n\nCoded_params2 %&gt;% \n  pivot_longer(!item, \n               names_sep = \"_\",\n               names_to = c(\"source\",\"threshold\")) %&gt;% \n  left_join(dpin_long[,-2], by = c(\"item\",\"threshold\")) %&gt;% \n  group_by(source,item,threshold) %&gt;% \n  summarise(abs_diff = abs(input - value)) %&gt;% \n  ungroup() %&gt;% \n  group_by(source,item) %&gt;% \n  summarise(sum_diff = sum(abs_diff)) %&gt;% \n  ungroup() %&gt;% \n  pivot_wider(values_from = \"sum_diff\",\n              names_from = \"source\",\n              id_cols = \"item\") %&gt;% \n  pivot_longer(!item) %&gt;% \n  mutate(item = factor(item, levels = names(itemlocs), \n                        labels = paste0(names(itemlocs),\"_\",as.numeric(itemlocs)))) %&gt;% \n  ggplot(aes(x = item, \n             y = value, \n             color = name)\n         ) +\n  geom_point(size = 3) +\n  labs(title = \"mirt: Total estimation bias for each item and item set\")\n\n\n\n\n\n\n\nNo difference here either.\n\n\n\n\n2.7 Theta estimation investigated\nThe method for estimating item parameters and thetas used in the function RIestThetas() may be at fault for the odd results in thetas estimated by the item set with item split? We can separate the two steps, and use a separate function for theta estimation with manual input of item parameters.\n\nCodeitemps &lt;- RIitemparams(d2, output = \"dataframe\") %&gt;% \n  select(!Location) %&gt;% \n  as.matrix()\n\nthetas_separate_catR &lt;- RIestThetasOLD(d2, itemParams = itemps, theta_range = c(-8,8))\n\nc &lt;- tibble(together_RIestThetas = thetas_together$WLE,\n           together_q3_rem = thetas_q3_removed$WLE,\n           separate_catR = thetas_separate_catR,\n           separate_RIestThetas = thetas_separate$WLE,\n           input_thetas = input_thetas)\n\nc %&gt;% \n  pivot_longer(everything(),\n               names_to = \"method\",\n               values_to = \"theta\") %&gt;% \n  ggplot(aes(x = theta)) +\n  geom_histogram(bins = 50) +\n  facet_wrap(~method, axes = \"all_x\") +\n  scale_x_continuous(breaks = seq(-5,5,1))\n\n\n\n\n\n\n\nLooks like the two step approach worked a lot better. Since the item parameter estimation is identical (both are using eRm::PCM()), the reason should be the difference in theta estimation. The two-step approach uses catR::thetaEst() for theta estimation, which is probably handling missing data better than iarm::person_estimates(). Note: both approaches use the Weighted Likelihood Estimation to minimize bias (Warm, 1989)."
  },
  {
    "objectID": "dif_magnitude.html#results",
    "href": "dif_magnitude.html#results",
    "title": "Rasch DIF magnitude & item split",
    "section": "\n3 Results",
    "text": "3 Results\n\n3.1 Summarised\nFirst, absolute differences in estimated thetas compared to input thetas. By using absolute differences we can assess both DIF groups simultaneously.\n\nCodec_diff &lt;- c %&gt;% \n  mutate(with_q3 = abs(together_RIestThetas - input_thetas),\n         q3_removed = abs(together_q3_rem - input_thetas),\n         q3_split = abs(separate_catR - input_thetas)) %&gt;% \n  select(!names(c))\n  \nc_diff %&gt;% \n  pivot_longer(everything()) %&gt;% \n  ggplot(aes(x = value)) +\n  geom_histogram(bins = 100) +\n  facet_wrap(~ name, ncol = 1) +\n  labs(x = \"Absolute difference in logits\",\n       title = \"Comparing input thetas to estimated\",\n       subtitle = \"Distribution of bias\")\n\n\n\n\n\n\n\n\nCodec_diff_descr &lt;- descr(c_diff) %&gt;% \n  as.data.frame() %&gt;% \n  rownames_to_column(\"Parameter\") \n\nc_diff_descr[1:9,] %&gt;% \n  pivot_longer(!Parameter) %&gt;% \n  \n  ggplot(aes(x = Parameter, y = value, color = name)) +\n  geom_point(alpha = 0.85) +\n  scale_color_viridis_d('Item set', end = 0.8) +\n  labs(y = \"Logits\",\n       x = \"Descriptive metric\")\n\n\n\n\n\n\n\n\nCodec_diff_descr[1:9,] %&gt;% \n  mutate_if(is.numeric, round, 3) %&gt;% \n  kbl_rise(tbl_width = 50)\n\n\n\n Parameter \n    q3_removed \n    q3_split \n    with_q3 \n  \n\n\n Mean \n    0.378 \n    0.354 \n    0.358 \n  \n\n Std.Dev \n    0.288 \n    0.275 \n    0.278 \n  \n\n Min \n    0.000 \n    0.001 \n    0.000 \n  \n\n Q1 \n    0.159 \n    0.125 \n    0.136 \n  \n\n Median \n    0.316 \n    0.292 \n    0.302 \n  \n\n Q3 \n    0.546 \n    0.525 \n    0.518 \n  \n\n Max \n    1.729 \n    1.732 \n    1.770 \n  \n\n MAD \n    0.283 \n    0.272 \n    0.278 \n  \n\n IQR \n    0.385 \n    0.400 \n    0.381 \n  \n\n\n\n\nNo real differences in these summary metrics.\nWe should look more closely at the particular region where the DIF item is located, since it should have the most impact there.\n\n3.2 Across the latent continuum\nFirst, the test information function (TIF) curve could be of interest to understand what to expect in terms of estimation bias due to reliability limitations. Even more interesting is the table showing range of SEM.\n\n\nTIF original data\nSEM original data\nTIF without q3\nSEM without q3\n\n\n\n\nCodeRItif(d, samplePSI = TRUE)\n\n\n\n\n\n\n\n\n\n\nCodesem_orig &lt;- RIscoreSE(d, output = \"dataframe\") %&gt;% \n  clean_names() %&gt;% \n  round(3)\n\nsem_orig_min &lt;- min(sem_orig$logit_std_error)\n\nsem_orig %&gt;% \n  ggplot(aes(x = logit_score, y = logit_std_error)) +\n  geom_point(size = 2.5) +\n  geom_line() +\n  geom_text(data = . %&gt;% \n              filter(logit_std_error == sem_orig_min),\n            aes(label = sem_orig_min), \n            position = position_nudge(y = -0.03)) +\n  scale_x_continuous(breaks = seq(-5,5,0.5)) +\n  scale_y_continuous(limits = c(0,NA), breaks = seq(0,1,0.1))\n\n\n\n\n\n\n\n\n\n\nCodeRItif(d %&gt;% select(!q3), samplePSI = TRUE)\n\n\n\n\n\n\n\n\n\n\nCodesem_noq3 &lt;- RIscoreSE(d %&gt;% select(!q3), output = \"dataframe\") %&gt;% \n  clean_names() %&gt;% \n  round(3)\n\nsem_noq3_min &lt;- min(sem_noq3$logit_std_error)\n\nsem_noq3 %&gt;% \n  ggplot(aes(x = logit_score, y = logit_std_error)) +\n  geom_point(size = 2.5) +\n  geom_line() +\n  geom_text(data = . %&gt;% \n              filter(logit_std_error == sem_noq3_min) %&gt;% \n              slice(1),\n            aes(label = sem_noq3_min), \n            position = position_nudge(y = -0.03)) +\n  scale_x_continuous(breaks = seq(-5,5,0.5)) +\n  scale_y_continuous(limits = c(0,NA), breaks = seq(0,1,0.1))\n\n\n\n\n\n\n\n\n\n\nThe lowest SEM with q3 is 0.384, at logit score 0.356.\nLowest SEM without q3 is 0.416, at logit score 0.322 to 0.491, which makes a difference in minimal SEM of about 0.032 compared to including the DIF item. 0.032 * 1.96 = 0.063 for a 95% CI.\n\n3.3 Theta estimation bias\n\n\nLoess smoothing\nScatterplot\nLoess “zoomed in”\n\n\n\n\nCodec_diff %&gt;% \n  add_column(Theta = input_thetas) %&gt;% \n  pivot_longer(!Theta) %&gt;% \n  \n  ggplot(aes(x = Theta, y = value, color = factor(name), fill = factor(name))) +\n  #geom_point(alpha = 0.85) +\n  geom_smooth(method = \"loess\",\n              aes(linetype = factor(name)), alpha = 0.15) +\n  scale_color_ghibli_d(\"MononokeMedium\", direction = -1) +\n  scale_fill_ghibli_d(\"MononokeMedium\", direction = -1) +\n  labs(color = \"Item set\",\n       title = \"Theta estimation bias\",\n       y = \"Absolute difference (logits)\") +\n  guides(fill = \"none\", linetype = \"none\") +\n  scale_x_continuous(breaks = seq(-5,5,1)) +\n  scale_y_continuous(limits = c(0,NA), breaks = seq(0,1.2,0.1)) +\n  theme_rise()\n\n\n\n\n\n\nFigure 1\n\n\n\n\n\n\n\nCodec_diff %&gt;% \n  add_column(Theta = input_thetas) %&gt;% \n  pivot_longer(!Theta) %&gt;% \n  \n  ggplot(aes(x = Theta, y = value, color = factor(name), fill = factor(name))) +\n  geom_point(alpha = 0.85) +\n  scale_color_ghibli_d(\"MononokeMedium\", direction = -1) +\n  scale_fill_ghibli_d(\"MononokeMedium\", direction = -1) +\n  labs(color = \"Item set\",\n       title = \"Theta estimation bias\",\n       y = \"Absolute difference in logits\") +\n  guides(fill = \"none\", linetype = \"none\") +\n  scale_x_continuous(breaks = seq(-5,5,1)) +\n  scale_y_continuous(limits = c(0,NA), breaks = seq(0,2,0.1)) +\n  theme_rise()\n\n\n\n\n\n\n\n\n\n\nCodec_diff %&gt;% \n  add_column(Theta = input_thetas) %&gt;% \n  pivot_longer(!Theta) %&gt;% \n  \n  ggplot(aes(x = Theta, y = value, color = factor(name), fill = factor(name))) +\n  #geom_point(alpha = 0.85) +\n  geom_smooth(method = \"loess\",\n              aes(linetype = factor(name)), alpha = 0.15) +\n  scale_color_ghibli_d(\"MononokeMedium\", direction = -1) +\n  scale_fill_ghibli_d(\"MononokeMedium\", direction = -1) +\n  labs(color = \"Item set\",\n       title = \"Theta estimation bias\",\n       subtitle = \"Cropped to improve readability\",\n       y = \"Absolute difference (logits)\") +\n  guides(fill = \"none\", linetype = \"none\") +\n  scale_x_continuous(limits = c(-2,2), breaks = seq(-5,5,1)) +\n  scale_y_continuous(breaks = seq(0,1.2,0.05)) +\n  theme_rise()\n\n\n\n\n\n\n\n\n\n\nItem split slightly reduces bias compared to keeping the DIF item (3), while removing item 3 increases bias in the theta range of about -1 to +1.5 logits. The maximum bias looks like approximately 0.055 logits (at theta = 0), according to the loess smoothed line.\n\n3.4 Statistical analysis of means (limited range)\nWhile the practical impact of theta estimation bias induced by a DIF variable should be judged by how problematic the maximum bias is for the intended use and need for precision, it could be interesting to quantify the differences using statistical analysis. It is sometimes suggested to look at the mean of the groups, but I think this is mistaken. The bias is local and related to the DIF item’s location, which makes it relevant to look at that region separately.\nAs such, the comparison below is about mean differences in estimated thetas to input thetas, limited to the theta range from -0.5 to +1 logits, where differences seem the biggest according to Figure 1. Note that the figure shows loess smoothed lines that include both groups.\nWe need to do this separately for the two DIF groups, since the groups will have opposite effects of the DIF.\n\nCodec %&gt;% \n  add_column(group = dif.group) %&gt;% \n  filter(group == 1) %&gt;% \n  select(c(separate_catR,input_thetas)) %&gt;%\n  filter(input_thetas &gt; -0.5 & input_thetas &lt; 1) %&gt;% \n  nrow()\n\n[1] 130\n\nCodec %&gt;% \n  add_column(group = dif.group) %&gt;% \n  filter(group == 2) %&gt;% \n  select(c(separate_catR,input_thetas)) %&gt;%\n  filter(input_thetas &gt; -0.5 & input_thetas &lt; 1) %&gt;% \n  nrow()\n\n[1] 115\n\n\nn = 130 and 115. More than a third of each group is located within the range affected by DIF.\n\nCodelm_all &lt;- c %&gt;% \n  add_column(group = dif.group) %&gt;% \n  filter(group == 1) %&gt;% \n  select(c(together_RIestThetas,input_thetas)) %&gt;%\n  filter(input_thetas &gt; -0.5 & input_thetas &lt; 1) %&gt;% \n  pivot_longer(everything()) %&gt;% \n  lm(value ~ name, data = .)\n\nlm_q3rem &lt;- c %&gt;% \n  add_column(group = dif.group) %&gt;% \n  filter(group == 1) %&gt;% \n  select(c(together_q3_rem,input_thetas)) %&gt;%\n  filter(input_thetas &gt; -0.5 & input_thetas &lt; 1) %&gt;% \n  pivot_longer(everything()) %&gt;% \n  lm(value ~ name, data = .)\n\nlm_q3split &lt;- c %&gt;% \n  add_column(group = dif.group) %&gt;% \n  filter(group == 1) %&gt;% \n  select(c(separate_catR,input_thetas)) %&gt;%\n  filter(input_thetas &gt; -0.5 & input_thetas &lt; 1) %&gt;% \n  pivot_longer(everything()) %&gt;% \n  lm(value ~ name, data = .)\n\ng1 &lt;- bind_rows(tidy(lm_all, conf.int = TRUE, conf.level = .84) %&gt;% slice(2),\n          tidy(lm_q3rem, conf.int = TRUE, conf.level = .84) %&gt;% slice(2),\n          tidy(lm_q3split, conf.int = TRUE, conf.level = .84) %&gt;% slice(2)\n          ) %&gt;% \n  select(!term) %&gt;% \n  round(3) %&gt;% \n  add_column(Model = c(\"All items\",\"Q3 removed\",\"Q3 split\"), .before = \"estimate\")\n\n\n\nCodelm_all2 &lt;- c %&gt;% \n  add_column(group = dif.group) %&gt;% \n  filter(group == 2) %&gt;% \n  select(c(together_RIestThetas,input_thetas)) %&gt;%\n  filter(input_thetas &gt; -0.5 & input_thetas &lt; 1) %&gt;% \n  pivot_longer(everything()) %&gt;% \n  lm(value ~ name, data = .)\n\nlm_q3rem2 &lt;- c %&gt;% \n  add_column(group = dif.group) %&gt;% \n  filter(group == 2) %&gt;% \n  select(c(together_q3_rem,input_thetas)) %&gt;%\n  filter(input_thetas &gt; -0.5 & input_thetas &lt; 1) %&gt;% \n  pivot_longer(everything()) %&gt;% \n  lm(value ~ name, data = .)\n\nlm_q3split2 &lt;- c %&gt;% \n  add_column(group = dif.group) %&gt;% \n  filter(group == 2) %&gt;% \n  select(c(separate_catR,input_thetas)) %&gt;%\n  filter(input_thetas &gt; -0.5 & input_thetas &lt; 1) %&gt;% \n  pivot_longer(everything()) %&gt;% \n  lm(value ~ name, data = .)\n\ng2 &lt;- bind_rows(tidy(lm_all2, conf.int = TRUE, conf.level = .84) %&gt;% slice(2),\n          tidy(lm_q3rem2, conf.int = TRUE, conf.level = .84) %&gt;% slice(2),\n          tidy(lm_q3split2, conf.int = TRUE, conf.level = .84) %&gt;% slice(2)\n          ) %&gt;% \n  select(!term) %&gt;% \n  round(3) %&gt;% \n  add_column(Model = c(\"All items\",\"Q3 removed\",\"Q3 split\"), .before = \"estimate\")\n\n\n\n3.4.1 Results figure\n\nCoderbind(g1,g2) %&gt;% \n  add_column(Group = rep(c(\"Group 1\",\"Group 2\"), each = 3)) %&gt;% \n  \n  ggplot(aes(x = Group, y = estimate, color = Model)) +\n  geom_point(position = position_dodge(width = 0.2)) +\n  geom_errorbar(aes(ymin = conf.low, ymax = conf.high),\n                width = 0.1,\n                position = position_dodge(width = 0.2)) +\n  scale_color_ghibli_d(\"MononokeMedium\", direction = -1) +\n  labs(color = \"Item set\",\n       title = \"Mean bias in theta estimation\",\n       subtitle = \"Across a limited theta region (-0.5 to 1.0 logits)\",\n       y = \"Model estimate\", \n       x = \"\")\n\n\n\n\n\n\n\nSince we are primarily interested in comparing the different item sets to each other, it is not the difference from input thetas (estimate = 0) that is most relevant here. As such, I chose to display 84% confidence intervals to be able to assess differences between item sets in each group by looking at whether the CI’s overlap or not.\n\n3.4.2 Theta bias - loess by group\n\nCodec_diff %&gt;% \n  add_column(Theta = input_thetas) %&gt;% \n  mutate(Group = case_match(dif.group, \n                            \"1\" ~ \"Group 1\",\n                            \"2\" ~ \"Group 2\")) %&gt;% \n  pivot_longer(!(c(\"Theta\",\"Group\"))) %&gt;% \n  \n  ggplot(aes(x = Theta, y = value, color = factor(name), fill = factor(name))) +\n  #geom_point(alpha = 0.85) +\n  geom_smooth(method = \"loess\",\n              aes(linetype = factor(name)), alpha = 0.15) +\n  scale_color_ghibli_d(\"MononokeMedium\", direction = -1) +\n  scale_fill_ghibli_d(\"MononokeMedium\", direction = -1) +\n  labs(color = \"Item set\",\n       title = \"Theta estimation bias\",\n       subtitle = \"Cropped to improve readability\",\n       y = \"Absolute difference (logits)\") +\n  guides(fill = \"none\", linetype = \"none\") +\n  scale_x_continuous(limits = c(-2,2), breaks = seq(-5,5,1)) +\n  scale_y_continuous(breaks = seq(0,1.2,0.05)) +\n  facet_wrap(~Group, ncol = 1,) +\n  theme_rise()"
  },
  {
    "objectID": "dif_magnitude.html#session-info",
    "href": "dif_magnitude.html#session-info",
    "title": "Rasch DIF magnitude & item split",
    "section": "\n4 Session info",
    "text": "4 Session info\n\nCodesessionInfo()\n\nR version 4.4.1 (2024-06-14)\nPlatform: aarch64-apple-darwin20\nRunning under: macOS Sonoma 14.6.1\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\ntime zone: Europe/Stockholm\ntzcode source: internal\n\nattached base packages:\n[1] grid      stats4    stats     graphics  grDevices utils     datasets \n[8] methods   base     \n\nother attached packages:\n [1] broom_1.0.7          ghibli_0.3.4         summarytools_1.0.1  \n [4] RISEkbmRasch_0.2.4.6 janitor_2.2.0        hexbin_1.28.4       \n [7] glue_1.7.0           ggrepel_0.9.6        patchwork_1.3.0     \n[10] reshape_0.8.9        matrixStats_1.4.1    psychotree_0.16-1   \n[13] psychotools_0.7-4    partykit_1.2-22      mvtnorm_1.3-1       \n[16] libcoin_1.0-10       psych_2.4.6.26       kableExtra_1.4.0    \n[19] formattable_0.2.1    catR_3.17            mirt_1.42           \n[22] lattice_0.22-6       iarm_0.4.3           eRm_1.0-6           \n[25] lubridate_1.9.3      forcats_1.0.0        stringr_1.5.1       \n[28] dplyr_1.1.4          purrr_1.0.2          readr_2.1.5         \n[31] tidyr_1.3.1          tibble_3.2.1         ggplot2_3.5.1       \n[34] tidyverse_2.0.0     \n\nloaded via a namespace (and not attached):\n  [1] vcd_1.4-12           rstudioapi_0.16.0    audio_0.1-11        \n  [4] jsonlite_1.8.9       magrittr_2.0.3       magick_2.8.5        \n  [7] farver_2.1.2         rmarkdown_2.28       vctrs_0.6.5         \n [10] base64enc_0.1-3      htmltools_0.5.8.1    curl_5.2.3          \n [13] cellranger_1.1.0     Formula_1.2-5        dcurver_0.9.2       \n [16] parallelly_1.38.0    htmlwidgets_1.6.4    plyr_1.8.9          \n [19] testthat_3.2.1.1     zoo_1.8-12           lifecycle_1.0.4     \n [22] pkgconfig_2.0.3      Matrix_1.7-0         R6_2.5.1            \n [25] fastmap_1.2.0        snakecase_0.11.1     future_1.34.0       \n [28] digest_0.6.37        colorspace_2.1-1     rprojroot_2.0.4     \n [31] prismatic_1.1.2      Hmisc_5.1-3          vegan_2.6-8         \n [34] labeling_0.4.3       progressr_0.14.0     fansi_1.0.6         \n [37] timechange_0.3.0     abind_1.4-5          mgcv_1.9-1          \n [40] compiler_4.4.1       here_1.0.1           vcdExtra_0.8-5      \n [43] pander_0.6.5         withr_3.0.1          htmlTable_2.4.3     \n [46] backports_1.5.0      carData_3.0-5        relimp_1.0-5        \n [49] R.utils_2.12.3       MASS_7.3-61          sessioninfo_1.2.2   \n [52] GPArotation_2024.3-1 permute_0.9-7        tools_4.4.1         \n [55] foreign_0.8-87       lmtest_0.9-40        future.apply_1.11.2 \n [58] nnet_7.3-19          R.oo_1.26.0          nlme_3.1-165        \n [61] inum_1.0-5           checkmate_2.3.2      reshape2_1.4.4      \n [64] cluster_2.1.6        generics_0.1.3       snow_0.4-4          \n [67] gtable_0.3.5         RPushbullet_0.3.4    tzdb_0.4.0          \n [70] R.methodsS3_1.8.2    ca_0.71.1            data.table_1.16.0   \n [73] hms_1.1.3            car_3.1-2            xml2_1.3.6          \n [76] Deriv_4.1.3          utf8_1.2.4           pillar_1.9.0        \n [79] splines_4.4.1        pryr_0.1.6           survival_3.7-0      \n [82] tidyselect_1.2.1     pbapply_1.7-2        knitr_1.48          \n [85] gridExtra_2.3        svglite_2.1.3        xfun_0.46           \n [88] gnm_1.1-5            rapportools_1.1      brio_1.1.5          \n [91] stringi_1.8.4        yaml_2.3.10          evaluate_0.24.0     \n [94] codetools_0.2-20     beepr_2.0            tcltk_4.4.1         \n [97] cli_3.6.3            rpart_4.1.23         qvcalc_1.0.3        \n[100] systemfonts_1.1.0    munsell_0.5.1        Rcpp_1.0.13         \n[103] readxl_1.4.3         globals_0.16.3       parallel_4.4.1      \n[106] listenv_0.9.1        viridisLite_0.4.2    SimDesign_2.17.1    \n[109] scales_1.3.0         rlang_1.1.4          mnormt_2.1.1"
  },
  {
    "objectID": "datawrangling.html#simulating-data-based-on-known-item-parameters",
    "href": "datawrangling.html#simulating-data-based-on-known-item-parameters",
    "title": "Data wrangling for psychometrics in R",
    "section": "7 Simulating data based on known item parameters",
    "text": "7 Simulating data based on known item parameters\nlibrary(easyRasch)\n\n# read item parameters\nw &lt;- read_csv(\"item_params.csv\") %&gt;% \n  select(!Location) %&gt;% \n  as.matrix()\n\n# generate 10 000 random theta values\nt &lt;- rnorm(10000,0,2)\n\n# get item parameters into a list object, each item as a separate vector\nitemlist &lt;- map(1:3, ~ w[.x,] %&gt;% as.numeric() %&gt;% na.omit())\n\n# simulate response data\nd &lt;- SimPartialScore(\n  deltaslist = itemlist,\n  thetavec = t\n) %&gt;%\n  as.data.frame()\n\nRItargeting(d)\n\nRItif(d, samplePSI = T)"
  },
  {
    "objectID": "clrt.html",
    "href": "clrt.html",
    "title": "Conditional Likelihood Ratio Test and sample size",
    "section": "",
    "text": "1 Introduction\nIn my recent preprint on detection of item misfit in Rasch models (Johansson 2025a), the conditional likelihood ratio test (LRT, Andersen 1973) was part of one of the simulation studies. In that study, only the detection rate of misfitting items was assessed. In this brief note, the false detection rate across varying sample sizes will be investigated.\n\n\n\n\n\n\nNote\n\n\n\nThe preprint was published in March 2025 in Educational Methods & Psychometrics and can be accessed at https://pgmj.github.io/rasch_itemfit/\nOn August 6th 2025, I added results for n = 150. This text was originally published on February 7th 2025.\n\n\n\n2 Method\nFor simplicity, the simulated dataset from the previously mentioned preprint will be re-used, and the misfitting item removed. This results in 19 dichotomous items, all simulated to fit the Rasch model. The easyRasch (Johansson 2025b) package contains a function to use non-parametric bootstrap with the LRT. The code to do so is presented below. As a comparison, a subset of 10 items from the same data was used to evaluate the impact of number of items on LRT false positive performance.\n\nRIbootLRT(simdata[[1]][,-9], iterations = 5000, samplesize = 300, cpu = 8)\n\nEach sample size variation used 5000 bootstrap iterations.\n\n3 Results\nResults are presented in Figure 1.\n\nCodelibrary(dplyr)\nlibrary(ggplot2)\n\nd &lt;- data.frame(\n  Percent = c(5.8, 6.6, 6.8, 8.2, 10.4, 12.2, 4.9, 5.3, 5.4, 5.3, 5.9, 6.7),\n  n = c(150, 250, 500, 1000, 1500, 2000, 150, 250, 500, 1000, 1500, 2000),\n  k = factor(c(19, 19, 19, 19, 19, 19, 10, 10, 10, 10, 10, 10))\n)\n\nd %&gt;%\n  ggplot(aes(x = n, y = Percent, color = k)) +\n  geom_point() +\n  geom_line() +\n  geom_text(\n    data = subset(d, k == 19),\n    aes(label = paste0(Percent, \"%\")),\n    position = position_dodge(width = 9),\n    hjust = 0.3,\n    vjust = -1,\n    color = \"black\"\n  ) +\n  geom_text(\n    data = subset(d, k == 10),\n    aes(label = paste0(Percent, \"%\")),\n    position = position_dodge(width = 9),\n    hjust = 0.3,\n    vjust = 2,\n    color = \"black\"\n  ) +\n  geom_hline(yintercept = 5, linetype = \"dashed\", color = \"grey\")  +\n  scale_x_continuous(\n    'Sample size',\n    limits = c(0, 2200),\n    breaks = c(0, 150, 250, 500, 1000, 1500, 2000),\n    minor_breaks = NULL\n  ) +\n  scale_y_continuous('% false positives', limits = c(0, 20)) +\n  scale_color_brewer('Items', palette = \"Dark2\") +\n  theme_bw()\n\n\n\n\n\n\nFigure 1: Percent of false positives indicated by LRT bootstrap procedure across sample sizes\n\n\n\n\n\n4 Discussion\nThis is a brief note, not a full scale simulation study. Many variables could be manipulated to better understand the expected behavior of LRT when all items fit a Rasch model. Nevertheless, this small study provides some useful information about the relationship between sample size, number of items, and false positive rate for the LRT. Even at the smaller sample sizes of 250 and 500, the false positive rate is above the expected 5% for both 10 and 19 item conditions. The effect is stronger for the condition with more items. It seems clear that one should not rely too heavily on the LRT in determining model fit, especially when sample size is above 1000 and number of items is high.\nWhile the RIbootLRT() function could be used instead of the standard CLRT, one would need to run simulations adapted to the dataset being analyzed to understand which sample size should be used with the bootstrap CLRT function. I might write such a function at some point in time.\nI will add a condition using polytomous items later on.\n\n\n\n\n\n\n\n5 References\n\nAndersen, Erling B. 1973. “A Goodness of Fit Test for the Rasch Model.” Psychometrika 38 (1): 123–40. https://doi.org/10.1007/BF02291180.\n\n\nJohansson, Magnus. 2025a. “Detecting Item Misfit in Rasch Models.” OSF Preprints. https://doi.org/10.31219/osf.io/j8fg2.\n\n\n———. 2025b. easyRasch: Psychometric Analysis in r with Rasch Measurement Theory. https://github.com/pgmj/easyRasch.\n\nReuseCC BY 4.0CitationBibTeX citation:@online{johansson2025,\n  author = {Johansson, Magnus},\n  title = {Conditional {Likelihood} {Ratio} {Test} and Sample Size},\n  date = {2025-08-06},\n  url = {https://pgmj.github.io/clrt.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nJohansson, Magnus. 2025. “Conditional Likelihood Ratio Test and\nSample Size.” August 6, 2025. https://pgmj.github.io/clrt.html."
  }
]